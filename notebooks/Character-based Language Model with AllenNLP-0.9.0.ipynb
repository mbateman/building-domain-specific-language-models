{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad52971",
   "metadata": {},
   "source": [
    "# Character-based Language Model with AllenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c82f9",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this task, we switch to character-based language models. We implement the same deep learning multinomial classification approach that we completed in the previous task.\n",
    "\n",
    "The goal remains to predict the next token given a preceding sequence of tokens. However, by using characters as tokens, instead of words, we solve 2 problems:\n",
    "\n",
    "There are no more Out Of Vocabulary (OOV) tokens, since all the characters are known in advance.\n",
    "The total number of classes to predict is reduced to a few dozen characters instead of thousands of different words. (This is true for alphabetic-based scripts such as Latin, Arabic or Cyrillic but not in the case of logographic scripts used in Mandarin, Korean, or Japanese.)\n",
    "To build a character-based language model on our domain specific corpus, we will use the AllenNLP framework. AllenNLP is a state-of-the-art NLP framework created by the Allen Institute for AI. Its generic approach allows us to work on a wide range of NLP problems. AllenNLP is based on PyTorch.\n",
    "\n",
    "Experimenting with character based language models underlines the difference with word based models in terms of the implementation process and the resulting outputs (generated texts and perplexity scores). In particular, switching from words to characters as the target multiclass has 2 main advantages:\n",
    "\n",
    "The number of target classes is drastically reduced from thousands of tokens to less than a hundred characters.\n",
    "All characters are known in advance.\n",
    "The abstraction level of the AllenNLP framework makes it particularly well suited to handle all sorts of NLP tasks (POS, NER, ). And the investment required to learn the framework is well worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb2745",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "- Character set\n",
    "\n",
    "As you will notice, the set of characters present in the corpus is much richer than the expected set of ASCII letters and digits. It may be useful to filter out rare and noisy characters.\n",
    "\n",
    "- AllenNLP\n",
    "\n",
    "AllenNLP is based on PyTorch and therefore enforces a precise object-oriented code structure with predefined classes and functions. Although specific to AllenNLP, these constraints bring an efficient conceptualization of diverse NLP tasks. The return on investment (ROI) is worth the time needed to learn the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13f1c5",
   "metadata": {},
   "source": [
    "## Explore and analyze the set of unique characters present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d3b59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/michael/.pyenv/versions/3.7.1/envs/allennlp=0.9.0/bin/python3.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb57988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "304befe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import allennlp\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder, TextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.data.tokenizers.token import Token\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.vocabulary import Vocabulary, DEFAULT_PADDING_TOKEN\n",
    "from allennlp.models import Model\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7fc811e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allennlp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568a65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"../data/stackexchange_812k.tokenized.csv\").sample(frac=1, random_state=8).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfaf7fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t' '\\x0b' '\\x0c' ' ' '!' \"'\" ',' '-' '.' '?' '\\\\' 'a' 'b' 'c' 'd' 'e'\n",
      " 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w'\n",
      " 'x' 'y' 'z' '\\x7f' '\\xa0' '¡' '¢' '£' '¥' '¦' '§' '¨' '©' 'ª' '«' '¬'\n",
      " '\\xad' '®' '¯' '°' '±' '²' '³' '´' 'µ' '¶' '·' '¹' 'º' '»' '¼' '½' '¾'\n",
      " '¿' '×' 'ß' 'à' 'á' 'â' 'ã' 'ä' 'å' 'æ' 'ç' 'è' 'é' 'ê' 'ë' 'ì' 'í' 'î'\n",
      " 'ï' 'ð' 'ñ' 'ò' 'ó' 'ô' 'õ' 'ö' '÷' 'ø' 'ù' 'ú' 'û' 'ü' 'ý' 'ā' 'ă' 'ą'\n",
      " 'ć' 'č' 'ē' 'ĕ' 'ė' 'ę' 'ğ' 'ī' 'ı' 'ĺ' 'ļ' 'ł' 'ń' 'ō' 'ő' 'œ' 'ř' 'ś'\n",
      " 'ş' 'š' 'ū' 'ů' 'ŷ' 'ź' 'ž' 'ƒ' 'ơ' 'ƴ' 'ț' 'ȳ' 'ɑ' 'ə' 'ɛ' 'ɣ' 'ɪ' 'ɵ'\n",
      " 'ʃ' 'ʊ' 'ʒ' 'ʼ' 'ˆ' 'ˇ' 'ˈ' 'ˉ' 'ˌ' '˙' '˚' '˜' '̀' '́' '̂' '̃' '̄' '̅'\n",
      " '̇' '̈' '̧' '̶' '̸' '͝' ';' '΄' 'ά' 'έ' 'ή' 'ί' 'α' 'β' 'γ' 'δ' 'ε' 'ζ'\n",
      " 'η' 'θ' 'ι' 'κ' 'λ' 'μ' 'ν' 'ξ' 'ο' 'π' 'ρ' 'ς' 'σ' 'τ' 'υ' 'φ' 'χ' 'ψ'\n",
      " 'ω' 'ό' 'ύ' 'ώ' 'ϐ' 'ϕ' 'ϵ' 'а' 'б' 'в' 'г' 'д' 'е' 'ж' 'з' 'и' 'й' 'к'\n",
      " 'л' 'м' 'н' 'о' 'п' 'р' 'с' 'т' 'у' 'ф' 'х' 'ц' 'ч' 'ш' 'щ' 'ъ' 'ы' 'ь'\n",
      " 'э' 'ю' 'я' 'ё' 'є' 'א' 'ב' 'ד' 'ה' 'ו' 'ח' 'י' 'כ' 'ל' 'מ' 'נ' 'ע' 'ק'\n",
      " 'ר' 'ת' '؟' 'ا' 'ب' 'ة' 'ت' 'ح' 'ر' 'س' 'ع' 'ك' 'ل' 'م' 'ن' 'ه' 'و' 'ى'\n",
      " 'ي' '٪' 'پ' 'ے' 'ಠ' 'ᵢ' 'ṓ' 'ṽ' 'ẁ' 'ẃ' 'ẋ' 'ἀ' 'ἐ' 'ἕ' 'ἡ' 'ὁ' 'ὅ' 'ὑ'\n",
      " 'ὲ' 'ὶ' '᾽' 'ῖ' 'ῡ' 'ῦ' '\\u2002' '\\u2005' '\\u2009' '\\u200a' '\\u200b'\n",
      " '\\u200c' '\\u200e' '\\u200f' '‐' '‑' '–' '—' '―' '‖' '‘' '’' '‚' '„' '‟'\n",
      " '†' '‡' '•' '…' '\\u202c' '\\u202f' '‰' '′' '″' '‽' '‾' '⁄' '⁎' '\\u2060'\n",
      " '\\u2061' '⁰' '⁴' '⁵' '⁶' '⁷' '⁸' '⁹' '⁺' '⁻' '⁽' 'ⁿ' '₀' '₁' '₂' '₃' '₄'\n",
      " '₅' '₋' 'ₐ' 'ₑ' 'ₓ' 'ₔ' 'ₙ' '€' '₹' '⃗' '℃' 'ℒ' 'ℓ' '№' '℗' 'ℙ' 'ℝ' '™'\n",
      " 'ℤ' 'ℱ' '⅓' '⅔' '⅕' '←' '↑' '→' '↦' '↵' '⇒' '⇔' '⇝' '∀' '∂' '∃' '∅' '∆'\n",
      " '∇' '∈' '∉' '∏' '∑' '−' '∓' '∕' '∗' '∘' '√' '∝' '∞' '∠' '∣' '∥' '∨' '∩'\n",
      " '∪' '∫' '∴' '∶' '∼' '≃' '≈' '≏' '≠' '≡' '≤' '≥' '≪' '≫' '≰' '⊂' '⊆' '⊙'\n",
      " '⊤' '⊥' '⋅' '⋯' '⍺' '⎡' '⎢' '⎣' '⎤' '⎥' '⎦' '⏜' '\\u242a' '\\u242f'\n",
      " '\\u2439' '─' '━' '│' '├' '┤' '┻' '┼' '═' '║' '╒' '╔' '╕' '╗' '╘' '╚' '╛'\n",
      " '╝' '╞' '╠' '╡' '╣' '╤' '╦' '╧' '╩' '╪' '╬' '╭' '╮' '╯' '▀' '▄' '▒' '■'\n",
      " '□' '▪' '▲' '△' '►' '▽' '◊' '☃' '☺' '♠' '♣' '♥' '♦' '♯' '⚽' '✓' '✔' '✗'\n",
      " '✘' '❌' '❤' '⟨' '⟩' '⟶' '⟺' '⩾' '\\u3000' '、' '。' '《' '》' '【' '】' '〖' '〗'\n",
      " 'あ' 'い' 'う' 'が' 'く' 'ご' 'ざ' 'し' 'た' 'て' 'と' 'ど' 'ひ' 'ま' 'み' 'め' 'も' 'よ'\n",
      " 'り' 'れ' 'を' 'イ' 'ズ' 'ツ' 'ベ' '・' 'ㅅ' '上' '事' '件' '你' '回' '处' '実' '帰' '形'\n",
      " '松' '枚' '植' '洪' '海' '湖' '物' '理' '皮' '盒' '第' '線' '花' '蛋' '装' '褒' '谢' '냅'\n",
      " '니' '다' '든' '들' '람' '랑' '모' '보' '사' '속' '시' '신' '십' '오' '완' '을' '의' '이'\n",
      " '쾌' '하' '히' '\\ue001' '\\uf03d' '\\uf04a' '\\uf071' '\\uf07d' '\\uf0b7'\n",
      " '\\uf0de' '\\uf0e0' '\\uf0e8' '\\uf0ed' '\\uf700' '\\uf72b' 'ﬀ' 'ﬁ' 'ﬂ' 'ﬃ' '️'\n",
      " '︵' '\\ufeff' '！' '（' '）' '＋' '，' '－' '：' '；' '？' '＠' 'ａ' 'ｂ' 'ｃ' 'ｄ' 'ｅ'\n",
      " 'ｆ' 'ｇ' 'ｈ' 'ｉ' 'ｋ' 'ｌ' 'ｍ' 'ｎ' 'ｏ' 'ｐ' 'ｑ' 'ｒ' 'ｓ' 'ｔ' 'ｕ' 'ｖ' 'ｗ' '｛'\n",
      " '｝' '￼' '�' '𝅘𝅥𝅮' '𝐘' '𝐰' '𝐱' '𝐲' '𝐴' '𝐵' '𝐶' '𝐷' '𝐸' '𝐹' '𝐺' '𝐻' '𝐼' '𝐾'\n",
      " '𝐿' '𝑀' '𝑁' '𝑃' '𝑄' '𝑅' '𝑆' '𝑇' '𝑈' '𝑉' '𝑊' '𝑋' '𝑌' '𝑍' '𝑎' '𝑏' '𝑐' '𝑑'\n",
      " '𝑒' '𝑓' '𝑔' '𝑖' '𝑗' '𝑘' '𝑙' '𝑚' '𝑛' '𝑜' '𝑝' '𝑞' '𝑟' '𝑠' '𝑡' '𝑢' '𝑣' '𝑤'\n",
      " '𝑥' '𝑦' '𝑧' '𝒩' '𝔼' '𝖯' '𝖳' '𝚯' '𝚽' '𝛂' '𝛔' '𝛼' '𝛽' '𝛿' '𝜀' '𝜃' '𝜆' '𝜇'\n",
      " '𝜈' '𝜋' '𝜎' '𝜒' '𝜔' '𝜖' '𝜙' '𝜷' '🎁' '🎈' '🎉' '🏻' '🏼' '👋' '👌' '👍' '👹' '💎'\n",
      " '💯' '🔥' '😀' '😁' '😂' '😃' '😅' '😉' '😊' '😎' '😔' '😕' '😛' '😜' '😝' '😩' '😬' '😭'\n",
      " '😳' '🙁' '🙂' '🤐' '🤔' '🤗' '🤣' '🤪' '🤷' '\\U00100000' '\\U0010fc00'\n",
      " '\\U0010fc01' '\\U0010fc03' '\\U0010fc04' '\\U0010fc06' '\\U0010fc10']\n",
      "[(' ', 41838479), ('e', 22649492), ('t', 18848027), ('a', 15614041), ('i', 15307965), ('o', 14812551), ('s', 13076523), ('n', 12756881), ('r', 11311575), ('h', 8004404), ('l', 7935954), ('d', 6606091), ('u', 6013733), ('c', 5874121), ('m', 5307240), ('f', 4206025), ('p', 4142895), ('y', 3538913), ('g', 3390453), ('w', 2959613), ('b', 2934463), ('.', 2818634), ('v', 2193287), (',', 2137165), ('k', 1184558), ('-', 949680), ('x', 721059), (\"'\", 627955), ('?', 430168), ('q', 420156), ('z', 216752), ('j', 214419), ('!', 65816), ('\\\\', 48970), ('’', 17695), ('‘', 9226), ('–', 5094), ('═', 2808), ('—', 1513), ('β', 1284), ('−', 1073), ('é', 957), ('σ', 894), ('…', 836), ('´', 827), ('θ', 801), ('μ', 715), ('α', 704), ('║', 657), ('²', 600), ('─', 507), ('λ', 405), ('\\xa0', 371), ('×', 321), ('±', 321), ('°', 307), ('•', 307), ('\\u200b', 298), ('ö', 296), ('≤', 239), ('ü', 237), ('ε', 231), ('δ', 202), ('£', 195), ('π', 184), ('\\xad', 184), ('\\t', 180), ('γ', 173), ('µ', 169), ('∗', 169), ('ﬁ', 168), ('¿', 165), ('а', 162), ('χ', 160), ('н', 153), ('ï', 150), ('е', 145), ('и', 145), ('ρ', 144), ('≠', 142), ('о', 142), ('√', 139), ('∑', 138), ('·', 138), ('á', 136), ('ä', 134), ('→', 131), ('⋅', 130), ('✗', 122), ('à', 118), ('∼', 114), ('∞', 110), ('≥', 108), ('∩', 108), ('≈', 97), ('ϵ', 96), ('è', 96), ('ω', 96), ('т', 94), ('‐', 92), ('ß', 91), ('с', 91), ('φ', 88), ('τ', 88), ('€', 81), ('í', 77), ('′', 77), ('å', 75), ('ó', 74), ('𝑥', 73), ('р', 73), ('½', 72), ('𝛽', 72), ('\\u200e', 71), ('∈', 70), ('л', 69), ('ı', 69), ('ç', 69), ('ϕ', 68), ('¯', 68), ('\\u200c', 68), ('«', 67), ('╬', 67), ('к', 65), ('„', 64), ('»', 64), ('ø', 63), ('§', 62), ('η', 62), ('│', 62), ('в', 61), ('𝑡', 59), ('╦', 59), ('╩', 59), ('º', 56), ('〖', 54), ('〗', 54), ('𝑋', 53), ('д', 51), ('̂', 50), ('ˆ', 50), ('✓', 48), ('∂', 45), ('†', 44), ('š', 43), ('³', 43), ('ы', 41), ('ł', 39), ('，', 37), ('м', 37), ('п', 37), ('𝑌', 37), ('ﬀ', 36), ('у', 36), ('？', 35), ('∣', 34), ('\\u3000', 34), ('♦', 33), ('𝐸', 33), ('̅', 33), ('ã', 32), ('ñ', 31), ('ч', 30), ('я', 29), ('̄', 28), ('κ', 27), ('х', 27), ('𝑛', 27), ('𝑦', 26), ('ë', 26), ('ξ', 26), ('ν', 25), ('¹', 25), ('\\u242a', 25), ('₁', 24), ('й', 24), ('╠', 24), ('╣', 24), ('®', 24), ('𝜆', 24), ('¬', 23), ('¦', 23), ('\\u2061', 22), ('∫', 22), ('з', 22), ('（', 22), ('╔', 22), ('╗', 22), ('╚', 22), ('╝', 22), ('𝑝', 22), ('𝑖', 22), ('ο', 22), ('♠', 22), ('ô', 21), ('г', 21), ('ş', 21), ('┼', 20), ('ê', 19), ('ь', 19), ('）', 19), ('ι', 19), ('⊥', 18), ('˜', 18), ('ﬂ', 17), ('𝜀', 17), ('𝑎', 17), ('ς', 17), ('𝑒', 16), ('₂', 16), ('б', 16), ('∝', 16), ('©', 16), ('ć', 15), ('∆', 15), ('𝐵', 15), ('ᵢ', 15), ('ツ', 14), ('⟨', 14), ('⟩', 14), ('―', 14), ('𝑟', 14), ('、', 14), ('̶', 14), ('ŷ', 13), ('：', 13), ('𝑃', 13), ('â', 13), ('ц', 13), ('‡', 13), ('𝑏', 13), ('∇', 12), ('ю', 12), ('𝑠', 12), ('́', 12), ('÷', 12), ('∪', 12), ('𝐹', 12), ('𝐼', 12), ('🤗', 12), ('洪', 12), ('湖', 12), ('‖', 12), ('ɛ', 12), ('⇒', 11), ('𝑇', 11), ('ψ', 11), ('\\x0c', 11), ('☺', 11), ('𝑁', 11), ('⋯', 11), ('👍', 11), ('𝜃', 11), ('ℝ', 11), ('ő', 11), ('ф', 11), ('ō', 11), ('¥', 11), ('\\uf0b7', 11), ('₃', 10), ('ш', 10), ('ú', 10), ('😊', 10), ('ə', 10), ('ń', 10), ('𝑚', 10), ('ù', 10), ('←', 10), ('𝑓', 9), ('ﬃ', 9), ('₀', 9), ('𝛼', 9), ('ý', 9), ('ì', 9), ('😀', 9), ('𝑙', 9), ('𝑜', 9), ('¢', 9), ('č', 9), ('ل', 9), ('∀', 9), (';', 9), ('∅', 9), ('ē', 9), ('⅓', 9), ('ｓ', 9), ('ａ', 9), ('𝑘', 8), ('ū', 8), ('├', 8), ('┤', 8), ('𝜇', 8), ('⁰', 8), ('▒', 8), ('ا', 8), ('ɪ', 8), ('ˈ', 8), ('🙂', 8), ('😜', 8), ('î', 8), ('έ', 8), ('ℓ', 8), ('\\u2060', 8), ('ｏ', 8), ('ｅ', 8), ('𝜋', 7), ('𝜎', 7), ('ɑ', 7), ('⁄', 7), ('י', 7), ('╤', 7), ('╧', 7), ('𝑢', 7), ('𝑧', 7), ('⁴', 7), ('‑', 7), ('𝑑', 7), ('𝐱', 7), ('⊂', 7), ('！', 7), ('٪', 7), ('ж', 7), ('\\u202f', 7), ('△', 7), ('˚', 7), ('ă', 7), ('𝐴', 7), ('𝔼', 7), ('️', 6), ('ę', 6), ('❌', 6), ('ב', 6), ('э', 6), ('æ', 6), ('\\u2002', 6), ('⁶', 6), ('ₙ', 6), ('₋', 6), ('✔', 6), ('松', 6), ('花', 6), ('皮', 6), ('蛋', 6), ('枚', 6), ('盒', 6), ('ğ', 6), ('ό', 6), ('ί', 6), ('̇', 6), ('υ', 6), ('处', 6), ('理', 6), ('𝐿', 6), ('ₓ', 6), ('ｈ', 6), ('¶', 5), ('\\U00100000', 5), ('𝑗', 5), ('😅', 5), ('￼', 5), ('م', 5), ('ي', 5), ('¼', 5), ('。', 5), ('𝜙', 5), ('𝑤', 5), ('̃', 5), ('‚', 5), ('ά', 5), ('😛', 5), ('ἐ', 5), ('𝑞', 5), ('ś', 5), ('⟶', 5), ('ｌ', 5), ('ｄ', 5), ('ｒ', 5), ('《', 4), ('𝐾', 4), ('━', 4), ('ā', 4), ('╒', 4), ('╕', 4), ('╘', 4), ('╛', 4), ('ή', 4), ('ƒ', 4), ('𝑔', 4), ('ח', 4), ('ר', 4), ('ה', 4), ('ð', 4), ('\\u2009', 4), ('う', 4), ('😁', 4), ('œ', 4), ('û', 4), ('谢', 4), ('\\ufeff', 4), ('⁷', 4), ('∨', 4), ('👌', 4), ('✘', 4), ('𝐰', 4), ('ẁ', 4), ('♥', 4), ('⊤', 4), ('\\uf071', 4), ('♣', 4), ('″', 4), ('⅔', 4), ('⏜', 4), ('∶', 4), ('ｕ', 4), ('ｔ', 4), ('¡', 3), ('\\x0b', 3), ('■', 3), ('ύ', 3), ('⁻', 3), ('𝐶', 3), ('¾', 3), ('𝛿', 3), ('℃', 3), ('≡', 3), ('ϐ', 3), ('ع', 3), ('ت', 3), ('\\uf0e8', 3), ('て', 3), ('є', 3), ('ž', 3), ('∕', 3), ('™', 3), ('̈', 3), ('˙', 3), ('\\U0010fc03', 3), ('ẃ', 3), ('ⁿ', 3), ('∉', 3), ('�', 3), ('≫', 3), ('\\u2439', 3), ('𝚽', 3), ('õ', 3), ('・', 3), ('𝑉', 3), ('사', 3), ('【', 3), ('】', 3), ('ὑ', 3), ('ｉ', 3), ('ｍ', 3), ('ｆ', 3), ('》', 2), ('𝐺', 2), ('\\uf0e0', 2), ('⎢', 2), ('⎥', 2), ('∘', 2), ('ĺ', 2), ('❤', 2), ('╯', 2), ('┻', 2), ('ɣ', 2), ('\\uf04a', 2), ('⇔', 2), ('ע', 2), ('ק', 2), ('ת', 2), ('א', 2), ('ו', 2), ('כ', 2), ('ל', 2), ('מ', 2), ('ד', 2), ('נ', 2), ('⁹', 2), ('𝑐', 2), ('𝜷', 2), ('◊', 2), ('ò', 2), ('ů', 2), ('∏', 2), ('≃', 2), ('ȳ', 2), ('№', 2), ('ª', 2), ('س', 2), ('ك', 2), ('و', 2), ('ر', 2), ('ح', 2), ('ة', 2), ('ه', 2), ('ن', 2), ('∃', 2), ('𝐻', 2), ('＠', 2), ('あ', 2), ('り', 2), ('が', 2), ('と', 2), ('𝜖', 2), ('ī', 2), ('\\uf72b', 2), ('🤔', 2), ('↦', 2), ('\\x7f', 2), ('̀', 2), ('🤪', 2), ('𝅘𝅥𝅮', 2), ('‾', 2), ('ώ', 2), ('ὁ', 2), ('し', 2), ('¨', 2), ('⊆', 2), ('ζ', 2), ('ъ', 2), ('𝜔', 2), ('\\u200f', 2), ('⁎', 2), ('ῦ', 2), ('᾽', 2), ('𝐲', 2), ('ಠ', 2), ('↵', 2), ('⅕', 2), ('\\u242f', 2), ('\\ue001', 2), ('▪', 2), ('😉', 2), ('𝑣', 2), ('ẋ', 2), ('ą', 2), ('回', 2), ('ˌ', 2), ('람', 2), ('들', 2), ('｛', 2), ('｝', 2), ('ₐ', 2), ('\\U0010fc04', 2), ('\\U0010fc06', 2), ('▽', 2), ('ơ', 2), ('ｐ', 2), ('ｃ', 2), ('ｎ', 2), ('ʊ', 2), ('ʒ', 2), ('ț', 1), ('𝖯', 1), ('𝖳', 1), ('⎡', 1), ('⎤', 1), ('⎣', 1), ('⎦', 1), ('ɵ', 1), ('\\u200a', 1), ('□', 1), ('︵', 1), ('̧', 1), ('╞', 1), ('╪', 1), ('╡', 1), ('☃', 1), ('🎉', 1), ('🎁', 1), ('🎈', 1), ('🤣', 1), ('𝑆', 1), ('\\u2005', 1), ('🏼', 1), ('ƴ', 1), ('\\uf03d', 1), ('😕', 1), ('▄', 1), ('▀', 1), ('𝜒', 1), ('👹', 1), ('ب', 1), ('ى', 1), ('‰', 1), ('𝑄', 1), ('𝑅', 1), ('褒', 1), ('め', 1), ('く', 1), ('れ', 1), ('ʃ', 1), ('ĕ', 1), ('𝛔', 1), ('😃', 1), ('̸', 1), ('►', 1), ('⊙', 1), ('💎', 1), ('𝐘', 1), ('💯', 1), ('上', 1), ('海', 1), ('\\U0010fc10', 1), ('ř', 1), ('\\uf07d', 1), ('\\uf0ed', 1), ('\\U0010fc01', 1), ('\\U0010fc00', 1), ('℗', 1), ('𝑊', 1), ('😔', 1), ('↑', 1), ('ṓ', 1), ('ど', 1), ('も', 1), ('ご', 1), ('ざ', 1), ('い', 1), ('ま', 1), ('た', 1), ('你', 1), ('⍺', 1), ('🤐', 1), ('⟺', 1), ('𝛂', 1), ('🔥', 1), ('🤷', 1), ('⁺', 1), ('ℙ', 1), ('𝑈', 1), ('⩾', 1), ('∴', 1), ('ὶ', 1), ('ἀ', 1), ('ὲ', 1), ('ῖ', 1), ('ἕ', 1), ('ὅ', 1), ('ἡ', 1), ('ˇ', 1), ('𝚯', 1), ('😎', 1), ('ṽ', 1), ('╭', 1), ('╮', 1), ('；', 1), ('植', 1), ('物', 1), ('₅', 1), ('₄', 1), ('ʼ', 1), ('≪', 1), ('⁸', 1), ('ῡ', 1), ('\\uf0de', 1), ('ё', 1), ('👋', 1), ('𝐷', 1), ('ź', 1), ('𝑍', 1), ('ひ', 1), ('پ', 1), ('ے', 1), ('－', 1), ('🏻', 1), ('😝', 1), ('≏', 1), ('第', 1), ('ベ', 1), ('イ', 1), ('ズ', 1), ('線', 1), ('形', 1), ('帰', 1), ('を', 1), ('実', 1), ('装', 1), ('み', 1), ('よ', 1), ('ℤ', 1), ('⃗', 1), ('𝑀', 1), ('♯', 1), ('щ', 1), ('𝜈', 1), ('؟', 1), ('‽', 1), ('\\uf700', 1), ('事', 1), ('件', 1), ('＋', 1), ('😳', 1), ('신', 1), ('속', 1), ('히', 1), ('완', 1), ('쾌', 1), ('하', 1), ('십', 1), ('시', 1), ('오', 1), ('의', 1), ('모', 1), ('든', 1), ('이', 1), ('랑', 1), ('을', 1), ('보', 1), ('냅', 1), ('니', 1), ('다', 1), ('∓', 1), ('ㅅ', 1), ('₹', 1), ('𝒩', 1), ('ˉ', 1), ('🙁', 1), ('∥', 1), ('͝', 1), ('≰', 1), ('ℱ', 1), ('΄', 1), ('ₑ', 1), ('ₔ', 1), ('ℒ', 1), ('⁵', 1), ('⚽', 1), ('⁽', 1), ('▲', 1), ('‟', 1), ('ė', 1), ('😂', 1), ('ļ', 1), ('\\u202c', 1), ('😭', 1), ('😬', 1), ('😩', 1), ('ｇ', 1), ('ｗ', 1), ('ｑ', 1), ('ｂ', 1), ('ｖ', 1), ('ｋ', 1), ('∠', 1), ('⇝', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all the original texts from the dataset and list the unique \n",
    "text = ''.join(df_raw.text.values).lower()\n",
    "\n",
    "# simple but efficient way to split string into list of characters\n",
    "all_characters = [s for s in text]\n",
    "unique_characters = np.unique(all_characters) \n",
    "print(unique_characters)\n",
    "\n",
    "char_count = Counter(all_characters)\n",
    "print(char_count.most_common())\n",
    "\n",
    "# limit the allowed characters to MAX_VOCAB_SIZE\n",
    "MAX_VOCAB_SIZE = 40\n",
    "valid_characters = [t[0] for t in  char_count.most_common(MAX_VOCAB_SIZE)]\n",
    "valid_characters.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eca142",
   "metadata": {},
   "source": [
    "## Subsample the dataset to take into account the titles only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1a4771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:  (10000, 7)\n",
      "['Dice probability for Yahtzee large straight'\n",
      " 'validation of a Zero Adjusted Gamma model']\n"
     ]
    }
   ],
   "source": [
    "POSTS_TYPE = 'title'\n",
    "DF_SAMPLE_COUNT = 10000\n",
    "\n",
    "# subsample the original dataset\n",
    "\n",
    "df = df_raw[(df_raw.category == POSTS_TYPE)].sample(DF_SAMPLE_COUNT).reset_index(drop = True)\n",
    "\n",
    "print(\"df.shape: \", df.shape)\n",
    "\n",
    "print(df.text.sample(2).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb188b4",
   "metadata": {},
   "source": [
    "## Implement the tokenization of the dataset and transform the tokens into AllenNLP Instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55598e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterTokenizer()\n",
    "train_set = df.text.apply(lambda txt : tokenizer.tokenize(txt.lower())).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46c877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_instance(tokens: List[Token], token_indexers: Dict[str, TokenIndexer]):\n",
    "    tokens = list(tokens)\n",
    "    tokens.insert(0, Token(START_SYMBOL))\n",
    "    tokens.append(Token(END_SYMBOL))\n",
    "\n",
    "    input_field  = TextField(tokens[:-1], token_indexers)\n",
    "    output_field = TextField(tokens[1:], token_indexers)\n",
    "    return Instance({'input_tokens': input_field, 'output_tokens': output_field})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd73dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "instances = [tokens_to_instance(tokens, token_indexers) for tokens in train_set]\n",
    "token_counts = {char: 1 for char in valid_characters}\n",
    "vocab = Vocabulary({'tokens': token_counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d14e1",
   "metadata": {},
   "source": [
    "## Design a RNN with AllenNLP that includes:\n",
    "- an embedding of the tokens,\n",
    "- a seq2seq LSTM layer,\n",
    "- a feed forward layer that outputs probability distribution of the characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac1094",
   "metadata": {},
   "source": [
    "AllenNLP is a modelling framework which is capable of many types of models. Most of the state of the art results in academia you will see are variations on Transformer networks which are not directly covered in this course but are available within AllenNLP. LSTMs and Seq2Seq models are still very competitive and widely used. In industry however you have to weigh up a lot of things to decide what the best model is; size of the dataset, how long you have to train/fine tune the model, infrastructure available, how the model will be served, what an acceptable level of accuracy is etc. Normally you start with a very simple model and gradually try more complex models until you reach an acceptable level of performance. It is usually a mistake to jump straight to an LSTM or Transformer model without having simpler baselines to benchmark them against and you will be surprised how often a simple model will achieve the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6294f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e185b20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "else:\n",
    "    cuda_device = -1\n",
    "    \n",
    "cuda_device == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e957d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Model.register('rnn_language_model')\n",
    "class RNNLanguageModel(Model):\n",
    "    def __init__(self,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 hidden_size: int,\n",
    "                 max_len: int,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "\n",
    "        self.embedder = embedder\n",
    "\n",
    "        # initialize a Seq2Seq encoder, LSTM\n",
    "        self.rnn = PytorchSeq2SeqWrapper(\n",
    "            torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))\n",
    "\n",
    "        self.hidden2out = torch.nn.Linear(in_features=self.rnn.get_output_dim(), out_features=vocab.get_vocab_size('tokens'))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, input_tokens, output_tokens):\n",
    "        '''\n",
    "        This is the main process of the Model where the actual computation happens. \n",
    "        Each Instance is fed to the forward method. \n",
    "        It takes dicts of tensors as input, with same keys as the fields in your Instance (input_tokens, output_tokens)\n",
    "        It outputs the results of predicted tokens and the evaluation metrics as a dictionary. \n",
    "        '''\n",
    "\n",
    "        mask = get_text_field_mask(input_tokens)\n",
    "        embeddings = self.embedder(input_tokens)\n",
    "        rnn_hidden = self.rnn(embeddings, mask)\n",
    "        out_logits = self.hidden2out(rnn_hidden)\n",
    "        loss = sequence_cross_entropy_with_logits(out_logits, output_tokens['tokens'], mask)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def generate(self) -> Tuple[List[Token], torch.tensor]:\n",
    "\n",
    "        start_symbol_idx = self.vocab.get_token_index(START_SYMBOL, 'tokens')\n",
    "        end_symbol_idx = self.vocab.get_token_index(END_SYMBOL, 'tokens')\n",
    "        padding_symbol_idx = self.vocab.get_token_index(DEFAULT_PADDING_TOKEN, 'tokens')\n",
    "\n",
    "        log_likelihood = 0.\n",
    "        words = []\n",
    "        state = (torch.zeros(1, 1, self.hidden_size).to(cuda_device), torch.zeros(1, 1, self.hidden_size).to(cuda_device))\n",
    "\n",
    "        word_idx = start_symbol_idx\n",
    "\n",
    "        for i in range(self.max_len):\n",
    "            tokens = torch.tensor([[word_idx]]).to(cuda_device)\n",
    "\n",
    "            embeddings = self.embedder({'tokens': tokens})\n",
    "            output, state = self.rnn._module(embeddings, state)\n",
    "            output = self.hidden2out(output)\n",
    "\n",
    "            log_prob = torch.log_softmax(output[0, 0], dim=0)\n",
    "\n",
    "            dist = torch.exp(log_prob)\n",
    "\n",
    "            word_idx = start_symbol_idx\n",
    "\n",
    "            while word_idx in {start_symbol_idx, padding_symbol_idx}:\n",
    "                word_idx = torch.multinomial(\n",
    "                   dist, num_samples=1, replacement=False).item()\n",
    "\n",
    "            log_likelihood += log_prob[word_idx]\n",
    "\n",
    "            if word_idx == end_symbol_idx:\n",
    "                break\n",
    "\n",
    "            token = Token(text=self.vocab.get_token_from_index(word_idx, 'tokens'))\n",
    "            words.append(token)\n",
    "\n",
    "        return words, log_likelihood    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a120e26",
   "metadata": {},
   "source": [
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73f4a101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a4b9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=EMBEDDING_SIZE)\n",
    "\n",
    "embedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "model = RNNLanguageModel(embedder=embedder, hidden_size=HIDDEN_SIZE, max_len=80, vocab=vocab)\n",
    "\n",
    "if cuda_device == 0:\n",
    "    model.to(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "886f2bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c50b7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BasicIterator(batch_size=BATCH_SIZE)\n",
    "iterator.index_with(vocab)\n",
    "optimizer = Adam(model.parameters(), lr=5.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "987ceaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.9274 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.94it/s]\n",
      "loss: 0.9187 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.91it/s]\n",
      "loss: 0.9148 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.90it/s]\n",
      "loss: 0.9087 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.91it/s]\n",
      "loss: 0.8989 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.82it/s]\n",
      "loss: 0.8944 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.81it/s]\n",
      "loss: 0.8870 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.75it/s]\n",
      "loss: 0.8830 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.77it/s]\n",
      "loss: 0.8785 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.85it/s]\n",
      "loss: 0.8732 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.74it/s]\n",
      "loss: 0.8689 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.92it/s]\n",
      "loss: 0.8651 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.80it/s]\n",
      "loss: 0.8613 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.72it/s]\n",
      "loss: 0.8551 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.72it/s]\n",
      "loss: 0.8518 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.74it/s]\n",
      "loss: 0.8451 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.75it/s]\n",
      "loss: 0.8421 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.84it/s]\n",
      "loss: 0.8380 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.67it/s]\n",
      "loss: 0.8370 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.87it/s]\n",
      "loss: 0.8334 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.87it/s]\n",
      "loss: 0.8296 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.77it/s]\n",
      "loss: 0.8252 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.72it/s]\n",
      "loss: 0.8219 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.81it/s]\n",
      "loss: 0.8191 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.73it/s]\n",
      "loss: 0.8155 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.85it/s]\n",
      "loss: 0.8130 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.96it/s]\n",
      "loss: 0.8117 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.90it/s]\n",
      "loss: 0.8137 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.87it/s]\n",
      "loss: 0.8067 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.80it/s]\n",
      "loss: 0.8033 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.92it/s]\n",
      "loss: 0.8023 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 12.00it/s]\n",
      "loss: 0.8001 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.84it/s]\n",
      "loss: 0.7963 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.94it/s]\n",
      "loss: 0.7980 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.88it/s]\n",
      "loss: 0.7928 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.94it/s]\n",
      "loss: 0.7909 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.86it/s]\n",
      "loss: 0.7889 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.90it/s]\n",
      "loss: 0.7889 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7860 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.93it/s]\n",
      "loss: 0.7901 ||: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 11.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 39,\n",
       " 'peak_cpu_memory_MB': 6254.796,\n",
       " 'peak_gpu_0_memory_MB': 947,\n",
       " 'training_duration': '0:04:31.333810',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 39,\n",
       " 'epoch': 39,\n",
       " 'training_loss': 0.7900567213191262,\n",
       " 'training_cpu_memory_MB': 6254.796,\n",
       " 'training_gpu_0_memory_MB': 947}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=instances,\n",
    "                  num_epochs=40,\n",
    "                  cuda_device=cuda_device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a7de2",
   "metadata": {},
   "source": [
    "## Evaluate the model by calculating the loss of some sentences and by generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60e96ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text: str, model: Model) -> float:\n",
    "    tokenizer = CharacterTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "    instance = tokens_to_instance(tokens, token_indexers)\n",
    "    output = model.forward_on_instance(instance)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c022706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.600434}\n",
      "{'loss': 3.3273149}\n",
      "{'loss': 2.8697264}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In a fixed-effects model only time-varying variables can be used.\"\n",
    "predict(sentence, model)\n",
    "\n",
    "sentence = \"I know a pretty little place in Southern California, down San Diego way.\"\n",
    "predict(sentence, model)\n",
    "\n",
    "sentence = \"This that is noon but yes apple whatever did regression variable\"\n",
    "predict(sentence, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1b62d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining a percentage of a given of predictor with many parameter for categorica\n",
      "what is the generation of missing values? for rmsemeter' we dath when sloupher t\n",
      "is using chi square fich approaches that factors on many zero? pairwise comparis\n",
      "testing whether two coefficients and bernoulli wins for estimating proportion or\n",
      "how to truncated processes? that is not correctly detection? in stata model and \n",
      "how does the sampling distribution is jags? all zero-information criterion in r?\n",
      "how is test gensod for confidence intervals and nn? --gbf ? into dereige iter's?\n",
      "sample regression contribution and divisa-- sampled between pseudo allorized in \n",
      "model disiders, estimate of correlation? how to detex a ?? mistage accept or p-v\n",
      "as an unbiased mining where roc correct? in r with bayesian marginal polynomials\n",
      "calculate the extrement used, use adaptive distribution? often researched by sta\n",
      "adapting cross validation results? of the goodness of fit cale of sample is sign\n",
      "optimal outliers for the stationary productions? and spss? if after a gring and \n",
      "how do you calculate quality of one predictor variables are bound and engiccers?\n",
      "derivative of the right-smoothing, interpreting cdf? in its in reference for hum\n",
      "change in graph creation of objects? as akaike's eartiving time distinct boundar\n",
      "how to reproducal's explanation when be simplication? stability? how to visualiz\n",
      "how to use figure outliers in garch , models? how inever lihter if acf and bayes\n",
      "what are the equation s u squared difference in higher dataset at random variabl\n",
      "machine learning newtorm for unknuskal applications? is a gaussian? for bernoull\n",
      "how to cheige a classifier historon give some or other boons change? polygration\n",
      "gain every random effects model videarian?? do? or something mixtures? a surgari\n",
      "random factor analysis particisations? should i use for forecasts? is only consi\n",
      "proportional experimental stadsals when out credible interval over time when pea\n",
      "see to interpret design a tuble offise minimum likelihood ? in d re? and optimal\n",
      "model interned based on normal distributions in r? q show that ? introprodu? do \n",
      "variance of classification for pca in class-ordinal variable?! and for graphod i\n",
      "shinging about the median selection on the average and standard deviation? and t\n",
      "when modefformation before clustering vs back? to find heterosted?pended in regr\n",
      "can you correct interactions vs random forest classification of different datase\n",
      "interpreting coefficients from differences partial dependent variables? hidden a\n",
      "calculating product of acf price? suchne model in r, distribution these carlo pr\n",
      "sequential structural bandit algorithm function, based and, alma? - thind zero? \n",
      "visual true and transform or not ways algorithms?? independent variables? mutula\n",
      "a friedman test when does't know effect size of the p value of increments half a\n",
      "false possible to high low p-value when than my residuals transformation? and st\n",
      "cursed confidence markov models defined setting increasing data in wect predicto\n",
      "p-values for training affect lase? and k-meight length? ranking? from statistica\n",
      "how to use this similarition density function derived 'e'mand? in my one cauchy?\n",
      "calculates from a number of energy scipy which pacf? variance matrix parameters \n",
      "hught plot for regressions with rth or cumel called? neural network? in a gaussi\n",
      "is number of a random effect in mcmmanor? as only if two degreating probability \n",
      "training a -kean regression? fails tophing conditionals? in r? and sales negativ\n",
      "distribution of neural networks have?? can i mean-than repeated measures anova w\n",
      "how should i use the actual process liment rate? ? updating that p a ? gymous se\n",
      "appropriate matrix for conditional expectation and distributions mixed model - ?\n",
      "number of participants using bayesian network faulty? how to calculate predictio\n",
      "help with similarity of correlation? summation? having transforming models that'\n",
      "using hele based on feature selection in machine learning by the begnes? how to \n",
      "in whoritative better processes for the ridge regression gaussian process and sp\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    tokens, _ = model.generate()\n",
    "    print(''.join(token.text for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b858ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
