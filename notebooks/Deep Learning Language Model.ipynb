{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16803e2",
   "metadata": {},
   "source": [
    "# Deep Learning Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f99a2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/michael/.pyenv/versions/3.9-dev/envs/nlpenv/bin/python3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0024c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fb3f7",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f87de",
   "metadata": {},
   "source": [
    "### Load the dataset that was prepared in task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "26625749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup variables\n",
    "POSTS_TYPE = 'post'\n",
    "MIN_TOKENS_LEN = 100\n",
    "MAX_TOKENS_LEN = 200\n",
    "DF_SAMPLE_COUNT = 20000\n",
    "\n",
    "TOKENS_MIN_COUNT = 10\n",
    "\n",
    "SEQUENCE_WINDOW = 4\n",
    "SEQUENCE_LEN = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "62ba188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_csv(\"../data/stackexchange_812k.tokenized.csv\").sample(frac=1, random_state=8).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a431602",
   "metadata": {},
   "source": [
    "The original dataset is too large and needs to be reduced. To reduce it, you can, for instance,\n",
    "filter out items that have too many or too little tokens,\n",
    "select items of a certain type: post, comments, or titles, or\n",
    "or sub sample items randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "48426214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comment', 'post', 'title'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "318c923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:  (20000, 7)\n",
      "[\"I ran a linear regression of acceptance into college against SAT scores and family ethnic background. The data are fictional. This is a follow-up on a prior question, already answered. The question focuses in the gathering and interpretation of odds ratios when leaving the SAT scores aside for simplicity. The variables are Accepted or and Background red or blue . I set up the data so that people of red background were more likely to get in fit lt - glm Accepted Background, data dat, family binomial exp cbind Odds Ratio RedvBlue coef fit , confint fit Odds Ratio RedvBlue . . Intercept . . . Backgroundred . . . Questions Is . the odd ratio of a person of blue background being accepted? I'm asking this because I also get . for Backgroundblue if instead I run the following code fit lt - glm Accepted Background- , data dat, family binomial exp cbind OR coef fit , confint fit Shouldn't the odds ratio of red being accepted ?\"\n",
      " \"Neither is appropriate! The usual ANOVA assumptions include constant variance among other things . This assumption won't generally be satisfied by count data with varying mean. In addition, with small expected counts you will have a very skewed, discrete distribution that won't be anywhere close to normal. A chi-squared test should be performed on counts, not scaled counts -- the variance will be wrong if you scale it like that. To adjust for an exposure variable like road length you don't divide by it, you use an offset of log-exposure with a log link in a GLM. This could be reasonable with a Poisson or a negative binomial response for example.\"]\n"
     ]
    }
   ],
   "source": [
    "df = df_raw[\n",
    "            (df_raw.category == POSTS_TYPE) & \n",
    "            (df_raw.n_tokens > MIN_TOKENS_LEN)  & \n",
    "            (df_raw.n_tokens < MAX_TOKENS_LEN)\n",
    "        ].sample(DF_SAMPLE_COUNT).reset_index(drop = True)\n",
    "\n",
    "print(\"df.shape: \", df.shape)\n",
    "print(df.text.sample(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b83a97ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['there', 'are', 'some', 'problematic', 'assumptions', 'you',\n",
      "        'seem', 'to', 'be', 'making', '-', 'you', 'rarely', 'know',\n",
      "        'population', 'parameters', ',', 'but', 'those', 'are', 'not',\n",
      "        'necessary', 'to', 'standardize', '.', 'you', 'can', 'standardize',\n",
      "        'based', 'on', 'your', 'sample', '.', '-', 'why', 'would', 'you',\n",
      "        'calculate', 'percentiles', 'and', 'what', 'makes', 'you', 'think',\n",
      "        'you', 'can', 'just', 'sum', 'them', '?', 'if', 'you', 'are',\n",
      "        'just', 'interested', 'in', 'rank', 'that', 'may', 'be',\n",
      "        'appropriate', 'but', 'you', 'don', \"'\", 't', 'give', 'us',\n",
      "        'enough', 'to', 'go', 'on', 'there', '.', '-', 'why', 'do', 'you',\n",
      "        'want', 'to', 'some', 'separate', 'measures', '?', 'you', 'must',\n",
      "        'be', 'trying', 'to', 'get', 'at', 'an', 'underlying', 'construct',\n",
      "        'or', 'create', 'an', 'index', 'of', 'some', 'sort', '.',\n",
      "        'creating', 'a', 'single', 'scale', 'from', 'different', 'scales',\n",
      "        'is', 'a', 'problem', 'that', 'an', 'entire', 'field', 'of',\n",
      "        'statistics', 'addresses', 'psychometrics', '.', 'i', 'would',\n",
      "        'recommend', 'taking', 'a', 'look', 'at', 'some', 'basic', 'text',\n",
      "        'book', 'references', 'on', 'the', 'topic', '.', 'i', 'don', \"'\",\n",
      "        't', 'mean', 'to', 'sound', 'harsh', 'but', 'you', 'should',\n",
      "        'probably', 'provide', 'some', 'more', 'details', 'so', 'we',\n",
      "        'can', 'provide', 'more', 'guidance', '.'], dtype='<U13')          ]\n"
     ]
    }
   ],
   "source": [
    "# transform the tokens field from white space separated strings into list of tokens\n",
    "df['tokens'] = df.tokens.apply(lambda t : np.array(t.split()))\n",
    "print(df.tokens.sample().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ae037",
   "metadata": {},
   "source": [
    "Build the vocabulary as the set of all unique tokens to construct the list of token indexes.\n",
    "\n",
    "Filtering on token frequency is one way to reduce the overall size of the vocabulary.\n",
    "\n",
    "Set a fixed sequence length and build sequences of token indexes from the corpus. (See for instance keras pad_sequences.)\n",
    "\n",
    "Split the sequences into predictors and labels (keras.utils.to_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a5a06",
   "metadata": {},
   "source": [
    "Reduce the overall vocabulary size by excluding the tokens that appear less than the present TOKENS_MIN_COUNT overall number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a0f48ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of tokens 2867753\n",
      "original vocab_size 32799\n",
      "new number of tokens 2804990\n",
      "new vocab_size 7256\n"
     ]
    }
   ],
   "source": [
    "#generate vocabulary\n",
    "#filter out words that are too scarce\n",
    "import itertools\n",
    "all_tokens = list(itertools.chain.from_iterable(df.tokens))\n",
    "\n",
    "#filter out least common tokens\n",
    "from collections import Counter\n",
    "counter_tokens = Counter(all_tokens)\n",
    "\n",
    "vocab_size  = len(set(all_tokens))\n",
    "vocab       = list(set(all_tokens))\n",
    "print(\"original number of tokens\", len(all_tokens))\n",
    "print(\"original vocab_size\", vocab_size)\n",
    "\n",
    "#remove all tokens that appear in less than TOKENS_MIN_COUNT times\n",
    "fltrd_tokens = [ token for token in all_tokens if counter_tokens[token] > TOKENS_MIN_COUNT ]\n",
    "\n",
    "print(\"new number of tokens\", len(fltrd_tokens))\n",
    "print(\"new vocab_size\", len(set(fltrd_tokens)))\n",
    "\n",
    "vocab_size  = len(set(fltrd_tokens))\n",
    "vocab       = list(set(fltrd_tokens))\n",
    "vocab.append('UNK')\n",
    "vocab_size +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d7765",
   "metadata": {},
   "source": [
    "Replace the missing tokens with a specific token to handle out of vocabulary tokens and create token indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7ba45717",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { w : i for i, w in enumerate(vocab) }\n",
    "\n",
    "def getidx(token):\n",
    "    try:\n",
    "        return mapping[token]\n",
    "    except:\n",
    "        return mapping['UNK']\n",
    "\n",
    "df['tokens_idx'] = df.tokens.apply(lambda tokens : np.array([getidx(token) for token in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a5ae7",
   "metadata": {},
   "source": [
    "Prepare the corpus as input to the neural network to train the model.\n",
    "\n",
    "Set a fixed sequence length and build sequences of token indexes from the corpus. (See for instance Keras pad_sequences).m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "23ec10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Generate sequences\n",
    "def generate_sequences(sentence):\n",
    "    sequences = []\n",
    "    _end = SEQUENCE_WINDOW\n",
    "    while _end < len(sentence) + SEQUENCE_WINDOW:\n",
    "        sequences.append(sentence[:_end])\n",
    "        _end += SEQUENCE_WINDOW\n",
    "    padded_seqs = pad_sequences(sequences, maxlen=SEQUENCE_LEN, padding='pre')\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "32578f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sequences = df.tokens_idx.apply(generate_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7399cf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:37<00:00, 531.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sequences.shape:  (724555, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for d in tqdm(multi_sequences.values):\n",
    "    if i == 0:\n",
    "        all_sequences = d\n",
    "    else:\n",
    "        all_sequences = np.concatenate( ( all_sequences, d )  )\n",
    "    i +=1\n",
    "print(\"\\nsequences.shape: \",all_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aeee61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:    \n",
    "    mask = np.random.choice([False, True], len(all_sequences), p=[0.50, 0.50])\n",
    "    sequences = all_sequences[mask].copy()\n",
    "else:\n",
    "    sequences = all_sequences.copy()\n",
    "    print(\"\\nsequences.shape: \",sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "647dd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the sequences into predictors and labels\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b80facb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictors.shape (362220, 12)\n",
      "label.shape (362220,)\n",
      "label_cat.shape (362220, 7257)\n"
     ]
    }
   ],
   "source": [
    "predictors  = sequences[:,:-1]\n",
    "label       = sequences[:,-1]\n",
    "\n",
    "print(\"predictors.shape\", predictors.shape)\n",
    "print(\"label.shape\", label.shape)\n",
    "\n",
    "# The to_categorical Keras function transforms the vocab_size vector of labels into a one hot encoded matrix of dimension (n, vocab_size)\n",
    "label_cat       = to_categorical(label, num_classes=vocab_size)\n",
    "\n",
    "print(\"label_cat.shape\", label_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accc415",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf215c",
   "metadata": {},
   "source": [
    " The data is now ready to be used to fit a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac2b4f",
   "metadata": {},
   "source": [
    "Define and train a simple sequential model with an embedding dimension (32, 64, …), 2 LSTM layers followed by a dense layer with softmax activation.\n",
    "\n",
    "The optimizer is RMSprop with a learning rate of 0.01\n",
    "\n",
    "Specify the number of epochs, the batch size, and other fitting parameters.\n",
    "\n",
    "Experiment with dropouts, different optimizers. \n",
    "\n",
    "Use any type of neural net; for example, Keras, TensorFlow, PyTorch, and so on.\n",
    "\n",
    "Fit the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a694e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2c1adf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 12, 64)            464448    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 12, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7257)              471705    \n",
      "=================================================================\n",
      "Total params: 1,084,377\n",
      "Trainable params: 1,084,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 15:34:27.670093: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 10514522160 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1415/1415 [==============================] - 67s 46ms/step - loss: 5.5652 - accuracy: 0.1404\n",
      "Epoch 2/4\n",
      "1415/1415 [==============================] - 65s 46ms/step - loss: 5.3450 - accuracy: 0.1848\n",
      "Epoch 3/4\n",
      "1415/1415 [==============================] - 74s 52ms/step - loss: 5.3181 - accuracy: 0.1975\n",
      "Epoch 4/4\n",
      "1415/1415 [==============================] - 74s 52ms/step - loss: 5.2997 - accuracy: 0.2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49ef646e50>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "embedding_dimension = 64\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(vocab_size,\n",
    "        embedding_dimension,\n",
    "        input_length=SEQUENCE_LEN -1)\n",
    "    )\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Model Fitting!\n",
    "model.fit(predictors, label_cat, batch_size = 256, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c1ba1",
   "metadata": {},
   "source": [
    "## Assessing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efe694",
   "metadata": {},
   "source": [
    "Write a function that generates text. Generate some text and take note of:\n",
    "- Token repetitions\n",
    "- Missing punctuations\n",
    "- Other anomalies\n",
    "\n",
    "Write a function that calculates perplexity of a sentence and apply it to a subset of sentences to evaluate the model.\n",
    "\n",
    "Define a validation set; for instance, 1000 titles.\n",
    "\n",
    "Transform that validation set into sequences of tokens using the training vocabulary.\n",
    "\n",
    "Tune the neural net and the parameters of the preprocessing phase to improve the model’s perplexity score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91eade",
   "metadata": {},
   "source": [
    "### A function that generates text, that takes care of - Token repetitions - Missing punctuations - Other anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3ca1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "32f8ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "983cba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Sequential.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c94660c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(nmax, text, temperature):\n",
    "    n = 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    while (len(tokens) < nmax) :\n",
    "        n +=1\n",
    "        # only takes known words into account\n",
    "        tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "        # print(tokens_idx)\n",
    "        tokens_list = pad_sequences([tokens_idx], maxlen=SEQUENCE_LEN-1, padding='pre')\n",
    "#         probas = model.predict_proba(tokens_list, verbose=0)[0]\n",
    "        probas = model.predict(tokens_list, verbose=0)[0]\n",
    "        next_word_idx = sample(probas, temperature = temperature)\n",
    "        next_word = vocab[next_word_idx]\n",
    "        # print(next_word_idx, next_word)\n",
    "\n",
    "        # next_word = np.random.choice(vocab, p = probas)\n",
    "        if next_word != '?':\n",
    "            print(next_word, probas[vocab.index(next_word)]  )\n",
    "            text += ' ' + next_word\n",
    "        # print(text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if n> 200:\n",
    "            break;\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9dd7260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK 0.0041229264\n",
      "gt 0.00014356604\n",
      "tseries 1.9241901e-07\n",
      "phone 1.097229e-06\n",
      "on 0.0038718362\n",
      "formula 0.00019257527\n",
      "eb 1.5505184e-06\n",
      "adjacent 1.7961676e-07\n",
      "f 0.002288006\n",
      "abs 1.04362525e-05\n",
      "diagrams 1.5340698e-06\n",
      "all 0.0018133362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a random variable UNK gt tseries phone on formula eb adjacent f abs diagrams all'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(15, 'a random variable', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d439e",
   "metadata": {},
   "source": [
    "### Function that calculates the perplexity of a sentence and applies it to a subset of sentences to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "43d32ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the sequence window to 1 to generate all the subsequences from the original sentence.\n",
    "\n",
    "SEQUENCE_WINDOW = 1\n",
    "\n",
    " # and define the perplexity for a sentence\n",
    "\n",
    "def perplexity(sentence):\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(tokens)\n",
    "    # find the indexes of the tokens from the vocabulary\n",
    "    tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "    # generate a N x SEQUENCE_LEN array of padded sequences \n",
    "    sequences = generate_sequences(tokens_idx)\n",
    "    predictors  = sequences[:,:-1]\n",
    "    label       = sequences[:,-1]\n",
    "    # the probabilities of all the words in the vocab given each padded sequence\n",
    "    probas = model.predict(predictors, verbose=0)\n",
    "    # add the log of the probability of the label given the padded sequence\n",
    "    logprob = 0\n",
    "    for k in range(N):\n",
    "        p = probas[k,label[k]]\n",
    "        logprob += np.log( p  )    \n",
    "    return np.exp(- logprob / N), logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "246b7cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a fixed-effects model only time-varying variables can be used. (95.14282674314309, -68.33068802952766)\n",
      "I know a pretty little place in Southern California, down San Diego way. (327.2831976033684, -86.86238765716553)\n",
      "This that is noon but yes apple whatever did regression variable (2013.0379361190335, -83.68140298128128)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In a fixed-effects model only time-varying variables can be used.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"I know a pretty little place in Southern California, down San Diego way.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"This that is noon but yes apple whatever did regression variable\"\n",
    "print(sentence, perplexity(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d33af0",
   "metadata": {},
   "source": [
    "### Perplexity On Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe9a2c",
   "metadata": {},
   "source": [
    "Calculate the perplexity on a validation set. Here we define the validation set as N random items from the original corpus.\n",
    "\n",
    "Transform that validation set into sequences of tokens using the training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e7a01f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_valid (100, 7)\n",
      "   post_id  parent_id  comment_id  \\\n",
      "0   208502        NaN         NaN   \n",
      "1    17451        NaN         NaN   \n",
      "\n",
      "                                                text category  \\\n",
      "0  Problem Training an LSTM network in Lasagne fo...    title   \n",
      "1  How do you report Kruskal Wallis one way analy...    title   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  problem training an lstm network in lasagne fo...        15  \n",
      "1  how do you report kruskal wallis one way analy...        20  \n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "df_valid = df_raw[(df_raw.category == 'title') & (df_raw.n_tokens > 10)].sample(100, random_state = 88).reset_index(drop = True)\n",
    "print(\"df_valid\",df_valid.shape)\n",
    "print(df_valid.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b86257cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus):\n",
    "    # start by calculating the total number of tokens in the corpus\n",
    "    all_sentences = ' '.join(corpus)\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    N = len(all_tokens)\n",
    "    logproba = 0\n",
    "    perps = []\n",
    "    for sentence in corpus:\n",
    "        pp, logp = perplexity(sentence)\n",
    "        logproba += logp\n",
    "        perps.append(pp)\n",
    "        print (\"{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{}\".format(pp, np.mean(perps), logp, logproba, np.exp( - logproba / (N  )), sentence  ))\n",
    "\n",
    "    return np.exp( - logproba / (N)), perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e89f6756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3701.38\t3701.38\t-123.25\t-123.25\t1.09\tproblem training an lstm network in lasagne for simple task determining parity of bit sequence\n",
      "5687.01\t4694.20\t-172.92\t-296.17\t1.22\thow do you report kruskal wallis one way analysis of variance and post hoc results following apa th standards ?\n",
      "550.86\t3313.08\t-69.43\t-365.59\t1.28\tgetting ls means of the response from logistic regression in sas\n",
      "1112.12\t2762.84\t-98.20\t-463.79\t1.37\tpost fisher ' s exact test - how to weight within categorial differences ?\n",
      "332.60\t2276.80\t-63.88\t-527.66\t1.43\thow to estimate power for individual associations in a regression ?\n",
      "908.86\t2048.81\t-163.49\t-691.16\t1.60\tstudy design will have missing data dependent on conditions . is it possible to get an omnibus statistic and determine relative outcome prevalence ?\n",
      "270.48\t1794.76\t-134.40\t-825.56\t1.75\twhat ' s the best estimator for expectation if we can draw samples iid and we know the likelihood of each sample we receive\n",
      "3787.25\t2043.82\t-98.87\t-924.43\t1.87\tcan a mathematically sound prediction interval have a negative lower bound ?\n",
      "174.35\t1836.10\t-56.77\t-981.21\t1.94\thow to determine if a price decrease is historically significant ?\n",
      "169.12\t1669.40\t-66.70\t-1047.91\t2.03\twhat to do when a log - binomial model ' s convergence fails\n",
      "290.23\t1544.02\t-90.73\t-1138.64\t2.16\tlooking for a metric to compare clustering solutions to a reference clustering for a large dataset\n",
      "156.25\t1428.38\t-121.24\t-1259.87\t2.34\thow to calculated a probability for a sum of integers to be equal to a given value if probabilities of addends are known ?\n",
      "140.94\t1329.34\t-74.22\t-1334.10\t2.46\thow to choose a model for survival regression when data does not fit assumptions ?\n",
      "2015.24\t1378.34\t-83.69\t-1417.79\t2.61\thow to properly test simple effects factor recode or data subsetting\n",
      "134.06\t1295.38\t-63.68\t-1481.47\t2.72\twhat is the importance of hat matrix , , in linear regression ?\n",
      "1591.21\t1313.87\t-125.33\t-1606.79\t2.96\toptimising simulation parameters - is it possible to produce regression style diagnostics such as p values ?\n",
      "268.56\t1252.38\t-61.52\t-1668.32\t3.09\twhen is it ok to not control for multiple comparison ?\n",
      "123.48\t1189.67\t-62.61\t-1730.93\t3.22\tthe effect of ommission of relevant variable in the regression model on adjusted\n",
      "1179.95\t1189.16\t-106.10\t-1837.03\t3.46\twhat is the intuition behind a long short term memory lstm recurrent neural network ?\n",
      "9452.52\t1602.32\t-119.00\t-1956.03\t3.75\tunderstanding no free lunch theorem in duda et al ' s pattern classification\n",
      "149.74\t1533.15\t-60.11\t-2016.13\t3.91\tusing a ecm vecm or ardl model ? why and how ?\n",
      "791.30\t1499.43\t-73.41\t-2089.54\t4.11\troc , variables , unbalanced data , where to start ?\n",
      "293.86\t1447.02\t-102.30\t-2191.84\t4.40\tis it normal that p - value getting from f - regression is very small even zero ?\n",
      "454.06\t1405.64\t-79.54\t-2271.38\t4.64\thow to interpolate a variable with frequency of years to annual data ?\n",
      "2964.62\t1468.00\t-103.93\t-2375.31\t4.98\tdo interactions in mixed designs inflate the main effect of fixed factor ?\n",
      "242.88\t1420.88\t-76.90\t-2452.20\t5.25\thow to understand the relationships among random variables , samples , and populations ?\n",
      "50323.69\t3232.10\t-119.09\t-2571.29\t5.69\trobust standard errors se estimators vs se estimators assuming conditionally homoskedasticity\n",
      "1073.05\t3154.99\t-83.74\t-2655.03\t6.02\tdoes zero - one loss correspond to any maximum likelihood procedure ?\n",
      "5828.18\t3247.17\t-130.06\t-2785.09\t6.57\tuse of robust spread measures such median average deviation and median filters for time series\n",
      "5951.63\t3337.32\t-112.99\t-2898.08\t7.10\tnaive bayes - additive smoothing or altering dropping columns when you get probability\n",
      "1893.64\t3290.75\t-83.01\t-2981.08\t7.51\tsearch for universal monotoneous function for time - series trend removal\n",
      "71.44\t3190.14\t-55.49\t-3036.58\t7.79\twhat is the regression coefficient of a constant predictor in linear regression ?\n",
      "1088.52\t3126.46\t-132.86\t-3169.44\t8.52\tdoes n - correction for pearson ' s chi - square apply to r x c tables generally ?\n",
      "402.07\t3046.33\t-89.95\t-3259.39\t9.06\tconfidence interval of the mean for a beta distribution when alpha and beta are estimated\n",
      "219.86\t2965.57\t-59.32\t-3318.71\t9.43\tis it possible for a computed variance to be negative ?\n",
      "2288.13\t2946.75\t-85.09\t-3403.80\t9.99\tasymptotic normality of mle in exponential with higher - power x\n",
      "280.38\t2874.69\t-135.27\t-3539.07\t10.94\thow could i check if the value for a is on average higher lower than b across all pairs containing a and b ?\n",
      "477.73\t2811.61\t-80.20\t-3619.27\t11.55\tscaling values so that the sum is equal to without changing their sign\n",
      "130.84\t2742.87\t-82.86\t-3702.12\t12.22\tam i wrong in assuming that the bayesian estimate has to converge to the true mean ?\n",
      "427.29\t2684.98\t-66.63\t-3768.76\t12.78\texplanation of the kolmogorov - smirnov test with applications in java\n",
      "4054.77\t2718.39\t-141.23\t-3909.99\t14.06\tforecasting daily data with trend , yearly , day of the week , and moving holiday effects\n",
      "11.99\t2653.96\t-27.33\t-3937.31\t14.33\thow to interpret the results of a t - test ?\n",
      "1130.44\t2618.53\t-84.36\t-4021.68\t15.17\tmixed - design anova or one - way anova with delta values\n",
      "711.23\t2575.18\t-131.34\t-4153.02\t16.58\thow can reliability and validity of content analysis be quantified when there is only one person coding the data ?\n",
      "623.91\t2531.82\t-77.23\t-4230.25\t17.47\twhat does y argument do in chisq . test in r ?\n",
      "1425.91\t2507.77\t-87.15\t-4317.40\t18.53\tencoding of categorical variables dummy vs . effects coding in mixed models\n",
      "125.52\t2457.09\t-77.32\t-4394.72\t19.52\tsignificance of difference in quality of fit between two sets of parameters of the same model\n",
      "27019.45\t2968.80\t-142.86\t-4537.58\t21.50\tconvolution operator in cnn and how it differs from feed forward nn operation ?\n",
      "669.37\t2921.88\t-104.10\t-4641.68\t23.07\testimation of a state - space model using bayesian analysis with the metropolis - hastings algorithm\n",
      "630.22\t2876.04\t-103.14\t-4744.82\t24.73\twhy null hypothesis of a statistical test is always opposite to what one wants to prove\n",
      "2652.58\t2871.66\t-102.48\t-4847.30\t26.51\tcan someone explain me how cumulative association in distributed lag models works ?\n",
      "187.76\t2820.05\t-68.06\t-4915.36\t27.76\twhat if response variable is ' yes or no ' in r ?\n",
      "1487.54\t2794.91\t-80.35\t-4995.71\t29.31\tfixed effects in first - difference model with percentage changes ?\n",
      "2649.82\t2792.22\t-102.47\t-5098.18\t31.41\tderivation of the iterative reweighted least squares solution for regularized least squares problem\n",
      "187.63\t2744.86\t-62.81\t-5160.99\t32.77\twhich cost function should be used for t - distributed noise ?\n",
      "94.46\t2697.54\t-50.03\t-5211.02\t33.90\thow to construct the likelihood if my errors are not gaussian\n",
      "279.64\t2655.12\t-84.50\t-5295.53\t35.89\tis it possible to apply a monotonicity constraint on a gaussian process regression fit ?\n",
      "183.88\t2612.51\t-83.43\t-5378.95\t37.97\tdoes p x x , y x p x x p y y implies independence ?\n",
      "96.37\t2569.86\t-63.96\t-5442.91\t39.65\twhat is the difference between finding the threshold and training a binary classifier ?\n",
      "112.97\t2528.91\t-104.00\t-5546.91\t42.54\thow to specify an autoregressive correlation structure in a linear model when the data has multiple observations at each time step ?\n",
      "368.59\t2493.50\t-124.10\t-5671.01\t46.26\twhat criteria would you used to choose the greatest number of metagenes when factorising an input matrix into two matrices ?\n",
      "504.60\t2461.42\t-105.80\t-5776.81\t49.69\thow to calculate f - measure base of fpr , tpr , tnr , fnr accuracy ?\n",
      "97.44\t2423.90\t-91.58\t-5868.40\t52.87\twhy am i getting a zero parameter estimate and se for one of my variables in a logistic regression ?\n",
      "1855.72\t2415.02\t-90.31\t-5958.71\t56.20\tlist of machine learning classifiers that naturally assume data in normal distribution\n",
      "26.37\t2378.27\t-75.26\t-6033.97\t59.13\thow to perform a t - test if i can ' t find the t - score on my t - table ?\n",
      "289.54\t2346.62\t-113.37\t-6147.34\t63.84\twhy glmnet ' s value is so small ? does it strictly implement the loss function under the hood ?\n",
      "872.48\t2324.62\t-74.48\t-6221.82\t67.14\tmultiple regression with nonlinear relationships between some ivs and the dv\n",
      "144.93\t2292.57\t-69.67\t-6291.49\t70.38\trepeated measures on a small sample with non - normally distributed continuous dependent variable\n",
      "5989.21\t2346.14\t-147.86\t-6439.35\t77.78\tgarch with student errors produces an error message in r but garch with normal errors runs fine\n",
      "366.25\t2317.86\t-94.45\t-6533.80\t82.91\twhat should the auroc be on the test set when no positive example is present ?\n",
      "202.19\t2288.06\t-69.02\t-6602.82\t86.87\twhat would be the logical inverse of a hypothesis test ? if any\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864.05\t2268.28\t-148.76\t-6751.58\t96.06\tif choice of learning algorithm is an unstable hyperparameter in nested cv , is the estimate of generalization error still valid ?\n",
      "1040.32\t2251.46\t-111.16\t-6862.73\t103.56\tmixed effects model failing to converge - better to remove the random intercept ? slope ?\n",
      "34.57\t2221.50\t-38.97\t-6901.71\t106.32\thow to calculate p - values for logistic regression coefficients ?\n",
      "1269.35\t2208.81\t-107.19\t-7008.90\t114.31\tusing deep neural networks for a severely imbalanced image dataset when some classes have images\n",
      "125.93\t2181.40\t-72.54\t-7081.44\t120.06\tare there any recent advances on the combination of deep learning and graphical models ?\n",
      "183.84\t2155.46\t-172.06\t-7253.50\t134.87\thow is it that cronbach ’ s alpha for these variables is so low , but using them to create a latent variable to predict a dv has such a high r ?\n",
      "193.60\t2130.31\t-63.19\t-7316.69\t140.76\tsignificance of categorical variables in regression in r - against each other\n",
      "12863.55\t2266.17\t-113.55\t-7430.24\t151.99\thypothesis testing rejection region depends on significance level or vice versa ?\n",
      "58.71\t2238.58\t-44.80\t-7475.04\t156.67\twhat is the mean of a cumulative beta distribution function ?\n",
      "221.12\t2213.67\t-59.39\t-7534.42\t163.08\tdoes the em algorithm require i . i . d ?\n",
      "216.43\t2189.31\t-80.66\t-7615.08\t172.23\thow to compare distributions of values by the way they cluster along a line ?\n",
      "95.70\t2164.09\t-123.15\t-7738.23\t187.18\twhy in box - cox method we try to make x and y normally distributed , but that ' s not an assumption for linear regression ?\n",
      "486.78\t2144.12\t-74.25\t-7812.49\t196.82\tcan gradient descent find a better solution than least squares regression ?\n",
      "843.05\t2128.81\t-101.06\t-7913.54\t210.74\tcan we use markov chain instead of hidden markov model to classify sequential data ?\n",
      "138.72\t2105.67\t-73.99\t-7987.53\t221.55\tdo we need to use one - hot if a feature has values , ?\n",
      "144.71\t2083.13\t-64.67\t-8052.20\t231.45\tis cross - validation still valid when the sample size is small ?\n",
      "945.58\t2070.21\t-82.22\t-8134.42\t244.68\twhich statistical methods are best suited for distribution with two peaks ?\n",
      "460.49\t2052.12\t-91.98\t-8226.41\t260.38\tchance of getting at least type i error out of tests for statistical significance ?\n",
      "164.16\t2031.14\t-107.12\t-8333.53\t279.94\tis there a way to approximately convert a standard deviation of log returns to a standard deviation of simple returns ?\n",
      "177.44\t2010.77\t-56.97\t-8390.49\t290.93\tinterpreting the coefficient of a non - binary defined dummy variable\n",
      "1623.37\t2006.56\t-81.31\t-8471.81\t307.37\twhy does box - cox transformation fail in following situation ?\n",
      "644.93\t1991.92\t-90.57\t-8562.37\t326.78\tdoes correlation . mean that there is an association in only in people ?\n",
      "950.19\t1980.84\t-75.42\t-8637.80\t343.88\ttotal effect is significant but both indirect and direct are not\n",
      "537.64\t1965.65\t-75.45\t-8713.24\t361.88\thow to define general covariance with a dirac delta with correct units\n",
      "744.30\t1952.92\t-99.19\t-8812.43\t386.98\twhy lambda regularization paramter in predict can be different from lambda for fitting model ?\n",
      "1801.60\t1951.36\t-97.45\t-8909.88\t413.34\tdealing with colinearity and if and how interaction terms could improve regression model\n",
      "141.44\t1932.90\t-54.47\t-8964.35\t428.84\twhat would p a , b a be equal to ?\n",
      "351.24\t1916.92\t-76.20\t-9040.55\t451.52\thow to implement a - d gaussian processes regression through gpml matlab ?\n",
      "282.86\t1900.58\t-84.67\t-9125.23\t478.12\texplain like i ' m five version for variograms in r ' s gstat package\n",
      " Corpus perplexity: 478.12\n"
     ]
    }
   ],
   "source": [
    "# Calculate Perplexity score on the validation set\n",
    "corpus = df_valid.tokens.values\n",
    "perplexity_score, scores = corpus_perplexity(corpus)\n",
    "print(\" Corpus perplexity: {:.2f}\".format(perplexity_score ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "508d6095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62687219.51330177,\n",
       " 177919585.8491131,\n",
       " 127487665.45277327,\n",
       " 601822187342.0973,\n",
       " 9996914202.452148,\n",
       " 989578659.1448282,\n",
       " 335436197.4638282,\n",
       " inf,\n",
       " 6431144974.132137,\n",
       " 5194216952.58106]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fce634b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6d0f72ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"perplexity scores of sentences - histogram\")\n",
    "plt.hist([sc for sc in scores if sc < 5000], bins=30);\n",
    "plt.xlabel('perplexity')\n",
    "plt.ylabel('# of sentences')\n",
    "plt.grid(alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fbd1bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sequential_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sequential_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('sequential_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424df361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
