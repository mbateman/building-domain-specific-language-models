"post_id","parent_id","comment_id","text","category"
"252114","","","i would like to learn the parameters for a truncated gaussian like this oneim using this formula for the probability densityf x mu sigma2 expleft frac xmu 2 2sigma2 right times frac 1 f mu sigma times mathbb 1 _ x0 where f mu sigma is a function so that the integral of f over x equals 1i tried to derive the maximum likelihood like we usually do for gaussian mixtures but then i got stuck i cant find an easy formula for mu and sigma how to find them","post"
"300306","","","mathematical definition of bayes optimality","title"
"151852","","289749.0","it cant hurt to show a few different specifications strictly speaking if there are no autocorrelation effects then you dont need newey west errors assuming no other effects either however someone might ask if you correct for it anyway","comment"
"28306","","52970.0","this is really quite a clever solution +1 i went looking for my metaanalysis book this morning but i couldnt find it","comment"
"229256","","","i have multiple dependant marketing variables measured on dichotomous scales and one independant variable with three levels students university personnel both i would like to define the type of study design in order to chose the correct statistical test for group differences i want to see if the different marketing activities are different according to the main type of market students university personnel both 1 between subject designs 2 within subject designs3 mixed designswhat actually worries me is the category both if not i would hae already classified it as between subject designcan you help me with some ideas and suggestions thank you in advance","post"
"269979","","516189.0","with regards to weighting i am interested not only how the current employment affects y but also how considerations of the past influence y this is why i wanted to weight","comment"
"260411","","498893.0","also see this relevant answer http statsstackexchangecom questions 243746 whatisprobabilisticinference 243759#243759 which points out that just using straight up optimization to solve a machine learning problem is _not_ a probabilistic approach _sensu stricto_","comment"
"340704","","643518.0","but you dont know the errors if you knew the errors you would incorporate them into the estimate and would have error free estimates you never get a measurement of 6000 pm 500 or anything like that you estimate the errors from the set of measurements","comment"
"148395","","284281.0","youre welcome med 8 is a strong association would be significant even w fairly small n","comment"
"149537","","286353.0","nickcox great point in fact maybe plain min max mean in a table might be enough but again thats why having a clear goal of the visualization is the most important part of making a good visualization","comment"
"149711","","","sequential mutually exclusive signingbonus offers a variant on the secretary problem","title"
"359569","","","hi i was wondering if someone would be able to help me decide on the proper set up for a multilevel model using the lmer package in my study we are looking at heart rate in beats per minute before and after a medication some of the participants have a different number of observations but each observations is identified with a day and month and there is no day and month that overlaps among any of the patients for instance id bpm day drug 1 80 3 10 no 1 49 3 10 yes 1 50 4 11 no 1 87 4 11 yes 2 47 4 12 no 2 47 4 12 yes 3 43 5 1 no 3 45 5 1 yesin setting up the model to control for the fact that these are paired could one concivably write it as either i believe these set ups would give same results lmer bpm drug + 1 + drug id date or lmer bpm drug + 1 + drug id + 1 date is it okay to treat date as a random effect we arent interested in time as an effect just using it as a categorical variable to identify that these are paired would this be the best appropriate approach","post"
"420960","","789326.0","cant see a clear question here","comment"
"88193","","172575.0","sean i doubt you can get ci as you reported above with your sample size note that you should not take 95 confidence interval of the mean of your distribution of 1000 estimations instead you should take 95 percentile interval ie almost the whole range are you sure you did that","comment"
"397596","","745793.0","thank you for the explanation i have a better idea of what im doing now","comment"
"106311","106309.0","","reject and fail to reject since when youre testing your hypothesis you basically challenge it based on some data it might happen that your data is just not the right kind proper sample that could reject the null hypothesis this does not mean that the null hypothesis is true it just means that the null hypothesis describes better your data than your alternative hypothesisin case you reject the null hypothesis you are accepting the alternative hypothesis here is a short article about this topic","post"
"300310","113097.0","","the complexity parameter cp in rpart is the minimum improvement in the model needed at each node its based on the cost complexity of the model defined asfor the given tree add up the misclassification at every terminal nodethen multiply the number of splits time a penalty term lambda and add it to the total misclassificationthe lambda is determined through crossvalidation and not reported in rfor regressionmodels see next section the scaled cp has a very direct interpretation if any split doesnot increase the overall r2 of the model by at least cp where r2is the usual linearmodelsdefinition then that split is decreed to be a priori not worth pursuing see the longintro document for rpart","post"
"124538","","237359.0","you could create random orthogonal matrix by qr or gramschmidt processes that will be eigenvectors of pca add scale to its columns turn into loadings get the covariance matrix from these loadings something like that","comment"
"22015","","39814.0","then i would be picking one molecule as a reference are you saying it doesnt make sense to pick every molecule as a reference the similarity between comparisons is a little different depending on which molecule is used as a reference","comment"
"344567","344565.0","","okay so this is tricky and from what i understand i see two scenariosif you have multiple mortality rates and wish to calculate confidence interval on the average of these rates which i am not sure about ci bar x pm z_ alpha 2 frac sigma sqrt n for large samples replace z_alpha with t_alpha for smaller models again bar x and hat sigma can be calculated if you have multiple data pointsbar x sum of the proportions n count totalsigma standard deviationagain this is if you have multiple mortality rates and you want to do an infernec on its averageif you either have one point or you want to infer on proportion of multiple pointsci p pm z_ alpha 2 frac sqrt hat phat q n + z2_ alpha 2 4n2 1+ z2_ alpha 2 n here hat p proportion 1 per 100 000 or 000001 and hat q 1 hat pfor large data sets you can use the formulap pm z_ alpha 2 sqrt hat phat q n","post"
"207864","","395120.0","welcome to our site this looks like a homework or textbook problem please add the selfstudy tag read its wiki http statsstackexchangecom tags selfstudy info","comment"
"125705","","239693.0","it appears you are estimating a system of three equations in such a case you must specify the relation between the three error terms","comment"
"179315","","339992.0","so i need to do bootrapping to get multiple accuracies for same algorithm with different subset","comment"
"46841","","91017.0","your dislike of the stattrek regression explanation is appropriate if you come from a background where true experiments are the norm your reasons all apply there but if you come from a background where quasiexperimental designs are more common then the stattrek site has more relevance in those cases both x and y values are often just random samples","comment"
"179365","","340061.0","e is a constant d has been provided by other experiments","comment"
"93625","","","random effect in crd model begin align y_ ij mu + tau_ i + varepsilon_ ij tau_ i sim mathcal n 0 sigma_ tau 2 varepsilon_ ij sim mathcal n 0 sigma2 end align i want to prove e mstr sigma2+nsigma_ tau 2the following is what i have done so far text let i 1 ldots p quadquad j 1 ldots nbegin align e overline y_ i ebigg frac sum_ j1 n y_ ij n bigg frac sum_ j1 n e mu+tau_ i +varepsilon_ ij n mu+frac sum_ j1 n tau_ i n mu+overline tau_ i e overline y_ ebigg frac sum_ i1 p sum_ j1 n y_ ij np bigg frac sum_ i1 p sum_ j1 n e mu+tau_ i +varepsilon_ ij np mu+frac sum_ i1 p overline tau_ i p e mstr sigma 2 +frac nsum_ i1 p e overline y_ i e overline y_ 2 p1 sigma 2 +frac nsum_ i1 p big overline tau_ i frac sum_ i1 p overline tau_ i p big 2 p1 sigma2+nsigma_ overline tau 2end align could anyone help me find the mistakes i made","post"
"226991","","429952.0","by uncertainty do you mean standard deviation or is ey a noise term or is this a reference to something else is this series independent or as is more typical for time series related over time","comment"
"310225","","589359.0","it seems like my data has monotonic relationship but nonliner the value goes up then steady is it the same as linear with interaction on age","comment"
"64215","","234897.0","why do you multiply the rbinom function by 21 and then d","comment"
"59639","59588.0","","johnross answer is very good in plain english endogeneity means you got the causation wrong that the model you wrote down and estimated does not properly capture the way causation works in the real world when you writebegin equation y_ibeta_0+beta_1x_i+epsilon_iend equation you can think of this equation in a number of ways you could think of it as a convenient way of predicting y based on xs values you could think of it as a convenient way of modeling e y x in either of these cases there is no such thing as endogeneity and you dont need to worry about ithowever you can also think of the equation as embodying causation you can think of beta_1 as the answer to the question what would happen to y if i reached in to this system and experimentally increased x by 1 if you want to think about it that way using ols to estimate it amounts to assuming thatx causes yepsilon causes yepsilon does not cause xy does not cause xnothing which causes epsilon also causes xfailure of any one of 35 will generally result in e epsilon x ne0 or not quite equivalently rm cov x epsilon ne0 instrumental variables is a way of correcting for the fact that you got the causation wrong by making another different causal assumption a perfectly conducted randomized controlled trial is a way of forcing 35 to be true if you pick x randomly then it sure aint caused by y epsilon or anything else socalled natural experiment methods are attempts to find special circumstances out in the world where 35 are true even when we dont think 35 are usually truein johnross example to calculate the wage value of education you need a causal interpretation of beta_1 but there are good reasons to believe that 3 or 5 is falseyour confusion is understandable though it is very typical in courses on the linear model for the instructor to use the causal interpretation of beta_1 i gave above while pretending not to be introducing causation pretending that its all just statistics its a cowardly lie but its also very common in fact it is part of a larger phenomenon in biomedicine and the social sciences it is almost always the case that we are trying to determine the causal effect of x on ythats what science is about after all on the other hand it is also almost always the case that there is some story you can tell leading to a conclusion that one of 35 is false so there is a kind of practiced fluid equivocating dishonesty in which we swat away objections by saying that were just doing associational work and then sneak the causal interpretation back elsewhere normally in the introduction and conclusion sections of the paper if you are really interested the guy to read is judea perl james heckman is also good","post"
"167601","","376131.0","alex right ols is ml at the gaussian and gaussian likelihood is not poisson likelihood","comment"
"81277","","","questioni want to test a medicine and i have two groups of people with baseline blood pressure one i give medicine a and the other one medicine b after 6 months i measure their blood pressurenow there are two optionsi measure the difference of the mean change i forget about the baseline and just measure the difference of the final valueswhen and why do you use method one or method two and what would happen in case you work with the lowest pvalue of each test to reject the nullhypothesis that both tests are equal","post"
"142655","","272701.0","glen_b the question asks for a unimodal distribution though which a smudgedtobecontinuous version of a bernoulli is not","comment"
"85986","","167737.0","so the statement is false as the correct answer is that the ith squared residual is less than or equal to the population variance","comment"
"23656","","","testing for significance of difference between two populations","title"
"59228","","","i am working with several hundred 3d point clouds generated using a 3d scanner and would like to be able to compare their shapes using something like a procrustes analysis instead of manually defining individual landmarks to compare however i was hoping to compare the entire point cloud of each scan to all of the other scans to measure similarity of each point cloud to the rest of the dataset is this possible and if so how would i go about it i would like to use r if possible to do this analysis though i do have access to other tools if need be","post"
"64225","","123816.0","yes the l2 regularisation parameter c lambda or whatever its something that is mentioned in an aside in bottous sgdqn paper and i presume explained better somewhere else basically your learning rate should be the 1 curvature inverse of hessian of your error surface now if your error surface is","comment"
"223506","","423032.0","conceptually i would like to explain a model with increments as proportion of what i have observed in the population so a medium increment would correspond to 68 of the population boundary for that predictor which is one standard deviation if normal perhaps it makes more sense to normalize the data by using quantiles","comment"
"436369","","","i have the following dataa random group of patients with a certain disease who receive a drug at a certain timedose of the drug given to the patient this drug has an affect to the heart which puts the patient to the risk of cardio attackfour parameters p_1 p_4 that are measured before and after the drug is given to the patient these parameters determine patients conditiongeneral information about the patients age bmi etc a binary variable indicating whether the patient survived or notthe data looks like thisd1 dataframe age sample 2050 5 bmi rnorm 5 20 40 p1_pre rnorm 5 05 2 p2_prernorm 5 05 2 p3_prernorm 5 05 2 p4_prernorm 5 05 2 p1_postrnorm 5 05 2 p2_postrnorm 5 05 2 p3_postrnorm 5 05 2 p4_postrnorm 5 05 2 survivedc 1 1 0 0 1 goalcurrent approach the current believe is that only one of the four parameters in item 3 of the above list lets call it p1 is an important indicator for knowing the patients condition who has taken the drug is bad and therefore we should stop giving him her the drugso the practice is that p1 is calculated before and after the treatment and if the value of p1 falls below an acceptable value the drug is stoppedhypothesis the hypothesis is that1is not the only indicator but the other 3 parameters in item 3 of the above list are also important indicators for knowing the patients condition after taking the drugquestion what is the best way to study this data how can i use the general information of the patients and the binary outcomeany help is greatly appreciatedthanks in advance","post"
"40502","39216.0","","creating a theoretical response doesnt seem very promising as it doesnt exist in practice on the other hand i would suggest to use a standard statistical model logistic regression with a random effect on the subject level for example to see which biomarker predicts the group treatment versus control more accurately this would be the biomarker which is most adapted to your study i suppose","post"
"420219","","783987.0","generally when you have an informative prior that is quite wrong in an informal sense eg normal 0 1 when the actual value is 36 your credible interval in the absence of a lot of data will be pretty poor when looked at from a frequentist perspective","comment"
"401389","401309.0","","first notice that we can write the last expectation as mathbf e ex x0 1 int ex f_ x x0 x dx 1 we will focus on evaluating the integrallet phi_ mu sigma be the distribution function of the mathcal n mu sigma2 writting in an informal manner the density of x x0 is given bybegin align f_ x x0 x frac p x x x0 p x0 mathcal 1 _ 0 infty x left frac e frac xmu 2 2sigma2 sqrt 2pisigma2 right frac 1 1phi_ mu sigma 0 quad end align therefore the integral becomes int ex f_ x x0 x dx frac 1 1phi_ mu sigma 0 int_0infty exfrac e frac xmu 2 2sigma2 sqrt 2pisigma2 dxbefore evaluating it we do a little algebra with the terms on the exponential functionbegin align x + frac xmu 2 2sigma2 frac x2+2x mu+sigma2 mu2 2sigma2 frac x2+2x mu+sigma2 mu+sigma2 2 2sigma2 +frac mu2+ mu+sigma2 2 2sigma2 frac x mu+sigma2 2 sigma2 + mu+frac sigma2 2 quadend align define mu mu+sigma2 we evaluate the integralbegin align int ex f_ x x0 x dx frac 1 1phi_ mu sigma 0 int_0infty exfrac e frac xmu 2 2sigma2 sqrt 2pisigma2 dx e mu+frac sigma2 2 frac 1 1phi_ mu sigma 0 int_0infty frac e frac xmu 2 2sigma2 sqrt 2pisigma2 dx e mu+frac sigma2 2 frac 1phi_ mu sigma 0 1phi_ mu sigma 0 int_0infty frac e frac xmu 2 2sigma2 sqrt 2pisigma2 frac 1 1phi_ mu sigma 0 dx e mu+frac sigma2 2 frac 1phi_ mu sigma 0 1phi_ mu sigma 0 end align the final equation holds because we are integrating the density of a random variable of the form x x0 where x sim mathcal n mu sigma2 hencee ex1 ex10 e mu+frac sigma2 2 frac 1phi_ mu sigma 0 1phi_ mu sigma 0 1quadobs this is my first answer i would be grateful if you could told me what i should improve thanks hope it helps","post"
"117541","","225069.0","1+ thank you very nice answer unfortunately i think you didnt address the specifics of determining values for priors when you already have the measurement variability assuming you did the work necessary in case 2 or believe what the manufacturer says in case 1 for example suppose the variability is 01 as in the original question why did the author writes 01 2 assuming 2 is the standard deviation what procedure leads to that result","comment"
"234303","233487.0","","you could simply use the fact that v y mathbb e y2 mathbb e y 2 estimate the conditional expectations of y2 and y separately then estimate the conditional variance by hat v y x hat mathbb e y2 x hat mathbb e y x 2no double smoothing involved this way","post"
"100854","","196282.0","thanks of the response this measure does not ensure that the highest similarity value will be when matching the same vector for example matching v1 with v1 according to the similarity table given in the question the similarity score will be mean aa + ab + ac + ba + bb + bc + ca + cb + cc 352 9 0391","comment"
"82020","","","i have two doubt one about theory and one about practical problemfirst i have not full understand how to work a bayesian network with continuous values i have learn that i can approximate p a the probability of node a with a gaussian distribution but i have a dataset mean and variance of the gaussian distribution is the mean and the variance of the dataset and if i have p a b c with a and b with continuous values how i can represent with a gaussian distribution the practical problem is i need to learn a a bayesian structure from a continuous values dataset and i use this toolbox for matlabhttp codegooglecom p bnt bayes net toolbox for matlab by kevin murphy now how i can use to learn a bayesian structure from a dataset of continuous values with this toolsif i use learn_struct_k2 function i need the order of nodes but where i can get this order there are other useful functions in this toolbox that you know about this problem","post"
"51604","","","ordinal trends and finding concordant and discordant pairs","title"
"198362","","","how to define a 2d gaussian using 1d variance of component gaussians","title"
"1446","","420479.0","also note that there are zeroinflated models extra zeroes and you care about some zeroes a mixture model and hurdle models zeroes and you care about nonzeroes a twostage model with an initial censored model","comment"
"52133","","102011.0","this is from a national database in usa the test is done to determine if more number of males have rectal cancer versus females in a particular age group","comment"
"16409","16305.0","","partial least squares will do what you want the pls library in r is what ive traditionally used heres an example that creates 3 groups of 50 points assembles them into a data frame with group labels and runs pls on themlibrary mass library pls pts1 mvrnorm 50 c 3 0 3 diag c 1 1 1 pts2 mvrnorm 50 c 3 0 3 diag c 1 1 1 pts3 mvrnorm 50 c 3 3 3 diag c 1 1 1 pts asdataframe rbind pts1 pts2 pts3 ptslabels c rep 1 50 rep 2 50 rep 3 50 plsresult plsr labels ncomp2 datapts plot plsresultscores colptslabels","post"
"191623","","364064.0","1 that a variable is on a bounded range doesnt make it discrete it may be discrete but being on 0100 isnt why continuous proportions are still bounded 2 likely the actual distribution wont be any common distribution but id probably start with a beta model for one proportion and dirichlet for a larger set of proportions that add to one 3 failure to reject normality in a distributional test doesnt tell you that you have normality 4 why do you care whether the data were drawn from some particular distribution or some other distribution","comment"
"242037","","461645.0","whuber im indeed looking for a small n asymptotic result which goes beyond the traditional argument that the empirical variance converges almost surely to the true variance such a result would provide the simple conditions which you are talking about im guessing that the fourth or fifth moment being finite might be sufficient if not having a moment generating function is probably more than sufficient","comment"
"79406","","","i have read that when you get more and more data you can find statistically significant differences wherever you lookmy question iswhy is this the case any intuitive examples that show this behavior why do such increases in statistical difference do not necessarily imply that the observed effects are meaningful important","post"
"277297","","","how can we determine the relationship between quantitative variables and qualitative variablesfor example age 20 45 and gender male female","post"
"373928","373659.0","","i found the issue the randomforest is also bootstrapping bagging the observations and hence one needs to average over the resampled observations not the initial sample ones so the averaging is done over the observations that were found in the node on a given bootstrap sample this is obtained using the keepinbagtrue argument for this case i1 t1 it shows that 1 1 1 0 ie last observation was not drawn in that specific sample which explains the resultsee codelibrary randomforest # randomforest 4614# type rfnews to see new features changes bug fixesdata swiss ## run forestsetseed 111 swissrf randomforest fertility dataswiss keepinbagtrue ## predict first obs from first tree onlyi 1t 1pred_i_t predict swissrf newdata swiss i predictalltrue individual t ## get node values extract obs in that nodenodes_tree1 attr predict swissrf newdata swiss nodes true nodes t bag_tree1 swissrfinbag swissnode nodes_tree1swissbag swissrfinbag t obs_i_tree_t subset swiss node nodes_tree1 i with obs_i_tree_t weightedmean fertility bag # 1 828pred_i_t# 1 828created on 20181026 by the reprex package v021","post"
"374624","","","i am working on a simple back propagation neural network that is predicting next value of a sensor reading it took me a while to train it based on the last year data and now i have the resultsthere are several inputs into my nn like temperature humidity day month hourthe answer i want from the nn is what is the next hour value for the sensor readinglets say that at 1300 reading is 50what will be the sensor reading at 1400 here are the outputs per houryellow predictedgreen sensor readingi think i could use it as an eventual trend for the next hour still i see some errors therelooking at the results above how do you tell if this network is performing well overfitted underfitted it suits at allhere are the stats of the training1000000 epochs 3700 training set 3700 validation set validation error 0000376 training error 0000214","post"
"70543","","179741.0","ok thats not a bad assumption","comment"
"365955","","","why does random forest variable importance not sum to 100","title"
"351746","","663592.0","i recently came about this issue too but couldnt solve it in r i used stata then where we can apply suest to see if two models are significantly different theres a suest function around in a package for r but i doubt that it is the same in stata suest is related to seemingly unrelated estimation note that sureg is somewhat different i am also interested in an r solution hope that would help somehow","comment"
"12508","","21997.0","this may well be in your list of layman measures already tried but doesnt simply ranking the values by frequency do what you want","comment"
"129081","","245414.0","were these real data or simulated data you can check the multiple r2 of the new predictor variable based on the set of previous predictors ie exclude y","comment"
"158281","","","i understand that principal component analysis pca can be applied basically for cross sectional data can pca be used for time series data effectively by specifying year as time series variable and running pca normally i have found that dynamic pca works for panel data and the coding in stata is designed for panel data and not time series is there any specific type of pca which works on time series dataupdate let me explain in detaili am presently constructing an index for infrastructure in india with variables like road length rail route length electricity generation capacity number of telephone subscribers etc i have 12 variables through 22 years for 1 country though i have reviewed papers that apply pca on time series and even panel data pca is designed for cross sectional data which assumes iid assumption panel and cross sectional data violates it and pca does not take into account the time series dimension in it i have seen dynamic pca being applied only on panel data i want to know if there is a specific pca that is applied on time series or running static pca with year defined as time series variable will do the job","post"
"387913","","728591.0","i think x y z should be independent for further analysis","comment"
"17116","","","category selection for text classification","title"
"12900","","22466.0","whuber thanks yes i am sure it gives a negative value i have posted an image of the output","comment"
"29207","","55989.0","sorry for the late delay anyways your approach doesnt really change my standard errors and your proposed solution is going again in the direction of the link i provided above that is computing the standard errors yourself i am just wondering why plm is not doing that automatically","comment"
"436229","","813158.0","no my target can actually take on any number between 2 and 2 the output of the network should be 1 or 1 since these are two discrete options that are multiplied with the target","comment"
"401701","","","this is exercise 629 from casella and bergers statistical inference so ill just post the question in full and ill also post the answer included in the solutions manual ill make the part that i dont fully understand bold and italicizedstatement of the problemthe concept of minimal sufficiency can be extended beyond parametric families of distributions show that if x_1 x_n are a random sample from a density f that is unknown then the order statistics are minimal sufficient hint use theorem 665 taking the family f_k boldsymbol x f_k boldsymbol x to be logistic densitiessolutionlet f_j text logistic alpha_j beta_j text j 0 1 k from theorem 665 the statistict boldsymbol x bigg frac prod_ i1 n f_1 x_i prod_ i1 n f_0 x_i frac prod_ i1 n f_k x_i prod_ i1 n f_0 x_i bigg bigg frac prod_ i1 n f_1 x_ i prod_ i1 n f_0 x_ i frac prod_ i1 n f_k x_ i prod_ i1 n f_0 x_ i bigg is minimal sufficient for the family f_k boldsymbol x f_k boldsymbol x as t is a 11 function of the order statistics the order statistics are also minimal sufficient for the family f_k boldsymbol x f_k boldsymbol x if mathscr f is a nonparametric family f_j in mathscr f so part b of theorem 665 can now be directly applied to show that the order statistics are minimal sufficient for mathscr f so my question is why is it true that these logistic densities are in mathscr f is it that if you include a possibly parametric family of densities in the same set as even a single density that you dont know how to parametrize you get a nonparametric family if so then i understand that since the order statistics are always sufficient for a random sample you get the desired result from theorem 55b but why can you just throw these logistic densities into some nonparametric family is it impossible for such an addition to dilute the family in the sense that some other smaller statistic was minimal sufficient for the family without these logistic densities but since youve now added them you add an element that renders that statistic insufficientfor the sake of completeness im posting theorem 665 belowtheorem 665 minimal sufficient statistics suppose that the family of densities f_0 boldsymbol x f_k boldsymbol x all have common support thena the statistic t boldsymbol x bigg frac f_1 boldsymbol x f_0 boldsymbol x frac f_2 boldsymbol x f_0 boldsymbol x frac f_k boldsymbol x f_0 boldsymbol x bigg is minimal sufficient for the family f_0 boldsymbol x f_k boldsymbol x b if mathscr f is a family of densities with common support and i f_i boldsymbol x in mathscr f i0 1 k ii t boldsymbol x is sufficient for mathscr f then t boldsymbol x is minimal sufficient for mathscr f","post"
"103229","","","is there some method which will allow me to find some set of random numbers z_1 dots z_n such thatz_1 c_1 + z_2 c_2 + + z_n c_n 0where for k1 dots n the c_k are fixed coefficients and z_k are realizations of a standard normal random variable many thanks in advanceedit the considered scenario is a multivariate normal distribution with known covariance matrix mathbf sigma and cholesky decomposition mathbf c mathbf c mathbf sigma i am using a montecarlo approach where i am interested in all realizations of the vector mathbf z z_1 z_2 z_n where using mathbf x mathbf c mathbf z the value of x_n z_1 c_1 + z_2 c_2 + + z_n c_n is equal to zero i hope this is a bit clearer","post"
"118239","","226701.0","thank you so much the picture was incredibly helpful thats exactly what i was confused about and this visualization helped to understand everything","comment"
"30729","","59788.0","neals marginal likelihood example and wilkinsons mh example are the sort of thing i had in mind ie algorithms that seem correct but arent do you want to make them in to answers","comment"
"191917","","364609.0","what do you mean by same distribution are observations of gamma considered as come from the same distribution or it is considered as the sum of exponential distributions","comment"
"149135","","","how to calculate mean and standard deviation of a range variable grades when the raw data is based on frequency categories","title"
"67709","","130749.0","theres not enough information were these coded as factors how were these variables used in the model were there interactions","comment"
"91737","","","can someone please walk me through this morass of sthere is a sigmasq in the summary output from geors likfit really this is the one id like to know about im not clear which of all of the other s it is a variance a different parameternmles variogram calls sigma an optional numeric value used as the height of a horizontal line displayed in the plot can be used to represent the process standard deviation default is null implying that no horizontal line is drawn i thought this was phigeor variofits documentation calls it a partial sill and says to set a starting value im also not quite conceptually clear on what variofit is modelling differently from likfit diggle ribiero modelbased geostatistics calls 2 signal variance is that the same sigma later p 33 they say 2 is the value the expectation of the vijs approach as the distance xixj grows and that uncorrelated yis have an expectation of 2","post"
"30247","","59082.0","also are the 30 output variables the same theoretically for both methods","comment"
"255743","","488255.0","the out put using crosstabs with statistics in spsspearson chisquare value 90000 df 81 asymptotic significance 2sided 231likelihood ratio value 46052df 81asymptotic significance 2sided 999linearbylinear association value 5739df 1asymptotic significance 2sided 017n of valid cases10a 100 cells 1000 have expected count less than 5 the minimum expected count is 10","comment"
"6780","","","how to calculate zipfs law coefficient from a set of top frequencies","title"
"55167","","106865.0","the numeric first derivative with respect to index is diff so if you have many ones in a row the derivative will be zeros if you have sparse ones then the each time it switches the diff will be bigger you could use ewma as a poor mans kernel smooth http enwikipediaorg wiki exponential_smoothing how does it work it makes a weighted average of a window of values a kernel function does something related but a little more complex it takes a window sometimes a much wider window and then computes a function based on the values in it sometimes the function looks like a pdf","comment"
"188750","","358404.0","until you can tell us more about the model the data and the roles of the theta_i in it this question has many possible correct answers presumably the question you are quoting has a contextperhaps its one in a series of questions about a particular statistical setting what is that setting","comment"
"262148","","501830.0","great answer i did as you suggested and i got negative but different slopes at both time points however none of those slopes is significant does this mean that the a vs b relationship is not actually significant if yes is there another reason why the interaction is significant","comment"
"212856","","404339.0","thank you whuber for your continued responses and patience i am a bit confused what outcomes are there that have a higher probability of occurrence that are in the critical region for any hypothesis test note that under my approach as with the fisher exact test we are not calculating a critical value we are directly calculating the pvalue compared to a significance level we always have the alternative for discrete distributions of calculating the pvalue directly based on the underlying distribution which in my example is i believe a joint binomial","comment"
"178307","","337645.0","pie we estimate the variance from the data","comment"
"88869","","173805.0","in the second graph it looks like the plotted point for the second peak is very close to the actual peak while in the first graph the closest points to the second peak are on either side of the actual peak redo your calculations on a finer grid and theyll probably look more alike","comment"
"157343","","299759.0","no need to change the formula formula in the answer should give you the correct results irrespective of the signs of the value see this http wwwpmeancom 13 predictedhtml","comment"
"192941","","","expectation of u given the first and second moments of u","title"
"281717","","","to gain an understanding of bptt im attempting to derive the formulas for bptt by hand but keep getting stuck heres what i have so farforward pass 1a_0 x_0 u_0b_0 s_ 1 w_0z_0 a_0 + b_0s_0 func_0 z_0 where func_0 is sig or tanh foward pass 2a_1 x_1 u_1b_1 s_0 w_1z_1 a_1 + b_1s_1 func_1 z_1 where func_1 is sig or tanh q s_1 v_1output passo func_2 q where func_2 is softmax e func_3 o where func_3 is xentropy now attempting to handbomb back prop for u by working backwards through the above network partial e partial u partial e partial u_1 + partial e partial u_0partial e partial u_1 partial e partial o partial o partial q partial q partial s_1 partial s_1 partial z_1 partial z_1 partial a_1 partial a_1 partial u_1partial e partial u_0 partial e partial o partial o partial q partial q partial s_1 partial s_1 partial z_1 partial z_1 partial b_1 partial b_1 partial s_0 partial s_0 dz_0 partial z_0 partial a_0 partial a_0 partial u_0gathering like termspartial e partial u partial e partial o partial o partial q partial q partial s_1 partial s_1 partial z_1 partial z_1 partial a_1 partial a_1 partial u_1 + partial z_1 partial b_1 partial b_1 partial s_0 partial s_0 partial z_0 partial z_0 partial a_0 partial a_0 partial u_0 making substitutionspartial e partial u partial e partial o partial o partial q v_1 partial s_1 partial z_1 1 x_1 + 1 w_1 partial s_0 partial z_0 1 x_0 ending with a nice clean formulapartial e partial u partial e partial o partial o partial q v_1 partial s_1 partial z_1 x_1 + w_1 partial s_0 partial z_0 x_0 and similarlypartial e partial w partial e partial o partial o partial q v_1 partial s_1 partial z_1 s_0 + w_1 partial s_0 partial z_0 s_ 1 the problem i face here is one of dimensionality my gut says w1 should not be in here does this formula make sense to anyone where have i gone wrongthanks","post"
"66886","","129140.0","how do you work out the sampling distribution of a mean without knowing what the distribution of a sum is first when you say the samples are related do you mean the observations are related or do you have multiple samples each containing many observations","comment"
"211057","","400984.0","so when q0 which happens often you try to calculate logarithm of negative number is it correct","comment"
"22616","","41074.0","rscript is clearly superior to r cmd batch which predates it","comment"
"335546","","634321.0","so that you are motivated to improve your answer thats how the voting system is designed to work if you adress some of the issues i mentioned above i will upvote","comment"
"35363","","74529.0","hi andy i do not have an example with that many observations interactivity is essential for data exploration i worked on a dataset with about 800 variables and 10 000 observations in a celllural telephone network to discover mysterious failures using the classifiers stagewise and with interactive the 11 variables responsible were found and tracing them backwards in time unsual activity in the network was detected 34 prior to noticing the failure","comment"
"367227","","690205.0","contd if you take it to an extreme and say 1 billion tosses came up heads then it is reasonable to conclude that the coin is heavily biased in favour of heads and again you dont need to know the underlying mechanism that causes the bias and therefore the probability of heads coming up in the next toss must be close to 1 also in the post i have assumed the prior to be 1 2 for heads","comment"
"123361","","235883.0","ctd though more recently than the 19th century the vysochanskijpetunin inequality http enwikipediaorg wiki vysochanskije28093petunin_inequality removes the need for symmetry for it to apply to intervals about the mean the original inequality was in relation to the mode but in symmetric unimodal distributions with a mean theyre the same that was published before my rediscovery of the older result but i only became aware of the generalization recently and am apparently still in the habit of needlessly restricting it","comment"
"321413","","610212.0","thanks could to help me out with this question https statsstackexchangecom questions 321442 whyismaximumlikelihoodusedforarimainsteadofleastsquares","comment"
"278552","","","estimating covariance matrix of innovations of multivariate random walk","title"
"69180","69177.0","","as it is there are many ways to formulate and thus solve your problem you are looking for the symmetric psd matrix a with a_ 1cdot 1 09 08 07 06 05 and a_ ii 1 i1 ldots 6 there are many matrices satisfying these requirements to pick one we need to add some more criterion constraints for example if we add the criterion that we also want a to be smallest and all the entries of asetminus a_ ii a_ 1cdot to be equal to some constant nu this is now a convex problem with a unique solution in r you would write solve it asfx01function nu fixedrow plenght fixedrow a1matrix nu p p diag a1 1 a1 1 a1 1 fixedrow det a1 fixedrowc 1 9 8 7 6 5 a1optimize fx01 intervalc 1 1 fixedrowfixedrow and the value nuapprox06 yielding the matrixnua1mina2matrix nu 6 6 diag a2 1a2 1 a2 1 c 1 9 8 7 6 5 a b c d e fa 10 09 08 07 06 05b 09 10 06 06 06 06c 08 06 10 06 06 06d 07 06 06 10 06 06e 06 06 06 06 10 06f 05 06 06 06 06 10which satisfies your requirements then to actually generate your matrix of variables simply dolibrary mass setseed 123 xmvrnorm 100 rep 0 6 a2 empiricaltrue cor x and you can check that for example cor x 1 x 2 returns 09","post"
"218252","","412874.0","related but not duplicate to http statsstackexchangecom questions 32318 differencebetweenstandarderrorandstandarddeviation","comment"
"40897","","79736.0","gui11aume the second part of my comment relates to the fact that the neymanpearson errordecision theoretic framework is not the entirety of frequentist statistics pvalues come from to fishers significance tests which are quite distinct from the neymanpearson approach many people know of the wars between fisher and neyman but unfortunately fewer know what they were actually fighting about this paper might make my concern clear","comment"
"351550","","757330.0","xavierbourretsicotte yes i agree the covariance matrix sigma is positive definite by definition but what does this covariance matrix intuitively mean is sigma not the similarity or correlation between each x i there is similarly a proof here https wwwstatlectcom fundamentalsofstatistics normaldistributionmaximumlikelihood as well i am simply wondering why likelihood is defined as l pi f_x which is exactly the same as the joint density function but shouldnt the joint density function be equal to l times p where p is the prior wheres the prior","comment"
"309549","","588729.0","user182007 sorry i misspoke what you said not what i said is what the slope will tell you i accidentally reversed the dv and the covariate","comment"
"53411","34410.0","","maybe it is not so much about the applications problems you are aiming for and its characteristics but more about the algorithms and variants you are using more concretely in order to handle big data many variants based on stochastic gradient descent of popular algorithms like svm have appear which are able to handle thatscikit offers support for some of this algorithms svm knn kmeans i guess this nice graph can help you to quickly figure out if scikit makes sense to you at allhope that helpsnote what follows is a reply on the comment by zelazny7now i understood you what you are looking for is pandas take a look at the talks section there is a presentation comparing pandas workflow and pandas briefly panda lets you import data in different formats and handle bgu files through hdf5 tables integration besides you can interface scikit","post"
"234396","","444999.0","if your question is is machine learning only about estimating something then it seems safe to say yes if your question is is machine learning only about estimating something based only analyzing previous results of that something one could say the answer is no with bayesian methods being a counter example you are estimating something using both observations and expert knowledge but im not sure if thats exactly what youre after","comment"
"306309","","585841.0","this suggests best fit means something nearer to conveniently fit by some criterion in which case theres no issue with bringing likelihood in at all","comment"
"148812","","327986.0","glen_b a problem is defined in terms of a loss function given that loss function various other model choices can be good or bad if youre going to come up with a loss function you are deciding how to penalize various errors another way to say the same thing is that you are interpreting the model prediction inducing a predictive distribution p over your data intuitively a bad loss function is one that focuses on an arbitrary subset of the data eg outliers if your data has very heavy tails these points correspond to points that are necessarily surprising given p","comment"
"253659","","518688.0","but why the second model starts from scratch as if no variable selection happened does not lasso selects explanatory variable with best predictive power btw i thought to do stuff lasso sparse matrix variable into glm again now i have understood lasso per se is a regression","comment"
"337070","","637051.0","yes it does regression coefficients from a linear regression","comment"
"169914","","322052.0","that is in fact the correct approach as i discovered almost a year ago but forgot to come back and edit this question to include the answer for some reason crossvalidated is giving me an error when i try to accept this as the correct answer to give you credit for it though","comment"
"291620","","556307.0","but put these quantities into a practical scenario as i mentioned discrete and finite samples apparently you can calculate the value of all these quantities either accurately or approximately and its very likely they wont be the same the question is whether these values make sense if so how to interpret them apologize if i didnt make it clear in my question","comment"
"428361","","799534.0","in python you use a huber loss why then is rsquared which is appropriate for quadratic loss even relevant indeed no conceivable loss would be appropriate for these data given their extreme heteroscedasticity both procedures performed admirably by finding good fits but summarizing them by giving an overall measure of the residual sizes would be meaningless","comment"
"327499","","","2y1 sqrt xsimmathcal n 0 1 when xsimchi2_ n1 and ysimtext beta left frac n 2 1 frac n 2 1right independently","title"
"65272","","","under which conditions do gradient boosting machines outperform random forests","title"
"377462","377451.0","","the negative binomial model is typically used for count data with no upper limit ie the outcome variable y can take values in the set 0 1 2 ldots infty however in your case you can only have a maximum count of 7 a more appropriate distribution for your data is the binomial distribution which describes the number of successes ie in your case success eating meat vegetables in n trials ie in your case n 7 is the number of days ps note that in both cases of negative binomial and binomial mixed models you need to be careful with the interpretation of the estimated fixedeffects coefficients for more details on this check this post","post"
"428363","","799299.0","i have deanonomized it a bit and i hope that helps","comment"
"371335","","697699.0","to go into it further your timeseries looks piecewise in time like it might be welldescribed by an ar model with a single ar model being a reasonable description in each piece but ar model and order might change when your system undergoes a transitionyou want to detect these transitions and also find a suitable model for each piece the paper i pasted claims has 2 methods that tries to find these individual dynamics in each temporal segment and also the # of transitions","comment"
"126885","","241981.0","spss displays you the matrix of the coefficients bf b and also saves standardized scores as new variables appending them to the dataset of original variables bf x the op i think standardized bf x and then multuplied bf xb and voila its what spss appended to the dataset so the ops question is that wow following by how bf b was computed","comment"
"342779","","647872.0","amoeba fair enough i honestly dont know much about random effects models in a frequentist framework but its pretty common to hear bayesians sing the praises of hierarchical models precisely to get better inference on the grouplevel effects by borrowing strength perhaps it would be more meaningful to ask the question purely from a bayesian standpoint focused on how to interpret the credible intervals","comment"
"235780","","","i have a homework but i dont know how to solve it or what should i dokindly help me or guide melet a constant a satisfy thatint_ 0 a x2 e frac x2 2 dx int_ a infty x2 e frac x2 2 dx suppose x is a standard normal random variable define y as followsy left begin array ll x quad if x geq a x quad if x a end array right a what is the distribution of y b show that x and y are uncorrelated c show that x and y are not independentactually i dont understand the purpose of given integration function like how or what can i do with the functionwhat i understand so far maybe true or false i consider when x geq a thats mean y int_ a infty x2 e frac x2 2 dx and the opposite when x aif i consider this thats mean y is uniform distribution because nothing different when x is greater or less than athen to show x and y are uncorrelated cov x y e xy e x e y 0 that means e xy e x e y but this understanding is the opposite of c question since this apply when x and y are independentthank you so much for your help","post"
"225933","","427936.0","+1 the notation p z sqrt x is a little weird to me my brain says shouldnt that be zero maybe introducing a small notation for the pdf and cdf say phi and phi would be even clearer","comment"
"296890","","564634.0","your code doesnt run mannkendall is not a function in r what did you leave out","comment"
"108493","","","im looking to run rolling onestep ahead forecasts on the mcomp holdout data future data with reestimation at each point ie reestimation over the entire historical and already forecast holdout data as mcomp is partitioned into historical x data and future xx data i am unsure of how to step through to achieve this my current h1 incarnation follows with thanks to stephan kolassalibrary forecast library mcomp result structure rep na length m3 namesnames m3 pb winprogressbar maxlength m3 for ii in seq_along m3 setwinprogressbar pb ii paste ii of length m3 yy m3 ii naivemodel fitted arima yyx orderc 0 1 0 arimamodel autoarima yyx arimaforecast forecast arimamodel h1 meanamean abs naivemodelyyx bmean abs arimaforecastyyxx 1 result ii b a close pb result writecsv result file arimacsv","post"
"19431","","","i am trying to estimate the joint distribution of stock returns using the copula package i have read a couple of papers on copulae but alas my lack of math knowledge prevents me from understanding beyond the basics if that i am thinking of estimating the joint distribution using the following steps and have a couple of questions about the process in generaldownload stock prices for two companiesconvert prices to continuous returnstransform returns to marginal uniform distributionhow do i do thiscreate the copula objectwhen creating the copula object using ellipcopula or archmcopula i need to specifiy the type of copula and the parameters how do i know which copula to use and how do i obtain the parameters i thought the point of fitting a copula to the data was to obtain the estimated parameters so why do i need to specify here am i just providing initial estimates of the parameters which are then corrected in the actual fitting specifiy the bivariate distributionwhen specifying the bivariate distribution i need to specify the margins in mvdc since according to a number of sources i should convert the data returns to uniform distributions do i specify the margins as unif in the functiondefine the data to be fittedi am assuming the will be a matrix with 2 columns in this case each column being the uniformsfit the dataas you can see i am very new at this and any help would be greatly appreciated","post"
"169436","","321078.0","it is not right to call lda only a classification technique it is a composite 2stage technique first reduce dimensionality then classify as dimensionality reduction it is supervised unlike pca as classification it considers marginal probability unlike logistic regression","comment"
"29702","","","were trying to develop a simple tool that will help our teams optimise their clients facebook posting strategies in our experience time of post can have a big impact on audience response but those times will differ from audience to audience based on their behaviours can they access facebook from work what times of day are they online when are they at a loose end and when are other activities taking up their concentration we expect responsiveness to vary throughout the day for these and other known unknown reasons furthermore of course time of day isnt the only reason that response rates vary the content of the post is one very significant reasoni dont know if this background is useful whether it makes the following question clearerthe following r script i lack sufficient reputation to post charts contains data that cover posting activity and audience response by hour of post youll notice that the 6 am point for which we have two observations massively outperforms the rest of the day we often see these very high response rates for hours that have fewer observationsposthour c 023 posts c 0 0 0 0 0 0 2 15 16 10 17 13 29 21 23 18 29 24 34 42 51 48 49 17 response c 0 0 0 0 0 0 5282 8627 6080 2716 2831 3258 6291 7756 4008 4614 11838 2611 10527 14706 5416 10970 19098 9505 meanresponse response postsd dataframe posthour posts response meanresponse library ggplot2 responsechart ggplot d aes posthour meanresponse + geom_point + geom_line + ylab mean response + opts titlemean audience response by post hour responsechartive tried manually removing hours that have fewer than a certain number of observationsthis seems unsatisfactory and furthermore is hard to automate across widely differing data setsshould i remove these hours if so what statistical tools should i use to perform this repeatedly and automatically across multiple data sets if the answer is no and i suspect that it might be what should i do to account for these tiny sample sizesconscious that this is possibly statistics 101 stuff it should be clear from this that i have not in fact had any statistical training id greatly appreciate you taking this into account if and when you respond to this question im not even sure that the hourly data represent samples","post"
"401084","","750858.0","this approach is called ecm for expectationconditionalmaximisation see meng and rubin 1993 http tinyurlcom mengrubinpdf","comment"
"49480","","95735.0","zero covariance does not imply independence but independence implies zero covariance more detail is here http statsstackexchangecom questions 12842 covarianceandindependence","comment"
"263","","160.0","you could have mentioned in your answer that matplotlib allows rendering of all typography in the plot with latex so it perfectly integrates visually","comment"
"265572","","665786.0","souptikdhar when you say better results in which way exactly do you mean","comment"
"134209","134194.0","","okay so based on what youve told me you have 18 measurements coming from 6 groups 5 treatment 1 control of 3 plots each that sounds like you would use a oneway anova to test to see if the means of the 6 groups are different however in this case your sample size for each group would be 3 which is small but fine as long as youre not breaking any of the assumptions http enwikipediaorg wiki analysis_of_variance#assumptions_of_anovahopefully this is a good starting point but im not an expert in your field and i dont know whats the convention there","post"
"151760","","289539.0","thanks peter for your remark but let us assume the cost value is a representative value and is valid for scaling up to the total population my main concern is how i handle these two confidence intervals","comment"
"240640","","457764.0","seanv507 heteroskedasticity doesnt bias the ols estimate what i think youre referring to is inefficiency by weighting highvariance observations and lowvariance observations equally the ols estimator has higher variance than is theoretically achievable with something like inverse variance weighting https enwikipediaorg wiki inversevariance_weighting whether you want to use your estimates of sigma2_i in the estimation phase ie for estimating mathbf b depends on how much you believe you know sigma2_i","comment"
"30245","","","how can we bound the probability that a random variable is maximal","title"
"121273","","","i am interested in a few areas with biostatistics and was reading the course catalog at a university that offers machine learning i am taking topology now and i think machine learning uses topology not sure but is machine learning very useful with cancer analytics if so is what fields of math does it require to study machine learning does it only require probability theoryfor me right now i am going into graduate studies in biostats and am interested in learning markov modeling regression modeling and linear algebra i think linear algebra is very very useful for doing multivariable analytics and large data setsetcmy concern is that i can not master everything it would be too difficult to master linear algebra machine learning statistical regression etcso i just am inquiring about input regarding machine learning and how to prepare for it if and only if it is highly regarded thank you ac","post"
"384936","","724271.0","martijnweterings why dont you answer the question why is it not a theory rather than just criticizing my attempt a doing so if you think that calling the lemma a theorem is rediculous why do you not say that if so the question is incorrect i took a stab at it all you did was criticize try to contribute a bit more","comment"
"22986","22931.0","","if you have a discrete distribution you would want to use pearson chi2 test x2 sum_ k1 k frac n_knp_k 2 p_k where p_k is the known population probability and n_k is the sample frequency frankly i doubt you will get much value of this test though as with a single testing you will never know if you have genuine differences or just type i error what you could do though is to break your series into shorter chunks compute either the x2 or pvalues and test them against the target distribution chi2_ k1 for the test statistic u 0 1 for the pvalues using kolmogorovsmirnov testi totally agree with whuber that working with moments is pointless moreover for high skewness and kurtosis the sample estimates may not be informative there are upper limits on these sample statistics so a kurtosis of say 10000 will never materialize in a sample of n50 observations see cox 2010 this is a mathematical curiosity that i have never seen in my graduate coursework although i now think it is an important caution against using the sample moments indiscriminately","post"
"414735","","774186.0","glen_b right the preference is to avoid models that are explicitly contain some level of sequential dependency even something relatively simple like an hmm","comment"
"12626","","798227.0","link didnt work for me instead i used this https wwwrdocumentationorg packages clue versions 0357 topics cl_predict","comment"
"355614","","764959.0","you dont even have to crossvalidate modern packages for estimation of penalized splines can also directly optimize ml or reml criterion eg mgcv in r","comment"
"7879","","12984.0","steffen id love to accept this as answer but i may need a little more help so that i can get start implementing it asap due to shortage of time i have started reading the chapter 2 of book you recommended it has multiple subsection under that chapter im extremely sorry but could you please guide me which method actionvalue methods softmax action selection etc is one i should go with and how would it map to my problem i am really running short of time thats why i am going shortcut again sorry for this i hope you understand the situation","comment"
"238397","","","central limit theorem proof not using characteristic functions","title"
"26214","","","inconsistent results from attempting to count matching values in lists within data frames in r","title"
"48086","","","algorithm to produce autocorrelated uniformly distributed number","title"
"32166","","63031.0","welcome to this site a working example or short illustration would be highly appreciated","comment"
"284886","","545213.0","do you know if there are other welldefined ways for constructing confidence intervals which ensure the answer is always within the restricted space","comment"
"71867","","139599.0","sorry not to respond on a more timely basis i have had no access to this site for 24 hours the paired t test was indeed used twice the first time was pretreatment and in this situation the objective was to maximize the similarity based on various sociodemographic and business indicators between the pilot and control stores in this situation you wanted to get a p value as high as possible suggesting very little difference between the stores there was a second paired t test conducted after the treatment my main concern was the use of the paired t test in those situations","comment"
"114132","","","cross tabulation of two variables with resulting average and sd","title"
"155862","","297391.0","quantopic in that case thank you this has proven to be such a teethpulling painful exercise in pedantry that i failed to notice but then all too many of the threads on this se site are like that as for the dupe thats not mine i dont have enough points to do anything like that","comment"
"193000","","366773.0","it is unclear to me what you are asking what are the estimates m and b from python the mean or the intercept and the slope or and what link functions have been used in the python estimates for the vuong test mod_pois and mod_nb_hurdle have not been defined but quite likely the two models are nested making the vuong test not the right kind of test for the question a reproducible example and a better explanation what you think should be correct might make it possible to give a useful answer","comment"
"372554","","","i have done the following bit myself#prediction using mixture cure modellingpdsmcure surv line_tenure_in_days churn_flag months_on_device cureform months_on_device datadf_acn4 model ph nboot100 vartrue printsmcure pd predm predictsmcure pd newxc 0 1 newzc 0 1 model ph predm_predictpredmpredictionasdataframe predm_predict uncure_probpredmnewuncureprobasdataframe uncure_prob plotpredictsmcure predm modelph now my aim is to obtain the individual survival probabilities at a fixed time instance using mixture cure model for which i am using this smcure in rquestionsi am well aware of the mixture cure model see https deepbluelibumichedu bitstream handle 202742 65901 j0006341x200000227xpdfsequence1isallowedy titled estimation in a cox proportional hazards cure modeli am well aware of the smcure in r i also know the breslow type estimator to calculate lambda_0 t y1 sigma_ it_ile t dfrac d_i sigma_ l in r_i w_l m e z_ltbeta and finally s_ pop tx z z s tx + 1 z now how do i obtain lambda_0 t y1 in r for the mixture cure modelhow do i obtain s_ pop tx z z s tx + 1 z for individual observations using the smcure in r by this i mean the following is there an equivalent of predict smcureobject typelp that which we used to do find out the linear predictors in cox ph model for individual observations i give you how i obtained the individual survival probabilities using coxph in rcox_jan_aug coxph surv line_tenure_in_days churn_flag months_on_device+subscriber_activity_price_plan_code+months_on_price_plan+price_plan_change_flag+total_mou+total_calls+market_name+byod_indicator1 method breslow summary cox_jan_aug pred_jan_aug survfit cox_jan_aug plot pred_jan_aug funfunction y log y 1y ylablogit s t colred plot pred_jan_aug main cox regression plot colblue width1 height1 abline h 05 colred summary pred_jan_aug pred_coef_times_vars4 predict cox_jan_aug type lp basehaz_jan_aug basehaz cox_jan_aug predicted_on_15thday4 asdataframe exp basehaz_jan_aug 16 1 exp pred_coef_times_vars4 colnames predicted_on_15thday4 predicted survival probability on 15th daypredicted_on_20thday4 asdataframe exp basehaz_jan_aug 21 1 exp pred_coef_times_vars4 colnames predicted_on_20thday4 predicted survival probability on 20th daypredicted_on_30thday4 asdataframe exp basehaz_jan_aug 31 1 exp pred_coef_times_vars4 colnames predicted_on_30thday4 predicted survival probability on 30th dayindiv_prob_jan_aug cbind predicted_on_15thday4 predicted_on_20thday4predicted survival probability on 20th day predicted_on_30thday4predicted survival probability on 30th day line_tenure_in_days total_mou churn_flag months_on_device months_on_price_plan price_plan_change_flag total_calls byod_indicator1 subscriber_activity_price_plan_code writecsv indiv_prob_jan_aug file individual probabilities on jan through augustcsv a few wordsplease try not to downvote this post for not posting the original data hereplease try to use any standard data like e1684 dataset","post"
"29527","","56113.0","the sum of two covariance matrices is positive semidefinite the eigenvalues are nonnegative","comment"
"254048","","484254.0","many thanks would you be so kind and show the steps in between i do not see how your get 051 and the other solutions many thanks","comment"
"29828","","57014.0","continuation that is why it might be helpful if they had good means for assessing probabilities for the upcoming events better than average gambler","comment"
"62473","","119822.0","by computing hat e hat e","comment"
"347416","","","bridge regression coefficient estimate hat br are the values that minimize thebegin equation text rss + lambda sum_ j1 p beta_j q end equation where q in mathbb r and q 0 my question is why this kind of regression called bridge regression i know that in 1993 frank and friedman proposed this in 1 however at that time in that paper there was no term like bridge nor bridge regression confusingly just 3 years later in 1996 robert tibshirani in the paper 2 cited the paper 1 using the term bridge viz in section 11 frank and friedman 1993 discuss a generalization of ridge regression and subset selection through the addition of a penalty of the form lambda sum_ j1 p beta_j q to the residual sum of squares this is equivalent to a constraint of the form sum_ j beta_j q le t they called this the bridge emmm they called when the word bridge even do not occur in 1 i search on google scholar and find no more paper before 2 citing 1 so where the word bridge come from do i miss something importanti think my question might be related to why is ridge regression called ridge why is it needed and what happens when lambda goes to infinity referencesa statistical view of some chemometrics regression tool pdf regression shrinkage and selection via the lasso pdf","post"
"139708","","279023.0","this is very close to being a duplicate of the linked thread python vs r is not an important distinction here but the r2 aspect is new it might be a good idea for question and answers to focus a little more on that aspect to avoid duplication i wonder whether r2 is prone to being misunderstood since even for poor fit the upwards slope thats inevitable in a qq plot means we expect an r2 somewhat larger than zero so values that might be quite impressive in a regression analysis may not be quite so impressive here","comment"
"375661","","","let x y z be random variables where zx+y and x y are independent by bayes law begin align p xx zz dfrac p zz xx p xx p zz dfrac p yzx p xx p zz end align such that we can compute this by just knowing the pdf of x and y this is because p zz can be found by computing their convolutionhowever when i see that i used p zz xx p yzx in going from the first line to the second it makes me think that i could have just used p xx zz p yzx initially clearly thats incorrect because it doesnt match up with the formula above but i cant figure out why can anyone intuitively or mathematically show why p xx zz neq p yzx when zx+y","post"
"307831","","585198.0","ttnphnsas i stated i have also presented this data using kmeans clustering","comment"
"336085","","635286.0","very helpful +1 why not use the information in abcdfw to extend this to ordered logit","comment"
"90512","","463787.0","to say that this estimation procedure does not require normally distributed errors would make sense when modeling categorical ordinal outcomes as continuous variables which would make sense if the sole interest was estimating a mean difference if data are binary their mean is a proportion and the resulting model estimates proportion differences however other probability models do not require normal error assumptions like a logit or quasilogit model are you saying that sem will not use a mean variance relationship to improve estimation with categorical outcomes","comment"
"86606","","169037.0","if you have the mean and covariance matrix sampling from a multivariate normal shouldnt require mcmc","comment"
"252700","","481169.0","this video helped me once to fully and thoroughly understand svms https wwwyoutubecom watchv_pwhiwxhk8o yet no explanation of weight vector and bias just great explanation of the whole concept","comment"
"70050","","135549.0","wikizero when youre talking to someone in particular put the symbol just before his her pseudo and then he she will be notified for example probabilityislogic is notified if i do probabilityislogic","comment"
"394830","","742363.0","came across with the following paper accidentally which illustrated the similarities of cost and time therefore survival analysis can be used in analysis of cost but also touched upon the problems of using survival models as their underlying assumptions are often violated in cost data https academicoupcom intqhc article 23 3 331 1792904","comment"
"122130","","232807.0","there must be a number of similar questions already have you searched factoranalysis repeated measures or pca repeated measures","comment"
"181707","","344973.0","please add the selfstudy tag read its tagwiki http statsstackexchangecom tags selfstudy info and modify your question to follow the guidelines on asking such questions in particular youll need to clearly identify what youve done to solve the problem yourself and indicate the specific help you need at the point you struck difficulty","comment"
"58766","","264288.0","a model is a set of distributions something essential is missing there","comment"
"226672","","","how few training examples is too few when training a neural network","title"
"302435","","575063.0","in order to solve xor problem with a shallow neural network you have to iterate quite a lot how many epochs have you tried","comment"
"245923","","467868.0","drey i had same idea but whats the next step assuming the residual density is far away from normal","comment"
"226206","","428597.0","according to wikipedia in german and in catalan wilcoxon test assumes symmetrical distribution answers to http statsstackexchangecom questions 14434 appropriatenessofwilcoxonsignedranktest give somehow conflicting statements about it therefore is skewness a problem for wilcoxon text if it were it would be less suitable for this question","comment"
"166660","","316187.0","unless the kernel is linear","comment"
"253179","","482317.0","yes but to do a mixed logistic model you need to use in sas glimmix or nlmixed the terminology is very confusing logistic regression is linear if you take the logit as the dv it is not linear in the original dv","comment"
"194205","194192.0","","the formula is incorrect interaction + icond mi if you draw venn diagrams this formula should be evident though you should be quite careful with threevariate venn diagrams because if they do offer a valid way to get the correct formulas they do not offer a faithful representation of it quantities anymore mainly because interaction informations can be negative","post"
"413366","","771899.0","op says something like and u apply bayesian inference on 4 p values","comment"
"221325","","","derive z or tstatistic from pvalues of regression coefficients from a probit logit model","title"
"190535","","362697.0","the problem states that the two classes have a common covariance matrix that was produced by independent characteristics with variances va1 and vb2","comment"
"235203","","446932.0","thanks for the help ben i tried including the random effects you suggested scorespeciessexday + day subject and i ended up with the error error number of observations 155 number of random effects 162 for term day subject the randomeffects parameters are probably unidentifiable any idea what this means","comment"
"90134","","176406.0","bayerj good question the prior is mvn but the likelihood isnt i think that limits my options","comment"
"47412","","92325.0","are you sure because if n_1 is greater than one it means that it can take all values but not one which is 1p n_11","comment"
"287159","286645.0","","i understand you to want the correlation between creativity for a single teacher and emotional intelligence for a single student however you have measurement of emotional intelligence for 4 students of each teacher and could calculate an estimate of the correlation in a large number of ways you could throw away three students for each teacher but this doesnt seem helpful if you take the average of the four students values for each teacher you can compute the correlation of x creativity with bar y average emotional intelligence this is not what you want however but it seems close and you use all the information about students in one estimate so lets look at the covariance of x with bar y this iscov x bar y cov x 1over 4 sumlimits_ i1 4 y_i so cov x bar y 1over 4 sumlimits_ i1 4 cov x y_i there is no reason to distinguish one measurement of y from another they all have the same variance and the same covariance with x socov x bar y 1over 4 4 cov x y cov x y this is just the numerator of the correlation of a single observation of y with a single observation of xif you calculate the correlation of x with bar y using standard statistical software then you will get the covariance above divided by the standard deviation of y and the standard deviation of bar y this isnt correctget the estimate of the variance among the bar y _is from a onefactor anova of them treat the n teachers as the treatments and the scores of students the y_is as the observationsin the random effects model y_ ij mu + tau_i + epsilon_i both tau and epsilon are random effects the first with n 1 degrees of freedom and the second with k n1 degrees of freedomthe variance of a single observation is var y sigma_ tau 2 + sigma_ epsilon 2i suggest you use the estimate of this for the denominator of the correlation this can be estimated from the mean squares of the random effects anovathe expected value of the ms_ treat sigma_ tau 2 + k sigma_ epsilon 2the expected value of the ms_ error sigma_ epsilon 2the estimate of sigma_ tau 2 is ms_ treat kms_ error then the estimate of the variance of a single observation of y is ms_ treat k1 ms_ error the square root of this should be used in the denominator of the correlation the correlation estimate iscorr x bar y cov x bar y var x msy_ treat k1 msy_ error 1 2 note that if the students all have identical scores on y then the ms_ error is zero and the estimate only uses the ms_ treat if the individual students of a teacher vary a lot then it is efficient to get more students per teacherit might not help here but there is an obvious generalization to several measurements per teacher each variance for x and y would be estimated from an anova this all assumes an equal number of student observations for each teacher","post"
"13918","","24693.0","two more comments i this was in the notes i linked but i think missing at random mar is a key idea maybe for you values are just randomly missing but usually theyre noteg survey data in india more likely missing for more distant rural households in my friends project ii above all id check any final estimate with imputation against the estimate you get when dropping the obs with missing valuesthough even that is biased if mar is not true as the notes discuss so unfortunately no easy way out w o mar","comment"
"369774","","694876.0","its 145am here im going to bed as whuber indicated i probably have not given enough information to answer this satisfactorily i will close the question tomorrow if you wish","comment"
"14781","","26420.0","thanks i sure did check i only found one directly relevant question http statsstackexchangecom questions 14691 kendallswformultipleobjectsandmultiplesurveys but it has received no answers","comment"
"53393","","105455.0","ttnphns my claim is one cannot compute w because one cannot estimate hatsigma and you disagree so i dont understand your disagreement im not sure about my claim may be theres another formula for w which do not require the computation of hatsigma but since h_0 is about sigma i would not be surprised that one needs hatsigma to compare h_0 with h_1","comment"
"181610","","345465.0","thanks but i really wasnt asking for a reading list i was asking for a claim to be substantiated you have the data now and context in the question and further comments the bounds of 0 1 would have to be respected by any serious model","comment"
"47627","","92543.0","many thanks for getting back to me help much appreciated and makes sense thanks for speaking plain english back","comment"
"91866","91853.0","","caution i am not a sas usermy understanding is that ols is equal to mle when the assumed distribution is normalyou are falsely opposing ols and mle ols estimates are mle parameter estimates ols models are appropriate for the class of models where you are modeling mathbb e y x sim mathcal n beta_0+beta_1x epsilon2 but there are many alternative cases these alternative cases are what genmod was designed forwhen there are not weights specified ols parameters are maximum likelihood estimatesis there something about the use of the log link function that no longer makes olsmle ive only ever read that equivalence is simply based on the assumption of a normal distributionyes that is the manifest purpose of genmod with nonidentity links to specify nonols models function genmod constructs generalized linear models glms this is a class of models where the expectation of the response may be of an alternative distribution than would be assumed under an ols model you can still use the glm command to build an ols model simply specify the identity link as you have already doneselecting a log link in the glm is telling sas that you want to build a specific kind of model one in which mathbb e y x simexp beta_0+beta_1x for a poisson regression model the log link is the canonical link and transforms your linear predictors to be on the appropriate scale of the assumed responsethere are of course other link functions the most commonly used in my experience is binary regression in which the response variable is a yes or no or a 1 or 0 or any dichotomous outcome in this case the link function is text logit and you are performing inference on the factors which cause our outcome to appear as 1 or 0parameters of glms are estimated using mle procedures so the coefficients will have the desirable qualities of mles provided that the usual assumptions are met","post"
"302342","","574960.0","i think this link http wwwjohnuebersaxcom stat likerthtm provides some clarification about the meaning of likert scale likert item and likerttype item you can see that one criterion is verbal labels are bivalent and symmetrical about a neutral middle could you share the cv posts you mentioned suggesting dropping neutral middle category without a specific reason it does not make sense to drop middle category","comment"
"112153","","214990.0","look at the documentation for the weights argument of lme which allows a call to a varfunc object and in particular varident which allows a different variance for each level of a factor by the way your lme call shows the random argument but you also need a fixed argument","comment"
"171501","","324090.0","student001 oh 2 should have an exponent thanks","comment"
"188087","","357039.0","hint how many possible collections of center points can there be","comment"
"47385","","","out of one pool ive randomly selected a small group group a and put everyone else in group b then im observing whether or not they take a desired actionif im looking to test at a 95 confidence whats the best methodology and stats test to use i read that with a large enough sample its ok to use a standard ttest but can someone tell me if thats accurateassuming it is ive applied it and come up with the result that group a has a much higher rate of the desired action than group b and applying the error bars its still showing that group a group b albeit a smaller rate this is highly unlikely at worst the expectation was group as actions would be more or less the same to group bany comments on what would cause such an unlikely result even with a high confidence level","post"
"152000","","290030.0","a bayesian network can be represented as a graph","comment"
"378115","","712930.0","currently it is not yet available to include weights","comment"
"251655","","","i have a group of images all contain some common object lets assume all at the same size and no other alterations i want to train a cnn that will learn the filter for the common object and gives me back a boolean flag of whether it exists in a given image or not for simplicity i assume the object is smaller than 5x5 pixels i thought of something likea conv layer with 1 filter of 5x5a maxpooling layer with pool_size input size meaning returns a single value the maximal activation of the filter output max activation or a variation of it some questionswould you output the raw max activation and use a threshold to decide sigmoid of it or force the network to output object and no_object classes and do softmaxquite related what loss function is it a problem to have a large maxpooling layer one that reduces all activation to a single one what happens to the gradient on backpropit seems like it would just go randomly to one of the activations and im not sure that it can learn the object that wayim wondering how to look at this problem as its not really a 2class classification problem even if i can come up with images that doesnt contain the object there is nothing to learn from this class except of the fact that it doesnt contain something which is a valuable information here i must agree ive tried playing with it on simulated small blackwhite images and a cross object that i randomly added couldnt get the filter to learn the cross pattern although on some tries it got the horizontal lineany idea would help thanks","post"
"412464","","770199.0","im trying to find it perhaps i am misremembering as i have found a couple of similar statements about proof of convergence for q learning which has been done about offpolicy mc control and about td vs mc","comment"
"88476","","","looking for a proof that overfitting a model leads to greater variance estimates under ols","title"
"29355","","55762.0","so dikran do you see we are talking about a different bias i am talking about bias in an error rate which is a probability between 0 and 1 you are talking about model selection criterion which is more like what i call testing that a parameter is 0 please explain how regularization works","comment"
"45575","","88421.0","in some circumstances yes but in others no","comment"
"16094","","28907.0","did you delete the so version its best to have just one copy of a question floating around the se site at a time","comment"
"221631","","419220.0","vivekkalyanarangan i do not think that it changes anything if you have tickets vs emails the main point is the same in both cases","comment"
"112591","","216029.0","glen_b see those imai papers discussion of sequential ignorability for more caveats than you can shake a stick at its amazing anyone signs up for that assumption once its spelled out","comment"
"51132","","100054.0","i expect the massive uptrend to continue the thing is i want fit a time series using only data that doesnt trend up and then predict data during that interval","comment"
"390","","75944.0","there are a bunch of comments on the black swan and taleb more generally here http statsstackexchangecom questions 35956 talebandtheblackswan","comment"
"272176","","523480.0","vyas my last comment was not supposed to be harsh i trust that from your side there is no illintent and that you are truly appreciative of answers that resolve your many conundrums however you are unfamiliar with how those open source platform work nevertheless i see that you have not yet acted on my not so subtle nudging i encourage you to do so you just will get more and better answers to your questions if you show signs of life on the appreciative side","comment"
"431693","","805410.0","very important the 52 company are across the years always the same this means that it will be a matched correlated test an extension of mcnemar but not bowker test because bowker applies to a situation where only 2 years are compared but we compare 4 years","comment"
"177045","","","i want to predict gm volume in a group of patients based on their degree of cognitive impairment corrected for age and sex to have a more disease specific measure of cognition i use cognitive performancescores in a large group n500 of healthy controls hc as a reference me and my supervisor discussed two methods for doing this the wscore vs the zscore method 1 wscore method a calculate the effect of age and sex on cognitive score in the hc group cognition a + b age + c sex b predict cognitive score in the patient group based on the regression coefficients we found in the hc groupc for each patient subtract this predicted score from his actual score and divide by the sd of the hcs residuals wscore cognitionobs cognitionpred sdres d perform a regression in which wscore predicts gm volume gm volume a + b wscore 2 zscore methoda calculate the mean and sd of cognitive score in the hc group b for each patient subtract the hcs mean from his actual cognitive score and divide by the hcs sd zscore cognitionobs cognitionmean sd c perform a regression in which zscore predicts gm volume using age and sex as covariates gm volume a + b zscore + c age + d sex my supervisor wants to use the wscore method because it is similar to the use of norm tables which are based on a hc group and have corrections for age sex i actually prefer the zscore method because the effect of age sex on cognition in my patient group is different from the age sex effect in the hc group if the logic behind correcting for age and sex is that they are a covariate confounder in my regression ie they directly relate to gm volume and might not be evenly distributed over cognitive scores wouldnt it make more sense to use the zscore method in that way you correct for the actual effect of age sex that exists in the patient group instead of a different effect that only exists in the hc group im very curious about your opinions thank you in advanceanita","post"
"232359","","","should i use hypothesis testing","title"
"167368","","317602.0","hi frank thanks for your reply however the functions i ran are lsmeans m1 pairwisenameorigin adjusttukey and summary glht m1 lsm pairwise nameorigin i am wondering what the differences are between the two and which is better if there exists such a thing m1 is the probit model i ran the first is tukey but i was wondering if there was a formal name for the second and which i should use","comment"
"207759","","397143.0","im struggling with the interpretation of pvalue and confidence interval they are both in agreement but yet cant be used to make one statement about population proportion so what can be said about the population proportion","comment"
"19033","19006.0","","peter is correct in it depends on what software you are using to check this in the survival package with r there is the coxph functionmost assessments of the assumption will involve looking at the schoenfeld residuals if plotted against time there should be no noticeable patternsee coxproportional hazards starting at page 12also if modeling categorical variables you could create kaplanmeier curves for each variables and see if they are roughly proportional to each other","post"
"389355","","734130.0","it would appear that you answered the question before figuring out what was really being asked the meaning only became clear after a series of clarifying comments exchanged over many hours the original question was equivalent to asking should i measure the size of an elephant from its trunk its leg or its tail length and the answer was no you should choose a model that best fits the whole elephant i have not wasted points downvoting your answer","comment"
"134014","","255004.0","i think you may find some answers here not sure if it is enough for this to count as a duplicate http statsstackexchangecom questions 32310 topicmodelsandwordcooccurrencemethods 32350#32350","comment"
"14301","","28114.0","hes only claimed that he can make that number come up more often than it would otherwise if he was thinking of 5 and the result was 2 opossite face should that be considered a worse failure than getting an adjacent number","comment"
"196835","","373226.0","in order to decide which question combinations to add to your model you have to analyze your data deeply to understand crosscorrelation and covariation of different questions and then iteratively try various models in order to find the best one it is a model selection which might be quite tricky","comment"
"179450","","","i run a weightedleast squares regression to account for the heteroscedasticity in my data when i examine the pearson correlations between all predictor variables i cant detect high collinearity however when i apply wls with the inverse of the variance as my weights and then screen the variance inflation factors of the regression i find vifs far above 10 my question is does wls always suffer from multicollinearity in my case as the weighting with the inverse of the variance necessarily leads to higher dependencies of the predictors","post"
"393724","","","using r i would like to fit a coxian phasetype distribution to a vector of waiting times t_i where the mus and lambdas are unknown ie i want to find the most optimal values of the mus and lambdas for reference i want to minimizebegin equation lsum_ i1 n log textbf p exp textbf q t_i textbf q end equation where begin equation textbf p 1 0 0 dots 0 0 end equation begin equation textbf q mu_1 mu_2 dots mu_n t end equation andbegin equation mathbf q begin bmatrix lambda_1 + mu_1 lambda_1 0 dots 0 0 0 lambda_2 + mu_2 lambda_2 dots 0 0 vdots vdots vdots ddots vdots vdots 0 0 0 dots lambda_ n1 + mu_ n1 lambda_ n1 0 0 0 dots 0 mu_n end bmatrix end equation ive attempted using several optimization functions optim seems to be the most appropriate but i keep running into errors so i tried breaking it down into smaller problems for instance when i know the exact values of mu_1 mu_2 and lambda_1 of a twophase coxian distribution i can easily obtain the value of l i used the following code to do thislibrary neldermead library expm t edtotaltimex1 01 # x_1 is mu_1x2 01 # x_2 is mu_2x3 01 # x_3 is lambda_1p matrix c 1 0 nrow1 ncol2 byrowtrue q matrix c x1 x2 nrow2 ncol1 byrowtrue likely vector for i in 1nrow ed likely i pexpm matrix c x3+x1 t i x3t i 0 x2t i nrow2 ncol2 byrowtrue q loglikely vector for i in 1nrow ed loglikely i log likely i l sum loglikely so that was fine however the errors occur when i try to run this code with unknown values of x as shown belowp2 matrix c 1 0 nrow1 ncol2 byrowtrue q2 function x matrix c x 1 x 2 nrow2 ncol1 byrowtrue likely2 vector for i in 1nrow ed likely2 i function x p2expm matrix c x 3 +x 1 t i x 3 t i 0 x 2 t i nrow2 ncol2 byrowtrue q2 the error message i get iserror in likely2 i function x incompatible types from closure to logical in subassignment type fixhad i not received an error message i would have continued with the following codeloglikely2 vector for i in 1nrow ed loglikely2 i log likely2 i x0 c 01 01 01 l function x sum loglikely2 optim x0 l methodneldermead if anyone has any suggestions that would be greatly appreciated also i dont think the neldermead method within optim can take lower and upper bounds of the parameters this is problematic because obviously the mus and lambdas have to be between 0 and 1","post"
"245104","","466272.0","do you want to predict the latest height","comment"
"109585","109566.0","","some choices include weibull gamma including exponential and lognormal distributions possibly with a shiftparameter if theres a nonzero minimum possible time however from your diagram it looks like theres also potentially a discreteness issue if the presentation drawing is reasonably accurate a shiftparameter will probably be requiredif theres a tendency for the times to be highly skew loglogistic inverse gaussian or pareto might be considered it doesnt look to be the case here though","post"
"38101","","75525.0","yes the acceptancerejection method will be invalid for quasirandom numbers","comment"
"90936","","810906.0","+1 gungreinstatemonica glen_b when you write an omnibus test for significant differences between more than 2 proportions it is probably important to add the explicit qualifier in unblocked data the omnibus test for proportions across blocked data is cochrans q test https enwikipediaorg wiki cochran27s_q_test though you can use mcnemars test when you have only two blocks ie the data are paired just as there is vanilla anova and repeated measures anova for approximately normal data with vars there is chi 2 and cohrans q for binomial proportions","comment"
"407207","","761032.0","nickcox thanks sir ill have a look regarding the relative insignificance of normality lets consider homogeneity of variance or independence instead which are presumably much more important again hypothesis testing can only reject them and failure to reject is sometimes incorrectly interpreted as acceptance im curious to learn methods which can prove these properties","comment"
"114093","114078.0","","i would aggregate the data to weekly aggregate numbers assuming that great bad agents have some what consistent call center performance over the six months sometimes aggregating erases the effects of outliers before they can be classified as such this would account for shifts in performance across the total 6 month period as well when it comes to sampling using 80 of data points to develop model and 20 to validate would be a good start can adjust those numbers depending on how big a data set you are dealing with i utilize iowa state papers some times here is a good one on the basics pdf hope you have funupdate just so we are clear you are aggregating by week per customer service rep rightboth models dont fit good you can tell variable fits using the coefficients section of the results significant variables have the stars next to there p value more stars equals more significant typically and lower p value based on that none of your variables are actuallyits good that you are comparing the model vs actual results roc curves capture the model differences pretty well try running this and post what you getlibrary proc g roc admit prob data mydata plot g update its weekly aggregates population wise ie the attrite population and the active population didn do it agentwise because we will have cases when a agent leaves when he was at his peak performance but those are exceptional cases so i thought it would be better to compare the two populations please advise if thats not the correct way of thinking so aw1 is first weeks performance metric aggregates for attrites similarly naw1 is first weeks performance metric aggregates for nonattrites active agentsran the step fuction selects a formulabased model by aic on the bayesglm model and the results are as below aggregating all the agent results together will mean you essentially are over fitting to match the total population metrics and not the agents performance recommend that you tie in the agent level results you mentioned there being a chance that a great agent leaves unexpectedly but for a well run unit that should be a rarity also recommend you change model family parameter tofamily binomial link probit this should give you probability of default for each agent this would","post"
"306955","","","just a simple question regarding the properties of dirichlet distributionsuppose x_1 ldots x_k sim dir alpha_1 ldots alpha_k can we express the distribution for 1x_1 ldots 1x_k in terms alpha_1 ldots alpha_kim thinking about the follow form but not 100 sure 1x_1 ldots 1x_k sim dir sum_ialpha_ialpha_1 ldots sum_ialpha_ialpha_k","post"
"308676","","","python svm why is my gram matrix not positive definite","title"
"377385","","","using maximum of forecasted values to forecast maximum","title"
"89539","","175100.0","p schnell in confirmatory analysis you would determine a priori what terms to keep","comment"
"109279","","209939.0","so what is the general purpose of such a model im really new on this topic","comment"
"125345","","240740.0","to get the noiseonly model onto the stage i guess what youd have to do is make n_rm obj a model parameter and do inference on it youll need proper priors over the objectspecific parameters otherwise youll run into the jeffreyslindley thingy","comment"
"143998","","276773.0","questions on how do i do x using r are best asked at stackoverflow using the r tag http stackoverflowcom questions tagged r i tried recommending your question for migration but this apparently did not work out i assume our mods judged that your question would have been closed there soon as well since there is no reproducible example","comment"
"390957","","","i have a question about how to check if the relationship between the independent variable yt and the explanatory variables t and t2 is lineari fitted the linear regression with the autoregressive time series errors i","post"
"200731","","381516.0","you can start with this answer https statsstackexchangecom questions 121852 howtochoosebetweenttestornonparametrictestegwilcoxoninsmallsampl 123389#123389 as it discusses samplesize issues and robustness of ttests","comment"
"33077","","65473.0","zen yes of course i phrased that wrongly thanks for pointing that out","comment"
"333574","","631624.0","thanks about point 2 since this data is not normally distributed i can evaluate the variance using levene test","comment"
"300615","","571682.0","thank you sami was thinking along the second option too i was going to model the blue data on a curve and try to manipulate it to linearize in using ratios from the trendline in the orange dataspread i am just not awfully familiar techniques so i was trying to figure out what would an expert opinion suggest","comment"
"176718","","334799.0","thanks glen_b i just noticed that i couldnt delete it i am fairly new on se wrt answering and submitting questions hope this serves its purpose","comment"
"43871","","85397.0","if you check the options of libsvm you will see the wi option in your case 5544 64 64 85625 so if your label for failure is 0 and success is 1 then you can add the option w0 85625 to your libsvm command","comment"
"171592","","324245.0","user777 no im not interested in beta distribution","comment"
"34743","","69445.0","thanks statr i dont know an awful lot about stats i have looked at the mgf page if i understand its about expressions of the form e tx where x is a random vector but t is deterministic so in effect its an exponential of a sum of rvs as opposed to an exponential of products so not sure how to proceed from there","comment"
"74353","","145439.0","ttnphns three responses 1 i think i have shown the mechanism of suppression perfectly adequately with my figures the passage that you highlight is merely my suggestion about why people use the word suppression to describe this situation it is not at attempt to explain how suppression works that is what i do at length in the rest of the answer 2 z is never absent in either of these examples in both cases the true datagenerating regression model has effects of both x and z effects which are equal as you note so z is always present but omitted from the fitted","comment"
"124828","","237913.0","yoki they are not necessarily random in fact four of them are known to be skewed distributions possibly beta","comment"
"300024","","570149.0","my assumptions are based on this https statisticslaerdcom spsstutorials dichotomousmoderatoranalysisusingspssstatisticsphp should i then move the moderators to the first block is this what you are describing thank you","comment"
"273538","","","why doesnt the use of a forget gate in lstms cause vanishing dying gradients","title"
"40489","","79081.0","interesting idea to use survival analysis but i am not sure i think what one should do depende also on for what use is the resulting model to be put for instance if the use of the model is to provide some weights to use in some analysis then the logistic regression seem the way to go","comment"
"18895","","34166.0","i think the difference in dof for the chi2 should read the other way around full reduced","comment"
"161553","161545.0","","im not exactly familiar with the specific r functions but if theres a bonferroni correction i believe that is likely to be the explanation for example suppose you tested two hypotheses and got unadjusted p 06 06 the simplistic bonferroni adjustment would be 12 12 but since these are not valid probabilities it would truncate these to 10 and 10","post"
"256943","","490782.0","all my data is quantitative i added more detail to the question it seems something like pca would be more relevant","comment"
"94538","","184596.0","this isnt an answer but an extension of your question if youre seeking rspecific help thats offtopic here","comment"
"223160","","422233.0","can you post some of your data to give us a better idea of the data youre dealing with you can do something like head acrsci and add it to your question","comment"
"403092","403079.0","","must be some misunderstanding here order statistics can be estimatorsin some cases even best estimators several examples a maximum of uniform in sampling from mathsf unif 0 theta the maximum observation is mle for theta even if biased for example if n 10 and theta 5 then w x_ 10 max x_1 dots x_ 10 is mle and wprimefrac n+1 n whas e wprime theta 5 simulation illustrates thismxunb replicate 106 11max runif 10 0 5 mean mxunb 1 4999921 aprx e w 5 b sample minimum to estimate exponential mean in sampling from an exponential distribution the minimum observation can be used to estimate the mean although there are better estimators for example if we take a sample of 10 observations frommathsf exp mu 1 then v x_ 1 min x_1 dots x_ 10 then nv is an unbiased estimator of mumn replicate 106 10min rexp 10 1 mean mn 1 100015 # aprx population mean 1 c median of laplace in many situations the median observation can be used to estimate the population median for laplace data the median is mle and unbiasedexample if we take 9 observations from a laplace distributionwith mean and median eta 7 then the sample median h x_ 5 is mle and e h 7med replicate 106 median rexp 9 rexp 9 +7 mean med 1 7000467 # aprs e h 7note on simulation the difference of two independent exponential random variables with the same rate is laplace with median 0 d various percentiles quite generally the pth percentile except possibly for the min or max of a sample is a goodestimator of the pth percentile of the population from which the sample was taken the variability of the estimate is smallest in regions where the density is largest for small samples these estimates may beslightly biased depending on the convention for finding sample percentiles about a dozen slightly different conventions are in common use for nine of the most common see r documentation for quantile here is an illustration of using 40th percentiles of samples of size ten from a normal population to estimate the the 40th percentile of the populationq40 replicate 106 quantile rnorm 10 100 15 4 mean q40 qnorm 4 100 15 1 9664472 # aprx mean of sample 40th percentile 1 9619979 # exact 40th population percentile","post"
"59193","","114284.0","sorry that i did not clealy state my point i would like to extract the slope of the fitted function y x b+mx doing this for the given data set with the least squares method i obtain something like m023 with an error of delta m029 my problem is that the error is larger than the result is there another method to fit a straight line which maybe gives smaller errors","comment"
"345438","","652121.0","this is something i actually started to ponder as i wrote out my answer unfortunately i never tested this out but i think you could probably find a middle ground where both reach the same minimum at about the same speed","comment"
"410100","","","what is the distribution of the sum of a lognormal variable and a logistic variable","title"
"178708","","338581.0","take the extreme case where the two are perfectly correlated cor1 then removing either should lead to no performance degradation im not sure i understand your first three sentences or your last sentence for that matter i just had a guess at the meaning","comment"
"8075","","13255.0","i suppose that doing this for each age group in each region will provide me the samplesize i need","comment"
"10809","","18597.0","whuber i would say that the exponentially weighted regression is a very crude approximation for the particular model you have described but it could well be an exact solution to a different model for the model you describe it would be much better to include the heteroscedastic component due to variation in alpha t or assume that it has no variation and you are dealing with random intercept you are making it seem as though geometric weighting is always suboptimal which is isnt it depends on your prior information","comment"
"137569","137544.0","","so you are estimating an equation liketext use _ jit beta text rate _ jit + text quarter _t delta + text individual _igamma + epsilon_ ijt where text use _ jit is the usage of credit card j at time t owned by individual i and text quarter _t and text individual _i are sets of time and individual dummies you are worried that credit cards owned by the same individual have common shocks errors which makes sense let me give you a quick refresher on clustered standard errors and then i will address your question regarding the benefits of having group dummies to account for clusteringfor the moment forget about the time component for simplicity if you didnt include your individual dummies the error would be a composite of the individual error the group error and the card level error epsilon_ ij nu_ i + eta_ ij with such an error structure your intraclass correlation is then rho frac sigma2_nu sigma2_nu + sigma2_eta comparing the variances of the usual standard error and the clusterrobust standard error clustered at the individual level you would getfrac var clust widehat beta var ols widehat beta 1 + n1 rho which would tell you by how much the usual standard errors overestimate precision by ignoring the within group correlation see for instance slide 17 and 18 of these notes if you include individual dummies ie individual fixed effects this would address the problem in that case you cannot include any timeinvariant control variables at the individual level though this only works if your control variables are at an aggregate level here at the individual level for this see page 8 bullet point 3 in these notes however the usage of credit cards of the same individual is probably very highly correlated in this situation the individual dummies will take out too much information in which case it is better to not use those dummies but still the errors would need to be adjusted thats bullet point four in the same notes correcting the errors is easily done by estimating the usual clusterrobust standard errors clustering at the individual levelanother advantage of the clustered standard errors is the time component clustering at the individual level will not only allow for within individual correlation of the credit card usage it will also account for heteroscedasticity between individuals and it will correct for serial correlation given that you have a time component serial correlation may be an issue the error of an individual yesterday is likely to be correlated with the error of the same individual today simply because she is the same person with the same habits shocks etclong story short your way works under certain assumptions the more flexible standard error correction is to estimate clusterrobust standard errors","post"
"47795","","92821.0","this doesnt feel like a programming question but rather one about how to interpret the results of a logistic regression as such i think this is off topic for so i would suggest consulting a statistics textbook or even better a statistician","comment"
"427054","","796779.0","halvorsen thank you for pointing me in the direction of that other answer here is a link to a copy of my working data https githubcom ldurning species blob master ratiobacctxt","comment"
"403490","99171.0","","this paper may help you too improved sqrtcosine similarity measurement visit https journalofbigdataspringeropencom articles 101186 s4053701700836this paper explains why euclidean distance is not a good metric in high dimensional data and what is the best replacement for euclidean distance in high dimensional data euclidean distance is l2 norm and by decreasing the value of k in lk norm we can alleviate the problem of distance in high dimensional data you can find the references in this paper as well","post"
"26340","","48598.0","this is much more transparent than my derivation","comment"
"219793","","","for the question i previously posted here with a general question about elo ranking applied to a a bit weird sport from the ranking pov its a bit like marathon you can have up to n players but unlike ie chess your final ranking is based on single performance on your part and final result is based on the number assigned to your single performanceis it important kind ofi could represent this sport as n2 n pairs this sucks a bit because it makes a lot more important for your elo ranking to win massively populated tournaments than to win several small it may inflate ones ranking based on just onetime fantastic result that just happened to coincide with a massively populated runon the other hand one can take the average of all the results and make each tournament worth just like one tournament one run after all this however leads to small fluctuations of the ranking if the competitions are not really that common which is a case in my case you can offset it by big k parameter but that might lead to very volatile ranking also it feels that 2 competitor tournament shouldnt be worth just as much 20 people tournament after trying both of the above approaches i settled for the latter but with variable k each competition has k growing with the amount of competitors so 1 3 of k is static and 2 3 depends on the number of competitors this kind of looks ok okish maybe but i still have my doubtsand to the question how reasonably verify that youve chosen correct k value so far i just build a ranking plotted it and looked for something i found weird but that doesnt sound mathematical or maybe variable k is not good solutionim aware i can apply suggestion from here how to prove that elo rating or page ranking have a meaning for my set but thats not completely what im asking about","post"
"204839","","388837.0","you can make some arbitrary assumptions and do data imputation but for the sample data posted i dont see the need you do not loose an entire variable but yeah sure you will loose data","comment"
"112251","","215752.0","should answers be exact or approximate","comment"
"13083","","22797.0","sorry that was just lazinessi is the ntimes n identity matrix this is another way of writing multivariate normal its the same as writing 2 pi sigma2 to the power n 2","comment"
"121270","","231217.0","mathematically it is possible and very easy because adding linear constraints to a convex optimization problem preserves convexity we don t really have flexible free qp solver in r but we have good free lp solvers if ladlasso http webhkuhk gdli papers 2007jbes_20lad_lassopdf can solve your problem then add this info to your question because it is easy to do modify ladlasso in r to make it do what you want","comment"
"205515","204559.0","","i would agree with your assessment that there are no comprehensive benchmarks established for mcmc methods this is because every mcmc sampler has pros and cons and are extremely problem specific in a typical bayesian modeling setting you can run the same sampler with diverse mixing rates when the data is different i would go to the extent of saying that if in the future there comes out a comprehensive benchmark study of various mcmc samplers i would not trust the results to be applicable outside of the examples shownregarding the usage of ess to assess sampling quality it is worth mentioning that ess depends on the quantity that is to be estimated from the sample if you want to find the mean of the sample the ess obtained will be different from if you want to estimate the 25th quantile having said that if the quantity of interest is fixed ess is a reasonable way of comparing samplers maybe a better idea is ess per unit timeone flaw with ess is that for multivariate estimation problems ess returns an effective sample size for each component separately ignoring all crosscorrelations in the estimation process in this paper recently a multivariate ess has been proposed and implemented in r package mcmcsevia the function multiess it is unclear how this method compares to the ess of the coda package but at the very outset seems more reasonable than univariate ess methods","post"
"35719","","72114.0","and can any one suggest me which one is best attribute among these esidual standard error degrees of freedom multiple rsquared adjusted rsquared fstatistics or pvalues for choosing best model to choose the best model","comment"
"11741","","20270.0","thanks zach your minimum example count has been my best solution so far but it still feels a bit hacky","comment"
"83286","","620643.0","thank you for those comments namey i can only agree with the example indeed the whole point of my answer as emphasized in its first couple of sentences is to establish some principles and approaches that can be used to optimize an experimental design to meet a clearly articulated set of objectives i did not overlook your examplei simply hadnt the time energy or space on this site to write a treatise that would cover all such cases","comment"
"71676","","","say i have multiple tests on n data points for each i get a pvalue 0 p 1 if the null hypothesis is true i would expect that the distribution of pvalue to be uniform if however the empiric cdf is larger than the uniform diagonal text cdf p p eg there are 01 fraction of samples with pvalue below 001 etc i could use a ks test to test for the pvalue distribution to be far from uniform and thus conclude with one pvalue to the question are there significant point of data which deviates from the null hypothesisis the above methodology valid can you please give me a reference if not do you think this can be written as a method paper","post"
"9940","","","i have n standard normal and independent random variables x_i in reality i have a large known number of them but lets just say i have n in my experiment i want to on average get exactly 3 random variables x_i under a threshold c to get that i can compute c having that property easily because the average number of x_i that are under a threshold c is n phi c where phi is the cdf of the standard normal distributionso i choose c phi 1 3 n in this case which is a negative number for large n but unfortunately i already know the value of two other standard normal random variables y and z which may depend on each other and on any number of the other x_iso my question is if i know that y and z are under the threshold cphi 1 3 n is it then still true that on average at most a constant number of the other random variables x_i are under the threshold c so by knowing that y and z are under the threshold they cant suddenly make many of the other random variables go under it tooi am almost certain that they cant but i dont know how to prove it any hints are welcome or books where you think this might be in","post"
"176471","","334067.0","what is wrong with this derivation the likelihood factorises through this sufficient statistic and this is a regular exponential family","comment"
"48455","","94099.0","my guess is that mancova is a reasonable option for your situation","comment"
"30093","","58568.0","so i input them as logs to the algorithm within i convert them calculate the gradients and i return the log of the gradients","comment"
"45575","","88859.0","i would be glad to anyway this rule is in your course and i suppose nowhere else only your professors can tell you how they expect you to apply it we cant help just advice you that once the exam is over you must always keep a critical eye on the material youve been taught as it can contains some mistakes this happens unfortunately even to excellent teachers","comment"
"41005","","79977.0","if you apply it to past ega minimumvariance kalman filter and a minimumvariance smoother may be used to recover data of interest from noisy measurements the aforementioned techniques rely on onestepahead predictors so still prediction one step ahead the future","comment"
"91911","","180012.0","moving average that you mention was what i was going for with one variable i figured it out after a while partially thanks to you pointing out that its not cumulative but point probabilities gung oneupped this one with lowess","comment"
"326490","","619376.0","if you apply the same prior it means that you literally had not learned anything since you applied it first time you ran once how come didnt your beliefs change at all so you plug the same prior to different data thats the main issue with bayesian approach its conceptually inconsistent in most applications we dont search for the submarines everyday","comment"
"24856","","45322.0","andymckenzie i dont think thats possible but am not sure","comment"
"149051","","285519.0","that is right my answer is not complete","comment"
"301034","","","gradient descent to optimize regularization parameter lambda instead of doing grid search","title"
"249287","","474555.0","either one is possible depending on the model the only way to answer that question would be to refer to the paper","comment"
"434888","","810809.0","you should include the selfstudy tag","comment"
"82651","","","treatment of missing values introduced by padding lagged variables","title"
"132929","","252687.0","my comment is not up to standards for an answer if i find something more to contribute i will return with an answer proper","comment"
"226321","","","improve fit by trend adjustment","title"
"340384","","","structural equation models allow for estimation of complex networks including latent and observed variables and endogenous and exogenous factors when an sem is fit the model results are summarized in terms of a number of fit indices i assume these indices and their pvalues are variations of goodnessoffittests an overview is given by kenny here what measures is an sem capable of generating predicted values forthe site refers to a chi2 value but does not explain how it is generated when there is no clear single y in the model is every single variable in the model taken as an expectedobserved how is it possible for an sem to have a low r2 but perfect fit and what does perfect fit mean in this sense","post"
"31597","","","i have the following probability functiontext prob frac 1 1 + e z wherez b_0 + b_1x_1 + dots + b_nx_nmy model looks likepr y1 frac 1 1 + expleft 392 + 0014times text bid right this is visualized via a probability curve which looks like the one belowi am considering adding a couple variables to my original regression equation lets say i add gender categorical f and m and age categorical 25 and 26 into the model i end up withpr y1 frac 1 1 + expleft 392 + 0014times text bid + 025times text gender + 015times text age right in r i can generate a similar probability curve which will tell me the probability of y1 when accounting for all three predictors where im lost is i want to find the probabilities for every possible permutation of these variationsso when bid 1 gender m and age is 26 what is the probability that y 1 similarly when bid 2 gender f and age is 26 what is the probability that y 1i want to generate a probability curve which will allow me to visualize this can anyone help i may be completely misunderstanding what kind of information one can glean from a logit model but please tell me if i am also misunderstanding the theory","post"
"219120","","","mcmc sampling of tree structure","title"
"90300","","176773.0","support of a distribution is the set on which the pdf or pmf is nonzero","comment"
"332967","","631876.0","whuber after reading help for predict the prediction intervals are for a single observation at each case in newdata or by default the data used for the fit with error variance s predvar this can be a multiple of resvar the estimated value of 2 the default is to assume that future observations have the same error variance as those used for fitting it seems that models error variance is assumed to match residual variance i guess that predvar should be supplied for unbiased estimate question was what to do we do if we had no idea about this value","comment"
"216061","","409924.0","matthewgunn i never knew that i would have assumed there would be a high autocorrelation the more you learn","comment"
"312905","","600023.0","could we say then if we would equip the topological space s 1 with sigma algebra and also the sigma we should be able to speak about measurability of this function right more precisely would it be possible to talk about lesbegue measurability with the right sigma algebras","comment"
"340399","340384.0","","sem doesnt really give predicted values in the sense of say regression you can get predicted values for the latent variables but these are nondeterministic that is there is more than one an infinite number or sets of predicted values this is the same as factor analysis what is predicted is the fitted covariance matrix chi2 is a measure of how closely the fitted covariance matrix matches the sample covariance matrix most commonly maximum likelihood is used in ml f_ ml log sigma + tr s sigma 1 log s pwhere s is the sample covariance matrix sigma is the fitted covariance matrix and p is the number of variables in the modelchi2 f_ ml n 1 yes its very possible perfect fit means that you reproduce the sample covariance matrix perfectly imagine a really simple model where x predicts y in the sample x and y are unrelated your model predicts a zero relationship between x and y the sample covariance matrix will be perfectly matched by the fitted covariance matrix chi2 will be zero and fit will be perfectnote ive been referring to the simpler case here where were only modeling covariances whats sometimes called the extended model can also include means the principle is the same","post"
"48388","48225.0","","for the svm algorthm you can look at that paper probabilistic outputs for support vector machines and comparisons to regularized likelihood methods and others works derived by this onethe algorithm is implemented also in the libsvm library i disagree with the fact that the distance from the margin proportional probability is a good estimator because an example could be distant from the margin and at the same time distant from the class distribution","post"
"111140","","213202.0","ttnphns why do you think that the variance of noise components should decrease linearly in fact i analyzed it once http statsstackexchangecom questions 87032 eigenvaluesofcorrelationmatricesexhibitexponentialdecay 87146#87146 and its more of a powerlaw decay in the middle and quite messy overall","comment"
"96857","","188923.0","spot on","comment"
"163976","","312033.0","i dont understand how you calculated the probability at all doesnt the uniform distribution state that for x in a b p x x xa ba","comment"
"399967","","748916.0","it is difficult to see how you could make any progress knowing only the probability of being an outlier to begin with what would the definition of outlier be second in order to use this probability at all you would need to know the distribution of nonoutliers","comment"
"408339","406399.0","","this is a relatively simple problem the basic model to test your question about differences in slope is m0 lm size time group dat # coefficients# intercept time groupb groupc timegroupb timegroupc # 01700 03930 01482 01032 02890 02956 i have ignored the question about the intercepts more on this at the end also the basic model you ran does not permit testing of differences in slopes if you perform the diagnostic tests you performed on the model m0 here they do not confirm misspecificationhowever it appears group a is on a different scale so it makes sense to run a heteroskedastic model additionally as whuber pointed out in the comments it makes sense to model the autocorrelation i use the simple autocorrelation of order 1library nlme m1 gls size time group dat correlation corar1 form time group weights varident form 1 i group a # coefficients# intercept time groupb groupc timegroupb timegroupc # 01768985 03900313 01543012 01146352 02860587 02912242 # # correlation structure ar 1 # formula time group # parameter estimate s # phi # 05295663 # variance function# structure different standard deviations per stratum# formula 1 i group a # parameter estimates# true false # 1000000 2152732 we find that the residual standard deviations of the groups that are not group a are about double the residual standard deviation for group a and that there is negative autocorrelation positivenegative residual switching pattern by timeto address your primary research questions we can golibrary emmeans pairs emtrends m1 group var time # contrast estimate se df tratio pvalue# a b 0286058700 0005098842 14 56103 0001# a c 0291224187 0005098842 14 57116 0001# b c 0005165488 0003857697 14 1339 03979# # p value adjustment tukey method for comparing a family of 3 estimates we find that there is not much statistical evidence to conclude that the slopes for group b and c are different from each other while there is the evidence to differentiate a from b and a from csince we have an interaction it is difficult to consider differences in the intercept given the current analysis the intercept relates to group differences at time 0 which does not exist in the data minimum time is 1 the emmeans package provide an option to view differences between the groups at different values of timeemmip m1 time group covreduce false we find that as time increases the group differences between a and b and a and c increase but b and c continue to be relatively similar be careful because there are no time point beyond time 4 for group a these are extrapolated valuesgiven what we have learned a parsimonous model would bempars gls size time i group a dat correlation corar1 form time group weights varident form 1 i group a","post"
"354379","","667568.0","you dont need quantum computer to use quantum phenomena to have a random generator https enwikipediaorg wiki hardware_random_number_generator","comment"
"307302","","","i have read many times that a good debugging step while building a machine learning model is to try to overfit your model to a very small subset of your data here is one such instance 1 provided your code is bug free is it always possible to achieve perfect or nearperfect performance on the training set when you do this could you do it even on a small dataset of random numbers i have a model that is achieving significantly better accuracy on my actual data than it does if i feed it random numbers but its far from perfect and it seems no matter how small i make the dataset how many layers i use or how big i make the layers the accuracy stays about the same what could cause thisupdatethanks to folks who responded i understand that it should always be possible to fit a small subset of your data so i took another look at my implementation it turned out there were several small issues switching from random uniform weight initialization to xavier initialization provided a significant bump in my results i assumed this would only improve the speed at which training would converge to the same crappy result but it actually improved the accuracy overall i also did not have fully normalized data everything was in a range from 0 to 10 which i initially thought should be good enough but i got another big bump in performance when i normalized to 1 to 1 a third problem i had was with my validation set my data is in several different sets from different sources and it turned out there were distinct styles or trends to each set i was training on a majority of the datasets and evaluating on one particular set when i shuffled all the individual examples together from all sets and then drew my validation set randomly from the complete shuffled set i started seeing accuracies in the mid and upper 90s","post"
"72033","","139742.0","if you cant make such an argument the discussion that its probably quite large and what that might mean for a test would come after you discuss what you can confidently show","comment"
"260083","","497506.0","apparently the reviewer requires you to post the answer as an edit to the op which means you can include it at the end of the op i am sorry your answer was downvoted i am giving you +1 on the question to compensate for that","comment"
"384255","","","i make a comparison on ridge regression and ols using simulationas i set my correlation as 09 which is high i expect the standard error of ridge regression to be low however it is not different on the coefficient of the ridge regression it is always lower than ols why does the standard error does not becoming lower than ols dont the coefficient affect the standard errori am using r to make my simulation","post"
"379031","","714403.0","isabellaghement i tried what you said converting genotype and treatment infactors and then i did this model","comment"
"280376","","","we have a density x defined as f x theta theta x theta 1 i_ 0 1 x the hypothesis to test is given as followsh_0theta leq1 vs h_1theta 1a sample size of two is selected and the critical region is defined as followsc x_1 x_2 frac 3 4x_1 leq x_2 i tried writing the power function which for a general theta as followsp frac 3 4x_1 leq x_2 p x_1x_2geqfrac 3 4 now since x_1 x_2 both are randoms we have to fix one of them hencep x_1x_2geqfrac 3 4 p x_1geqfrac 3 4x_2 int_01 p x_1geqfrac 3 4x_2 x_2x_2 f_ x_2 x_2 dx_2am i going in the right direction evaluating this gives a log term and hence the value of the integral is coming out to be inftywhat am i doing wrong","post"
"428678","","800086.0","markowitz what then is the time series component","comment"
"324095","","616068.0","once again there is absolutely no meaning to arithmetics of correlations see https statsstackexchangecom questions 105134 sumofcorrelationcoefficientsandcorrelationcoefficientofproduct","comment"
"308066","","585419.0","can you write mu in terms of theta","comment"
"70986","","137869.0","thank you for the very detailed explanation this is now clear for me on a side note i can put this in a new question if you prefer how do you think it is possible to evaluate the effect of an input variable that is actually only implicitly defined in the model eg y is the amount of bacteria on hands after touching n surfaces such that y_ia_iv_il_ib_iy_ i1 where n1i and a v l and b are independently distributed variables is it possible to evaluate the effect of n in the same manner as a v l or b","comment"
"146215","146214.0","","random forest is not not just a bunch of trees each of them is built on a different resample of objects ie it is a bagging ensemble and optimisation of splits is clipped to m randomly selected attributes both procedures are applied mainly to reduce the correlation of individual trees and thus make voting work and in context of a single tree they actually hurt its accuracyalso you have set max_depth of a decision tree to 1 which makes it a dumb singlesplit model rather than a decision treethis way you got unrealistically bad accuracies in both cases","post"
"350140","","660274.0","well good news thank you do you know the r function that does that","comment"
"226541","","429235.0","well i dont know whats happening in industry but if ratings are in range 15 it makes no sense to return a predicted 6 the same if you are in a mlcompetition the knowledge is apriori information which is hard to incorporate into a model of constrained complexity of course one could introduce some special lossfunction and depending on the noise of the data i would expect that there are always some predictions out of range if using matrixfactorization techniques you should check out some netflix competition tutorials papers the work during this competition made a big impact","comment"
"86092","","168799.0","mdmoore313 youre basically saying that the interviewee interview impression provides zero information about the result does not reduce the entropy of the result lets test it empirically we take 1000 people who were interviewed at amazon and felt that they did good on the first interview if they had the 2nd interview you pay me 1 otherwise i pay you 1","comment"
"32295","","63275.0","see ecdf a joke but only in part im sure others will explain","comment"
"392222","","736653.0","yes thats why i reopened the question the record shows that you did not mention this crucial assumption until after i posted my comments though","comment"
"104811","","202559.0","i know nothing about the package but jaccard is a popular coefficient of association for binary data it is like say a correlation coefficient for continuous data","comment"
"229623","","434688.0","mdewey not true many events can have a very large number of medals in the event of ties or special rulings by the officials","comment"
"306347","","582406.0","for oder statistic there should have a upper limit there","comment"
"401918","","","i have an experimental data set where there are a 16 fish in 4 tanks 64 fish there are two treatments heated and control and four tanks heated was done in tank 1 and 2 whilst control in tank 3 and 4 at 4 different time points throughout the experiment day 1 2 3 4 four fish were sampled and microbiomes analysed at four positions along the gut therefore i have 256 samplesmy question is what is the correct formula syntax to input into the manyglm function packagemvabund to represent this sampling designis it treatment time + 1 tank individual gutlocation this is what the hierarchical structure of the experiment looks likeindividual asfactor rep 164 each4 gutlocation asfactor rep c 1 2 3 4 times64 tank asfactor rep rep 14 each16 time4 treatment asfactor rep rep 12 each32 times4 time asfactor rep 14 each64 x dataframe cbind individual gutlocation tank treatment time the sampling was destructive so each fish individual is a different fish that was sampled at four locations along the gut the response data is a typical multivariate community matrix hence why i am using glms fitted to a negative binomial distributioni am really interested in how treatment effected the microbiomes response matrix while accounting for tank effects pooling the 4 subsamples from each individual gutlocation may be necessary and that is ok the study is also longitudinal there should be no difference between the treatments at t1 but i am expecting differences at time 2 3 and 4","post"
"239123","","454749.0","sk1ll3r still its sample drawn from a common distribution","comment"
"391580","","735856.0","my first attempt would be to simulate this on a computer which can flip many fair coins very fast but maybe you need an exact solution","comment"
"342278","","646349.0","thank you very much how do i know if my random sample is large enough is there some test to check it","comment"
"234483","","445055.0","geomatt22 what i found recently about seasonality is a seasonal pattern exists when a series is influenced by seasonal factors eg the quarter of the year the month or day of the week seasonality is always of a fixed and known period and about cyclic a cyclic pattern exists when data exhibit rises and falls that are not of fixed period the duration of these fluctuations is usually of at least 2 years it is quite confused the difference between cycle and seasonality","comment"
"133929","","592583.0","if they are not replaced is something else done cleanup oiling if that is relevant something else","comment"
"48069","","93694.0","the proof is for the discrete case but the continuous version is pretty similar","comment"
"143705","","274922.0","under normal error assumption as is typically assumed in linear regression the mle and the lse are the same","comment"
"321223","","609941.0","a moving average would be a good choice i only heard of it in here but thats probably the best solution for it","comment"
"192057","","365155.0","wolfies completely agree this question is not a duplicate edited the question to reflect that fact","comment"
"178774","","338765.0","your first bullet is now a very interesting guess","comment"
"316072","","600387.0","calimo were saying the same thing if the max and min prediction values on some data set are 09 and 01 then the roc points 0 0 and 1 1 are generated by appending predictions smaller and larger than 01 and 09 resp and then proceeding as usual with cumulative sums or whatnot another way to look at this is that a roc curve is characterized as a nondecreasing curve from 0 0 to 1 1 so appending 0 0 and 1 1 has no significance beyond satisfying this quality","comment"
"29899","","57167.0","user995434 thats a good observation it is addressed in the last line of my reply both calculations are equivalent to pr z gt 1 where z xmean_1 sigma_1 or z xmean_2 sigma_2 has a standard normal distribution of zero mean and unit sd its easy to understand as a change of units its like counting the number of days when the temperature exceeded 86 degrees f and noting that its exactly the same number of days the temperature exceeded 30 degrees c","comment"
"254186","254161.0","","the restriction just says that the variables in x_1 do not enter the regression so that the formula constrains those coefficients to zero and the others then are the standard ols coefficients of a regression of y on x_2here is an implementation in r a more formal answer follows below library lrmest # example datan 100k_1 3k_2 4x_1 matrix rnorm nk_1 ncolk_1 x_2 matrix rnorm nk_2 ncolk_2 y rnorm n x cbind x_1 x_2 # implementing the restriction in lm directlycoef lm yx_21 x_21 x_22 x_23 x_24 002171519 004595167 008971665 004871940 # restricted least squaresr cbind diag k_1 matrix 0 nrowk_1 ncolk_2 r delta rep 0 k_1 rls yx1 r r deltdelta restricted least square estimator estimate standard_error t_statistic pvaluex1 00000 00000 0 1x2 00000 00000 0 1x3 00000 00000 0 1x4 00217 00960 na nax5 00460 01091 na nax6 00897 01093 na nax7 00487 00922 na naboth question and answer make use of properties of restricted ols which are for example discussed further herein particular the restricted ols estimator is related to the unrestricted one viabetahatbeta+ xtx 1 rt r xtx 1 rt 1 rrhatbeta algebraically the mistake seems to be inrhatbeta r i_l0 hatbeta which is just hatbeta_lin more detail denote begin align xtx 1 left begin array c c x_1tx_1x_1tx_2 x_2tx_1x_2tx_2end array right 1 equivleft begin array c c tilde atilde b tilde ctilde dend array right end align then r xtx 1 rt tilde aand xtx 1 rt left begin array c tilde a tilde cend array right leading tobegin align beta left begin array c hatbeta_l hatbeta_kend array right left begin array c c tilde a tilde cend array right tilde a 1 rhatbeta left begin array c hatbeta_l hatbeta_kend array right left begin array c c i tilde ctilde a 1 end array right hatbeta_lend align thus using hatbetaleft begin array c c tilde atilde b tilde ctilde dend array right left begin array c x_1tyx_2tyend array right we havehatbeta_ktilde c x_1ty+tilde dx_2tyandhatbeta_ltilde a x_1ty+tilde bx_2tyso thattilde ctilde a 1 hatbeta_ltilde ctilde a 1 tilde a x_1ty+tilde ctilde a 1 tilde bx_2tyand begin align hatbeta_ktilde ctilde a 1 hatbeta_ltilde c x_1ty+tilde dx_2tytilde ctilde a 1 tilde a x_1tytilde ctilde a 1 tilde bx_2ty tilde dtilde ctilde a 1 tilde b x_2tyend align now use results for partitioned inversesbegin align xtx 1 left begin array c c tilde atilde b tilde ctilde dend array right left begin array c c ab cdend array right 1 left begin array c c eebd 1 d 1 ced 1 +d 1 cebd 1 end array right end align where e abd 1 c 1 hence begin align tilde dtilde ctilde a 1 tilde bd 1 +d 1 cebd 1 d 1 cee 1 ebd 1 d 1 end align the result is complete by noting thatd 1 x_2tx_2 1 so that hatbeta_ktilde ctilde a 1 hatbeta_l x_2tx_2 1 x_2ty","post"
"125636","","240339.0","glen_b if by concerns you mean wrong bucko then i agree with you after reviewing it not sure what i was thinking stphane lauren","comment"
"85609","85605.0","","they can look but not touch after all the residuals are the part of the data that dont carry any information about model parameters and their prior expresses all uncertainty about thosethey cant change their prior based on what they see in the datafor example suppose youre fitting a gaussian model but notice far too much kurtosis in the residuals perhaps your prior hypothesis should have been a tdistribution with nonzero probability over low degrees of freedom but it wasntit was effectively a tdistribution with zero probability everywhere except on infinite degrees of freedom nothing in the likelihood can result in nonzero probabilities over regions of the posterior density where the prior density is zero so the notion of continually updating priors based on likelihoods from data doesnt work when the original prior is misspecifiedof course if you google bayesian model checking youll see this is a parody of actual bayesian practice still it does represent something of a difficulty for logic of sciencetype arguments for the superiority of bayesianism on philosophical grounds andrew gelmans blog is interesting on this topic","post"
"337852","","638480.0","are you including the breakpoints in the estimation procedure or are they externally specified","comment"
"157148","","","time to completion","title"
"134701","","256470.0","like closetoc i wonder if youve come across regressions where on the one hand youve been told hat beta is asymptotically normal but on the other hand you have been told it converges to the true beta","comment"
"262956","","503670.0","wh thats a good reason why the gamma function should be chosen to be that way and i already suggested such a reason existed above and i accept some form of reasoning akin to that but necessarily with different formalism came into eulers choice correspondingly compelling reasons occur with the density but that doesnt establish that this was actually the reason for the choice why the form was chosen as it was only that its a good reason to do so the form of the gamma function ctd","comment"
"31888","","127994.0","its easy to see the mean minimizes the l2 norm where can i find a proof for the median and mode to minimize l1 and l0 norm","comment"
"189585","","360627.0","thanks bernardoaflalo this will indeed give a good approximation of this and is what we are currently using but i wonder if there is a more accurate way to directly calculate the value","comment"
"415531","","775708.0","i am not certain because i havent tested the python applicationbut it looks like thats something you could readily check","comment"
"306286","","582239.0","i suspect you will only get educated guesses concerning what drawing probability could possibly mean your taggaussianmixture tag is a hint but it strongly suggests you havent supplied enough information have you consulted the person who set you this problem to ask for clarification if theyre not available could you please quote the problem exactly","comment"
"126866","","241911.0","why the first throw might be 2 4 6 then the second throw might be 3 3 3","comment"
"76643","","149760.0","what does p t_i represent in that law of total probability calculation","comment"
"99508","","193048.0","i have never seen this kind of notation with the error term but it is the general probit p y1 x1 phi beta_0+beta_1x so i guess it is option b","comment"
"331792","331780.0","","data is either paired or not consequently it is either appropriate to use the paired testwilcoxon signedrank w test or the unpaired testmannwhitney u test aka wilcoxon rank sum test but never both it is possible to ignore pairing and erroneously use the less powerful rank sum test that has the tendency of spuriously increasing the probability of significant results and decreasing the probability of highly insignificant results and although that might have meaning in some other context eg contrasting wilcoxon with ttesting for example it has no meaning hereit is difficult to imagine a common scenario in which one would mistakenly use the paired test on unpaired data as typically the groups contrasted have unequal sizes frankly i do not know what would happen if the sizes were equal and one mistakenly used the paired test it is somewhat pointless to ask as well as one should not be doing thatif one wants to see effect size despite having a motive for not doing so ie it is an error one might think that one could do a monte carlo simulation to explore it my personal problem in doing so is that if i start with a false premise the result of such a simulation would not be logical which makes it conceptually difficult for me to evolve appropriate other assumptions to use to set up that simulation perhaps that is only my personal problem so i ask and what exactly does one assume to set up such a test for example randomly correlated pairing if so randomly correlated how should the correlation itself be modeled as random or should it be random correlation that arises from random data","post"
"306577","","582827.0","i did not say that in general no only for entries in bigoh that tend to zero","comment"
"381340","","716996.0","do you mean given y_i y_j x_i x_j are independent from each other and alpha and beta are constant prove that cov u_i u_j 0","comment"
"218797","","","tl dri have to decontaminate a training data set that includes irrelevant observations that will harm the quality of any statistical estimation and inference it is of course initially unknown which observations must be discarded and no outofsample information to help i can obtain using the sample itself an imperfect criterion for this purpose and then apply it iteratively and simulations show that the procedure is successful but i would like to obtain some theoretical justification as to why this worksare there any theoretical results on the task of decontaminating a training data set using the training data set itselfts dri am trying to match my situation to any available theoretical results related i guess to binary classification and data sample decontamination the situation i have a sizen sample mathbf y mathbf x where y takes only the values 0 1 mathbf x is a matrix of any kind of random variables the sample is comprised of two subsamples neither the population proportions nor the sample proportions of the two subsamples are knownfor the one subsample say a the relation mathbf y_a g mathbf x_a beta + mathbf e holds where beta is an unknown parameter vector and mathbf e is a conditional expectation function error for the other subsample say b y_ bi is a constant either of value 0 or of value 1 so i cannot use y to separate the subsamples essentially i have three subsamples but i can initially lump the two of them together for my purposes the two subsamples come initially from the same population what links the two is that at an underlying level if g x_i beta in s for some known compact realline interval s then the ith observation belongs to subsample a if g x_j beta notin s then the jth observation belongs to subsample b my purpose is to estimate beta so the observations belonging to subsample b are irrelevant for this purpose and contaminate the sample i am also interested in obtaining an accurate classifier to be able to characterize the quantitative features of the subsamples subpopulationsi can use the whole sample to obtain estimates hat beta but of course since the estimator will use also the irrelevant observations it will be a biased and inconsistent estimator also the consequent series of estimates g_i x_i hat beta i1 n appear to be of unknown classification accuracy obtaining g_i x_i hat beta in s does not necessarily mean that g_i x_i beta in s neither g_i x_i hat beta notin s guarantees that g_i x_i beta notin sbut all simulations i have run show that an iterative cleanup operation is successful if i use the estimated hat g_i series as a binary classifier in order to discard observations those for which i get hat g_i notin s and then apply the same estimation procedure to the reducedsize sample obtain new hat g_i discard observations reestimate etc i end up with a clean sample namely a sample that contains only observations that belong to subsample a i may have lost some relevant observations in the process due to wrong classification but thats a small price to pay since in this way i do get consistent estimates for betato be accurate i end up with an almost clean sample and almost consistent estimates for beta but this has to do with the finite sample when i run the simulations with very large sample sizes i end up with essentially perfect estimates for beta even though in the process i may have discarded half of the observationswhat is my problem i want to prove that this iterative procedure even if it uses initially a biased inconsistent etc binary classifier improves on itself by making the sample less and less contaminated the classifier more and more accurate and so the hat beta estimator more and more consistent and acceptable or at least obtain the conditions assumptions under which this happensi note that the usual setup where we start with a training data set and then apply the classifier on new observations is not the case here here we start with all available observations obtain a classifier and then we apply the classifier to discard observations from the initial sample i guess what i am doing here can be described as decontaminating the training data setare there any theoretical results on such a task","post"
"387788","","742193.0","use a as an offset in your model search this site","comment"
"153067","","807511.0","jess i had hoped that this answer would be understood as showing among other things that there is no such thing as the formula there are uncountably many correct formulas for the a_n and b_n","comment"
"320415","","608356.0","viktor time takes values only on integers for the purposes of this question","comment"
"24654","","45018.0","i figured as much david but it also occurred to me that your ability to make observations could be related to weather conditions and other factors that themselves might influence the time of bud burst so although the process of choosing the sampling dates may have been considered to be independent of the process of bud burst the two could still have a strong statistical dependence","comment"
"234668","","445451.0","sfactor you could do as you suggest and add vdlh and it could be this is correlated to weight wrho v ie rhodensity not corrcoef this makes sense in terms of dimensional analysis yes my point was that these types of allometricscaling relationships will be loglinear eg log w logrho + log d + log l + log h so rather than making v apriori you could let pca on the logtransformed features figure out what to do automatically","comment"
"405804","","759088.0","i would be interested in finding h together with g and f without resorting to finding it outside the regression as i can only do that for a very limited set of possible hs and the data wouldnt be on a regular grid itll be continuous","comment"
"29106","","124532.0","xmjx so what do you recommend to obtain reasonable results","comment"
"393718","","739371.0","meghanmartin that would be a hierarchical model and it ultimately boils down to one test and if youre using a linear link it should give the same results as the mixed model possibly more conservative since the degrees of freedom are reduced in the final comparison","comment"
"14292","13000.0","","it doesnt make sense to both include tank as a random effect and nest tank within the pop temp fixed effect you only need one of these depending on how tank is codedif tank is coded 18 you only need the tank random effect nesting it within the pop temp fixed effect results in the same 8 units so is not necessaryif tank is coded 12 that is which rep it was you only need to nest tank within the pop temp fixed effect because that gives you your 8 unique tanks including the tank random effect is only desired if the tanks were first divided into two groups and then randomized to treatment if the eight tanks were completely randomized to treatment this is not necessary you could do this with likelihood based solutions such those in nlme and lme4 but if everything is balanced it might be simpler to use the traditional anova approach using aovcreating some sample datasetseed 5 d within expandgrid popfactor c a b tempfactor c warm cold rep12 fish1100 tank factor paste pop temp rep sep tanke round rnorm nlevels tank unclass tank 1 e round rnorm length pop 1 m 10 + 2asnumeric pop asnumeric temp growth m + tanke + e using aov like thisa0 aov growth poptemp + error tank datad summary a0 or lme like thislibrary nlme m1 lme growth poptemp random1 tank datad anova m1","post"
"413651","","772351.0","thanks plus it seems to me that for predictions based on this model one should keep the constant otherwise the predicted results will be based on a model in which the intercept was forced to go through 0 rather than follow a slope based on the underlying data","comment"
"348096","","656241.0","thanks ill take a look at cvxr it seems to be what im after","comment"
"73048","73045.0","","you have almost performed what is usually called a power analysis i say almost because what you usually measure in a power calculation is not the mean pvalue but rather the probability that given the sample size and the hypothesised mean difference you would get a pvalue lower than say 005 you can make small changes to your calculations in order to get this probability however the following script is a modification of your script that calculates the power for sample sizes from 2 to 50ctrlmean 1ctrlsd 01treatedmean 11treatedsd 022n_range 250max_samples 50power nullptheshold 005rpt 1000for n in n_range pvals replicate rpt ttest rnorm n ctrlmean ctrlsd y rnorm n treatedmean treatedsd pvalue power rbind power mean pvals ptheshold plot n_range power typel ylimc 0 1 the way i would read this graph goes like given my assumptions of the two groups the probability that i would find a significant effect at n 30 is roughly 50 often an 80 chance of finding an actual effect is considered a high level of power by the way power analysis is generally considered a good thing","post"
"229576","","434431.0","neither the mean nor the median are always better than the other the median is a more robust estimator median had breakdown point of 50 while the mean has breakdown of 0 which is important if you have outliers or data thats not continuous however there are also many cases you would want to use the mean a simple example of when the mean is better is if im interested in how far i drove today and i know how many hours i drove then i would multiply my average speed by hours the median would not be as helpful not to mention if youre analyzing normally distributed data use the mean","comment"
"16234","","35715.0","could you please elaborate on the purpose of the welch part","comment"
"74532","","145383.0","oh and indeed in the r help i see that i can use a formula therefore it works i thought i could only two input in the function xand g ok thank you","comment"
"51284","","100339.0","ziggystar clarified i meant the same word to have the same input an array of length word of number between 1 and 255","comment"
"398940","","747313.0","this https stackoverflowcom questions 42620153 howdoidoubleuptheweightsofcountvectoriserfromscikitfortfidfmatrix might help you","comment"
"7707","","12468.0","what sort of outcome variable are you using in this case","comment"
"80432","80429.0","","to clarify the hypothesis you are testing is is there a difference between group 1 and 2 in the time to discharge postsurgery if so you are looking at a survival analysis","post"
"73896","","","using expectation to detect bias","title"
"52187","","","if we apply linear regression on a data which has a binary 0 1 dependant variable the very important assumption of constant variance of the dependant variable across independant variables is violated can anyone explain how","post"
"281035","","539239.0","thanks and any literature that this approach is based in and i could dig any further in","comment"
"424044","","791447.0","also if you are happy with my answer please dont forget to mark it as accepted by using the tick","comment"
"204524","","388022.0","many thanks djohnson for your answer im no longer part of the company i worked for however i still welcome your answer as it can be still very relevant but in a different manner","comment"
"36112","","132884.0","if it is a markov chain then p 1mid 1 2 p 1mid 2 so there is nothing new if it is not a markov chain keep a record of the n3 triple of three consecutive transitions simulate the process and compute the fraction of each triple","comment"
"250698","","477019.0","since this question can be viewed as one concerning the logical design of a database it should be considered on topic statisticians need to know a great deal about relational databases and how to model entities appropriately with them","comment"
"122428","","233314.0","yes they are the same eg in terms of the density just with different parameterization","comment"
"210727","","400355.0","what is your n please edit your post to include what the very large n is as some users may not want to scroll through the comments for that detail","comment"
"301954","","574130.0","yes and more generally this is true for any random variable whose density function is symmetric","comment"
"340996","","","variance of a predicted y when x_0 is given","title"
"58465","","112199.0","statistically significant doesnt imply large r2 with large n even very tiny effects are distinguishable from chance statistical significance is not practical significance","comment"
"158971","","302757.0","whuber edited again my question is in the last paragraph its about the critical region","comment"
"292533","","557873.0","the coef output makes sense because you have asked the model to determine an intercept for each person this is what the code in the random option means youll note that the coefficients for group a and group b in this output are the same as in the table for summary","comment"
"210919","","","i am looking to implement a generalised linear mixed model in sas using proc glimmix the data is from a survey and a single set of weights is provided with the product scaled to the population while proc glimmix has a weight option it doesnt state anywhere how the weights should be scaled possibly because it doesnt make a difference should i be scaling them to the sample size","post"
"109740","","210840.0","whuber youre right i wasnt being clear about the nature of the assumption i added a few more details i hope there isnt room for misunderstanding now","comment"
"280906","","538386.0","i was hoping to discern more precisely the manner of deviation from normality in order to make a more concrete suggestion about whether there was anything to be concerned about looks good tells me nothing about what deviation the shapiro wilk was likely to be picking up","comment"
"188644","","649715.0","huh so there are actually more than just those 2 cases then hmmm","comment"
"59059","58745.0","","i think that your approach is correct model m1 specifies a separate intercept for each subject model m2 adds a separate slope for each subject your slope is across days as subjects only participate in one treatment group if you write model m2 as follows its more obvious that you model a separate intercept and slope for each subjectm2 lmer obs treatment day + 1+day subject mydata this is equivalent tom2 lmer obs treatment + day + treatmentday + 1+day subject mydata ie the main effects of treatment day and the interaction between the twoi think that you dont need to worry about nesting as long as you dont repeat subject ids within treatment groups which model is correct really depends on your research question is there reason to believe that subjects slopes vary in addition to the treatment effect you could run both models and compare them with anova m1 m2 to see if the data supports either oneim not sure what you want to express with model m3 the nesting syntax uses a eg 1 group subgroup i dont think that you need to worry about autocorrelation with such a small number of time points","post"
"346325","","","say i have x that follows an ornsteinuhlenbeck process dx_t phi mu x_t d_t + sigma d w_t let y_t exp x_t is there anything that helps me compute lim_ ttoinfty e y_tgamma gamma neq 1here is my approachwe know that the stationary solution for x_t is gaussian with mean mu and variance sigma2 2phi hence the expected value of lim_ ttoinfty y_t is that of the lognormal distribution lim_ ttoinfty e y_t expleft mu + frac sigma2 2phi right now y_tgamma exp gamma x_t i would intuitively guess that the process gamma x_t follows dgamma x_t phi gamma mu x_t d_t + gamma sigma dw_tand therefore computelim_ ttoinfty e y_tgamma expleft mu + frac gamma2sigma2 2gammaphi right but clearly im out of my reach here","post"
"363","","","what is the single most influential book every statistician should read","title"
"81427","","159616.0","which textbooks are you referring to when you say i looked in textbooks and they seem strange on aic it looks like a larger difference is weak and a smaller difference in aic means one model is better and what do they actually say","comment"
"221753","","","is simpsons paradox always an example of confounding or is it possible to have a simpsons paradox effect without an extra variable lurking in the background","post"
"23126","","42321.0","conjugateprior i largely agree however when results dont look like what you expect it can be quite common to go looking for candidate predictors that will give you the picture you expected to see thus in practice this can occur in addition the question as stated suggests this might have occurred here i dont want to sound critical because this approach is quite intuitive many people believe that it is the appropriate strategy i just want to explicitly raise the issue to nip it in the bud","comment"
"119797","","228393.0","if youre going to reference fisher you need to spell his name right my suggestion if fisher gave a reference when he first used it use that reference if he didnt reference him i think most people do that","comment"
"437430","","815876.0","for 99 ci or 1 level the magic number is around n 66 in r qnorm 995 returns 2576 aprx 26 and qt 995 65 retirms 2654 aprx 26 unfortunately this gets conflated with the size of n for which clt gives bar x aprx normal another issue entirely for uniform data thats about n 12 and for exponential data n 100 is not always sufficient","comment"
"30132","","57863.0","michaelchernick thank you a further question should i obtain sas certificates to demonstrate my knowledge of sas for landing the first job in the field of statistics","comment"
"295035","","568688.0","i still think that the appropriate numerical approximation is to just run the simulation in my time studying bandits i did not come across any shorttime numerical analysis besides running simulations i am sure that you could find special cases where you can get snappy analysis but those will break down very quickly as you add in decays and arbitrary success probabilities if you want a starter implementation in julia i could throw one together for you in a few minutes","comment"
"60687","","116394.0","what exactly is it that you want to know if its 5050","comment"
"51614","","100904.0","ill add it to the answer take a look","comment"
"225818","","428092.0","putut_purwandono if ln is natural log then the effect of treatment is a semielasticity which corresponds to an astounding 100 cdot exp 7833 1 252 14855 increase in passengers","comment"
"18511","","","i am comparing three relatively simple glms having a gamma distribution with aic and bic the aim is to identify the effects of fertilizers fdung year and site on biomass of a specific grass species hence the aim is not to predict new values but merely to identify the effects of the three factorshere are the models usedres1 glm biomassgm fdung fyear fblock familygamma linkidentity res2 glm biomassgm fdung fyear + fblock familygamma linkidentity res3 glm biomassgm fdung + fyear + fblock familygamma linkidentity i expect the third model to be the most simplistic one and want to confirm this by an information criterion however when looking at aic and bic i get this outputaic res1 res2 res3 bic result1 res2 res3 df aic df bicres1 49 5271617 res1 49 5465198res2 16 5334234 res2 16 539744res3 10 5331253 res3 10 5370760for aic the most complex model is best and for bic the one with fewest df is best i am thinking that with regard to my aim identify effects on biomass i should trust bicam i wrong here with my conclusioni already tried mixed effect models with the fblock as random factor but then the model with the gamma distribution did not work any more and also i could not use fblock as fixed effect any more leading to nas for fblock but this is not part of my question","post"
"228780","","432676.0","well i mean is it possible to train a cnn using only ekg vector measurments and if so what would be a suitable architecture","comment"
"368950","","","an x trimmed mean means","title"
"133419","","253833.0","well i dont know if i would say uniformly but in general i thought it was a superior approach because there is less bias than twofold cv since you use the whole data set to train and test the model am i missing an important distinction between the two approaches im not opposed to trying twofold cv but i would suspect that estimate is more biased to get back to the main question i have is why would an interceptonly model perform almost as well as a model that contains significant predictors","comment"
"207921","","395593.0","richardhardy link to knime https wwwknimeorg i got the idea of hmm after reading many posts most of the similar posts suggested the use of hmm to achieve this task even though i am unsure how this could be used in knime thanks for the edit","comment"
"30188","","57959.0","arnoldneumaier xty are externally given dont collected","comment"
"31383","","61036.0","michaelchernick i agree its pretty rare that you wind up comparing two distributions with medians different enough to detect but means that are pretty much the same or vice versa however it is a gotcha if it gets written up","comment"
"126771","","241811.0","nick i am fully in agreement with lose the label minimizing remote legends and with the use of highlighting one curve at a time letting others dim as you did you can be almost as effective using solid black for the highlighted curve i think this is a great idea for presentations but is not so optimal for publishing a paper because of space limitations","comment"
"147245","","282172.0","the lack of i subscript on sigma2 in the notation means that the variance is assumed constant the same for every y_i various models the usual forms of anova regression additive models for example use such an assumption","comment"
"412085","","769470.0","note that the details can depend on the software package used for the survival analysis for example sas uses the last named level of a categorical variable as the reference unlike r see the end of this answer https statsstackexchangecom a 26540 28500 for example also if you use the basehaz function in r at default settings the result is provided for a situation with all predictors at their average values across the dataset this makes no sense for categorical predictors as the authors themselves note https cranrprojectorg web packages survival survivalpdf","comment"
"353298","","","how to embed in euclidean space","title"
"117434","","225229.0","user20637 thats what i have been thinking after all i am not trying to prove something that hard to prove i was simply overzealous when generating possible causes","comment"
"286505","","548166.0","i am glad you find it useful","comment"
"428444","","","i am putting together a model which involves a simple linear regression and to aid the development i have put together a process for generating synthetic observationsthe idea is that you have several variables which evolve according to an unobservable continuous time multivariate process each variable can only be sampled once a day but not all at the same time some variables are sampled at 8am and some at 8pm the key point is that the same continuous time process under the hood drives all variables my task is to try to discover the nature of that hidden processas part of this i need to fit a model of 8am early variables regressed on lagged observations of 8pm late variables because there is some overlap from when the late variables from t1 are evolving and the early variables from t are evolving remember the same underlying process drives the variables they are just sampled out of phase which introduces autocorrelation in the observations the underlying process itself does not have any autocorrelationx_ t early ax_ t1 late + epsilon_tso far this is all fairly straight forward and it is easy to fit this modeli have the following r code to generate synthetic observations and fit the modellibrary magrittr library mass library xts rm list ls cv matrix 04 nrow 6 ncol 6 t diag 1 n 50001x mvrnorm nn mu rep 0 6 sigma cv zooearlyindex ceiling 1n 2 60 60 24 asposixct origin 20200101 tz utc asdatelateindex ceiling 1n 1 2 60 60 24 asposixct origin 20200101 tz utc asdatex_early x 13 aggregate earlyindex sum xts t colnames paste0 early 13 x_late x 13 + 3 aggregate lateindex sum xts t colnames paste0 late 13 joined merge x_early lag merge x_early x_late join inner napad f join inner lmar1 lm joined 13 joined 16 1 summary lmar1 the idea is serially independent observations x are drawn from a multivariate distribution these observations represent the hidden process if the entire set can be sampled at both 8am and 8pm i then aggregate the early x_ early and late x_ late out of phase the results show a highly significant fit with all p values essentially 0but then as a sense check i wanted to check that if i include lagged early regressors in the model these regressors would not have significant explanatory power intuitively this seems to make sense because the underlying process generates serially independent observations indeed in the model x_ t early bx_ t1 early + epsilon_t the values in b are all insignificanthowever if we define x_ all x_ early x_ late to be the inner join of all the observations then we fit the following modelx_ t early cx_ t1 all + epsilon_tlmar2 lm joined 13 joined 13 1 summary lmar2 then rather confusingly for me at least we see all regressors including lagged early observations playing an extremely significant role here is the output for the first regressandresponse early11 calllm formula early11 joined 13 1 residuals early11min 49505291q 0929542median 00062483q 0919383max 5146397coefficients estimate std error t value pr t joined 13 early1 0046012 0006967 6604 407e11 joined 13 early2 0049209 0006955 7075 153e12 joined 13 early3 0040320 0006976 5780 756e09 joined 13 late1 0120096 0006931 17328 2e16 joined 13 late2 0121404 0006940 17493 2e16 joined 13 late3 0125071 0006974 17934 2e16 signif codes 0 0001 001 005 01 1this doesnt make sense to me but maybe i have forgotten something important about linear regression since i learned about it all those years ago why do the extra regressors have extremely significant coefficientsupdatefurther to baers answer the conditional expectation can be calculated assig c 2 0 04 0 2 04 04 04 2 matrix nrow 3 sig_xy sig 1 23 sig_yy sig 23 23 sig_xy solve sig_yy for a proof see theorem a3 in this pdf","post"
"251713","","","is there an online guide on how to visualise different kinds of survey question types multiple choice ranking the options etc in microsoft excel","title"
"24462","","44577.0","is there a reason why you cant have a series of dummies representing #times ate pizza in last 4 weeks ranging from 1time to 4times the omitted reference would be that they didnt eat any pizza in the last 4 weeks is there a reason not to use a series of dummy variables","comment"
"420853","","","i have a multiple linear model that works on different datasets suppose that the first dataset produces y in range of 1 100 and the second one in range of 1 1000 i cant simply compare the mae for the two datasets if mae for the first one is 2 and for the second one is 20 id say the model is consistent but i could not find a scientific way to show thisthere is no such thing as a normalised mae i can consider nrmse using rmse ymax ymin but i was wondering if there are any better ways to compare the effectiveness of the same model on different datasets i am also aware of mape and mase just wondering what is the best practice in reporting a scaleindependent forecast error metrici am interested in the theory which one of these work for my case nrmse mape or mase im also using python","post"
"46361","","","which 2tailed test is best to use to compare means medians when one variable has a normal distribution and the other does not","post"
"167252","","317255.0","youre right ive found the right tables here http wwwdegruytercom view j eqc200823issue1 eqc20085 eqc20085xml when adding onesided keyword thanks ill modify the wiki page also how to mark the question solved","comment"
"139301","","","i have percentage data for diet per area example here i have no data on the individuals contributing to this diet only for the population as a whole for each area i want to assess whether the size of the area significantly affects the percentage of grains in the diet so a glm with area as the explanatory variable and grains as the response ignoring all other diet items for now i just want to check which distribution these data would fall underi fitted the model using a poisson distribution as although all the values are fairly low 47 is the highest for the whole dataset it was overdispersed so i then tried a negative binomial which seemed to correct the problem does this sound like the correct approach","post"
"234574","","","i asked this question on stackoverflow but i think it might be more appropriate here i have a matrix of monthly returns and a vector of forecasts i want to minimize my potential risk for a given level of profit based on the forecasts the risk is based on the 5th percentile of the vector that is the result of passing a given portfolio through the monthly returnsminimize variancefn function h xbalances 1 h yt t mreturns asvector x return quantile rowsums y 05 narmtrue fn2 function h return fn h #for a given forecastforecastforecast colnames balances target0eqn function h x balances h return sum x forecast target sol auglag pars fnfn2 grnull heqeqn lowerlh upperuh nlinfo false control list xtol_rel 1e8 maxeval 20000 my problem is that i do not get a very smooth set of solutions when i run it for 100 points i would expect this line to be closer to a nice curve with most of the points near the spikes you see i would expect it to be smooth because theoretically i could just make small changes to the weights in h to get from one point to the next is there a better optimizer for this type of problemi have a matrix of monthly returns and a vector of forecasts i want to maximize my potential profit based on the forecasts for a given level of risk the risk is based on the 5th percentile of the vector that is the result of passing a given portfolio through the monthly returnsminimize variancefn function h xbalances 1 h yt t mreturns asvector x return quantile rowsums y 05 narmtrue fn2 function h return fn h for a given forecastforecastforecast colnames balances target0eqn function h x balances h return sum x forecast target sol optim pars eqn grnull lower0 upper1 controllist fnscale1 my problem is that i do not get a very smooth set of solutions when i run it for 100 points i would expect this line to be closer to a nice curve with most of the points near the spikes you see is there a better optimizer for this type of problementer image description herehere is the databalances aaa bbb ccc ddd eee fff 1 3300000 2000000 7700000 5500000 4000000 1000000forecast aaa bbb ccc ddd eee fff1 0000768006 0000635124 0001526249 0008919934 0000152549 0001271481monthly returns aaa bbb ccc ddd eee fff10 1 2006 00273758311 00173219254 00092231793 00138312574 00124329157 0012466884811 1 2006 00386007238 00195502377 00097401588 00115189105 00125419543 0006548840112 1 2006 00180668473 00380363598 00137722146 00103839765 00110926718 004546523481 1 2007 00006337939 00111542926 00301578502 00142295446 00180118359 001019719582 1 2007 00193818090 00038791343 00147327469 00171395937 00106915572 001355957883 1 2007 00136933213 00042708969 00272062488 00086841626 00051172709 001259407164 1 2007 00304574997 00047998366 00017821783 00288433537 00160311962 001274078575 1 2007 00233737324 00157028153 00170184226 00122388783 00387973703 001872098256 1 2007 00067673716 00081553009 00182637526 00680350676 00472285071 001139370777 1 2007 00195654783 00132667474 00086871710 00124935191 00042241618 000684065738 1 2007 00076524606 00118484592 00353900671 00249734320 00087112958 000930507359 1 2007 00435798575 00075768758 00275545857 00407258270 00004736651 0002707251010 1 2007 00924749572 00130880968 00002592090 00845218362 00644348088 0044093910511 1 2007 00201274740 00179104478 00093311906 00350746684 00427970555 0013632211412 1 2007 00305820454 00114417576 00308341887 00249386286 00477620911 001441941071 1 2008 00093848937 00339930944 00036721437 00069662916 00061454765 000280188612 1 2008 00321881064 00107229158 00484552051 00198235360 00027127500 001439144743 1 2008 00294150171 00122131189 00265085560 00323534622 00075926301 002546953114 1 2008 00254592330 00066361671 00185567006 00307842234 00328013313 002865801445 1 2008 00291005291 00005567083 00247055516 00492000484 00012745099 000896631236 1 2008 00239931448 00037978529 00103297983 00218807620 00267767258 000516996257 1 2008 00010460251 00063562528 00057476747 00181476849 00281745247 001536582238 1 2008 00270157068 00099754374 00146689710 00243589740 00048676019 001450009509 1 2008 00848041326 00878987342 00039763301 00543735223 00370300926 0060845541010 1 2008 00740827846 00175974242 00228906428 01397058821 00037639971 0041595402611 1 2008 01518923038 00915974459 00736264752 01118110238 01235463915 0091583981712 1 2008 00413297394 00741477980 00566584579 00762846012 00286790030 000903661791 1 2009 01018431740 00138403655 00268870859 00098509388 00231128587 011371025302 1 2009 00962574426 00094018259 00092303638 00036590625 00077260893 008771804913 1 2009 00023529412 00152682256 00784996236 00266487893 00365901429 001123858584 1 2009 00943661972 00104763235 00096417819 00496085860 00129365079 004578104035 1 2009 00447590448 00313104783 00059523944 00459061643 00631117103 000181145756 1 2009 01088146729 01020038871 00261932537 01138436318 00838591685 006675205307 1 2009 00019750648 00021285653 00007242318 00099870620 00487995829 000120064988 1 2009 00338899196 00142614395 00209125845 00361374737 00668151452 000813180609 1 2009 00118435220 00330880153 00189410175 00253958301 00240898396 0002314652510 1 2009 00529055690 00126856436 00369378363 00711407143 00187286653 0022567491611 1 2009 00344946533 00312127860 00053280313 00144219837 00008296452 0011962873812 1 2009 00281204846 00096031119 00393401412 00235369340 00368954311 002459406211 1 2010 00291891892 00276924929 00683577530 00136428779 00062689978 005019561042 1 2010 00073496659 00119497245 00268182226 00560575728 00073543270 002743647033 1 2010 00106573929 00605965660 00166050081 00279786409 00184367195 002663125404 1 2010 00224220224 00200787139 00499893623 00188144624 00325203247 000213864315 1 2010 00034741070 00011770861 00003196447 00144877542 00091364566 002170873506 1 2010 01007248729 00407882676 00318856314 00586643576 00355315523 008011132847 1 2010 00146775746 00359019862 00382420201 00316565244 00039637605 002436830488 1 2010 00720891629 00337352573 00130681681 00206279554 00290375846 004190947559 1 2010 00082946251 00149149085 00240407343 00050973650 00191465044 0018617836310 1 2010 00665789185 00238757684 00146598812 00333195239 00295184853 0076664845011 1 2010 00151172357 00133973711 00336604101 00078097478 00044326247 0007396127912 1 2010 00190456894 00257545523 00437106512 00011144347 00017699119 005427193551 1 2011 00568005783 00007040901 00378451737 00262445077 00190380763 001864677682 1 2011 00119222124 00339482449 00028272754 00018625333 00074702196 003324865513 1 2011 00022747503 00077437740 00062301897 00003004991 00162084538 000376021404 1 2011 00249654628 00095899674 00261717073 00354075923 00120431890 003338898165 1 2011 00562241263 00369933586 00353491621 00200583972 00191514125 004003652456 1 2011 00325403336 00223259711 00029648152 00117927481 00328489560 003234956447 1 2011 00146975692 00159177176 00014845420 00245501284 00195096504 001381909558 1 2011 00186629526 00138111236 00468851189 00064491408 00015673977 001900041319 1 2011 00226961991 00071796760 00036396795 00330925481 00203705594 0000631578910 1 2011 00988621526 00367760677 00016869980 01381365404 00698847952 0061154358711 1 2011 00690333264 00234214579 00167155471 00770244711 00293022346 0023604989912 1 2011 00082292574 00162392626 00086228941 00306649633 00064108885 001766036631 1 2012 00034166341 00093690249 00102717068 00357295909 00072456670 003714434292 1 2012 00485845822 00187222544 00093176468 00763997010 00227318248 001543090813 1 2012 00097150864 00077680940 00606509309 00117255850 00132927441 001139731024 1 2012 00428346748 00031960895 00211174021 00616378357 00132171821 000240402675 1 2012 00011598685 00133058471 00347109283 00427081690 00134970570 000794424046 1 2012 00612541126 00528943962 00265316552 00646046778 00534101827 006066329237 1 2012 00553551180 00223914600 00221831931 00152781926 00240015740 001873894168 1 2012 00217815980 00108868657 00172106230 00170718584 00109387431 003489381869 1 2012 00132874486 00212409887 00006377956 00066476282 00195680830 0028957055210 1 2012 00037783375 00166393546 00051289010 00019735538 00040720757 0024564750811 1 2012 00038606312 00000000000 00265851526 00022644484 00142498744 0004267535712 1 2012 00025959042 00072535648 00286130154 00489700375 00021118266 000332225911 1 2013 00031645570 00147380254 00486735937 00411386231 00010066431 001678730942 1 2013 00011544012 00342174903 00654305847 00319919529 00031108877 003302029693 1 2013 00196021908 00417383547 00087616625 00041925528 00295091542 004530791794 1 2013 00215622856 00127011571 00038614299 00203384796 00098347756 001328520965 1 2013 00138156001 00214065270 00427148166 00097436671 00084300305 002576075966 1 2013 00688782956 00229508197 00304629093 00652935412 00281445781 001373292877 1 2013 00347926027 00012501645 00079269110 00394347242 00116223686 000500038468 1 2013 00338817926 00063744496 00012055870 00326360551 00145950131 001094611159 1 2013 00026890756 00253968254 00139553083 00340809061 00181266017 0001135761310 1 2013 00558364229 00445691434 00017347621 00762463339 00208292961 0022991983111 1 2013 00042562247 00166100648 00067903218 00165498253 00094049904 0002883335812 1 2013 00349650350 00277533593 00368020343 00351883563 00182777460 000771112921 1 2014 00243741765 00120967742 00266983220 00110494902 00028184893 001265543372 1 2014 00146297547 00076663045 00314581713 00210129322 00434938896 002012642593 1 2014 00191868433 00186142709 00023576343 00292198115 00057845269 002343170704 1 2014 00361945316 00068677217 00178485935 00364295499 00036284465 000065207945 1 2014 00029198659 00158147925 00128994503 00132598662 00062066454 000558254196 1 2014 00038818201 00081690641 00055025913 00041043938 00101419878 001694304257 1 2014 00200859291 00235750522 00023639137 00179844668 00201279157 000322698948 1 2014 00194798357 00191836735 00105253101 00247154181 00260168559 001842239939 1 2014 00021477663 00126627430 00166746378 00052540178 00041394531 0022268563310 1 2014 00636519503 00254094412 00416934585 00947966609 00261578426 0038467397911 1 2014 00068665599 00118003213 00305377641 00013318247 00091425528 0007763606112 1 2014 00348942942 00165676774 00513513556 00323739586 00055609505 000439121761 1 2015 00362737016 00091544819 00111909311 00364614693 00242873130 002935044112 1 2015 00514481242 00337482356 00191506171 00094300950 00880458693 006716787843 1 2015 00059263077 00250996016 00178885567 00557158943 00173391939 000841378094 1 2015 00266393443 00398367664 00010855227 01022781142 00083987014 003867452665 1 2015 00330263158 00219253862 00032458554 00498921894 00379965466 004050915176 1 2015 00312062158 00034988117 00370281860 00485734132 00289889795 002428788287 1 2015 00051275309 00273666206 00129901792 00060967874 00054011118 001153106988 1 2015 00440810988 00003201639 00058116534 00795580758 00382705683 000624264919 1 2015 00395457033 00203559083 00378654941 00749722897 00126706388 0030134741410 1 2015 00015671748 00113042342 00046693964 00775158974 00006783238 0010605391111 1 2015 00153627312 00196285771 00057204213 00398620240 00143730891 0016882536812 1 2015 00259176240 00224267565 00183120461 00007267065 00208114991 003389060511 1 2016 00027311211 00222782124 00192451285 00272167213 00358715265 002097244432 1 2016 00258797754 00212260952 00036366319 00007064285 00066676232 000294767873 1 2016 00085746416 00333264048 00612226330 00074985366 00401193145 000183688464 1 2016 00699651568 00197104358 00207718249 01070122134 00306663595 004812292975 1 2016 00096391820 00270612216 00487323576 00343442572 00362376556 000526731636 1 2016 00455083520 00134136326 00277524101 00460086088 00399143600 002296742647 1 2016 00332093151 00797031077 00684744419 01128105301 00128562573 000464783708 1 2016 00050680181 00066329992 00012697102 00089113465 00161536118 000242456909 1 2016 00019904459 00067531679 00081372403 00025174229 00016026871 0003045776","post"
"4384","","6607.0","do you mean that excel doesnt recognize that your data are numerical if you look at the cell format is it defined as numbers just to make sure i understand your comment","comment"
"74479","","145272.0","i dont think youve clearly defined the weight but im guessing its the number of times j followed i and perhaps you meant w_ ij if im right then maybe youre just looking for a first order markov model more useful terms might be stochastic matrix and transition matrix","comment"
"305256","","594083.0","http citeseerxistpsuedu viewdoc downloaddoi10111506616reprep1typepdf i hope this paper helps","comment"
"116174","","222814.0","is this to be used to set alert and alarm warnings for future predictions eg for new patients","comment"
"183990","","420700.0","updated link cant link to the original slides anymore so opted for well compiled notes by alex holehouse http wwwholehouseorg mlclass","comment"
"362344","","","i have 5 samplessample a size 10 000 obesity rate 5sample b size 15 000 obesity rate 10sample c size 12 000 obesity rate 5sample d size 9 000 obesity rate 6sample e size 8 000 obesity rate 7what statistical test should be conducted to assert that there is a statistically significant difference amongst the 5 groups i was thinking chisquarewould appreciate any inputs","post"
"252174","","480254.0","a book url is helpful but what would really be nice is if someone could give an explanation of how to interpret the confounding matrix and diagonality that the algdesign package provides to help understand how good a particular design is","comment"
"60660","","116326.0","to start with from the look of that data take logs of both y and t so you can perhaps begin to make out whats going on","comment"
"315786","","599451.0","can you please provide a reference to paper you mention","comment"
"83199","83186.0","","i have a friend who used to work for the us defense department long time ago cold war era and was once asked to answer a question using a single data point when he insisted that he needed more data he was told that the person who had provided the single data point had been caught and executed for espionage shortly after providing the single data point so there would be no more data coming that is when my friend started to learn about bayesian statisticsi also remember seeing an article several years ago possibly in the american statistician possibly in chance that derived a way to compute a confidence interval for a mean based on a single data point the 95 interval from a value of x was something like x to 3x if you were willing to make certain assumptions and the usual diagnostics were not of any help with only 1 point so yes you can do valid statistics with very small sample sizes but you will tend to have low power precision and large sample properties will not help you so violations of any assumptions will have a potentially much larger impact","post"
"372898","","700788.0","i just corrected alpha_1 to alpha in 2nd equation still the reason is not convincing that it simplified the computation can you kindly elaborate further how could overline x suddenly enter the equation without an associated mathematical logic","comment"
"308761","","586570.0","ignore the joint distribution imagine a single rv now imagine that you have a circle that goes from 0 to 1 with an arrow you can spin in the middle sort of like a roulette wheel the probability distribution would in that case be a uniform distribution when you spin the arrow it lands anywhere with equal chance now and this is a bit hard mathematically the probability of any single number coming up on a spin like sqrt pi 10 is zero the probability it lies between say 0 and 05 is 50 it will land on a number but the probability of that number is zero follow","comment"
"314410","","596881.0","did you just make a try to search an answer on the site","comment"
"63088","","121468.0","yes for any continuous random variable x f x is uniformly distributed on the unit interval","comment"
"14806","","26541.0","yang it sounds like your datawhich you havent describeddo not conform to the model underlying the use of the chisquared statistic the standard model is one of multinomial sampling http enwikipediaorg wiki multinomial_distribution strictly speaking not even unconditional poisson sampling is covered which is what gregs answer supposes i make a perhaps obtuse reference to this in my previous comment","comment"
"129806","","246974.0","ok i added all the context im able to provide","comment"
"419277","","","how to design a neural network model that combines components of feedforward and recurrent features","title"
"362878","","681853.0","amoeba hmmyou may be right but im not sure yes thats the test i was thinking i think the flaw in henriks proposal stems from the fact its a 2 degreeoffreedom test where the larger model not only allows unequal variances but also variable treatment effects so its really testing 2 things at once my footnote refers to a 1 df test which i was thinking avoided this problem but im not positive since my large model just models the random means in each condition and their variances but in your scenario the 2 variances are equal it seems that it shouldnt fit any better no","comment"
"17816","","","how to interpret regression coefficients in a loglog model","title"
"197176","","373850.0","hastie t j 1992 generalized additive models chapter 7 of statistical models in s eds j m chambers and t j hastie wadsworth brooks cole","comment"
"323834","","","im struggling because while i want to show the interrelationship of correlation between my fields i realize that trying to plot nodes in terms of distance away from each other based on correlation will lead to impossibilities such as a case where a and b are 1 unit apart b and c are 1 unit apart but c and a are say 5 units apart there is no way to represent this on a 2 dimensional planei simply want to create a visualization that generally clusters things with high correlation together and moves things that are anticorrelated apart so far the closest ive gotten is using python networkximport pandas as pdimport numpy as npimport networkx as nximport matplotlibpyplot as pltg nxgraph for ii in range len links_filtered a data var1 ii b data var2 ii c data value ii gadd_edge a b lengthc weightc elarge u v for u v d in gedges datatrue if d weight 0 esmall u v for u v d in gedges datatrue if d weight 0 pos nxspring_layout g k2 iterations10000 nxdraw_networkx_nodes g pos node_colororange node_size400 nxdraw_networkx_edges g pos edgelistelarge edge_colorblue nxdraw_networkx_edges g pos edgelistesmall edge_colorred alpha05 styledashed # nxdraw_networkx_labels g pos font_size8 for k v in positeritems x y pos k plttext x y k bboxdict facecolorwhite alpha08 horizontalalignmentcenter verticalalignmentbaseline fontsize8 colorblack however the result is generally ugly and doesnt actually ensure that clusters that are not correlated are not in close proximity since it is only drawing edges not actually calcing a distance between nodes","post"
"236346","","449050.0","you may want to clarify whether you mean segmenting according to some variable s eg splitting the data according to high or low values of x or segmenting at random i interpreted it as the former but evidently its possible to interpret it as the latter","comment"
"190879","","362658.0","i was also thinking about the same answerhowever thanks","comment"
"3831","3788.0","","whether a set of observations are iid or not is a decision that is typically taken after a consideration of the underlying data generating process in your case the underlying data generating process seems to be the measurements of the speed of a river i would not consider these observations to be independent if a particular measurement is on the high end of the scale the next measurement is also likely to be on the high end of the scale in other words if i know one measurement i can infer something about the likely values for my next measurement however the values are likely to be identically distributed as the errors in your measurement probably come from the methodology used to measure velocity i would imagine that you would use the same methodology to collect multiple measurements you could of course use formal statistical tests to see if your observations are iid but for that you have to specify a data generating process estimate the model under iid assumptions and examine for residuals for deviation from iidhowever do note that i know nothing about engineering wavelets wavelet space so i may be wayoff in my above assumptions answer","post"
"154868","","","estimate ucm equation from ucm model","title"
"423569","","790406.0","for large n i think it should be frac p x p x +p y","comment"
"32434","","63564.0","questions to consider 1 why is accuracy measured by variance just because two estimators are unbiased why not any other convex symmetric loss function for example yes its convenient 2 why are you only considering a in 0 1 in particular what happens if theyre negatively correlated and l_1 has much higher variance than l_2 hint you can do better than a in 0 1 3 best linear combinations of unbiased estimators are nice but we should also remain aware that in some circumstances there may be better nonlinear unbiased estimators","comment"
"411567","","768692.0","it does make sense thank you if im reading it correctly it asserts the null distribution of x is uniform on 0 1 however that captures only part of the properties of pvalues it does not characterize pvalues and it says nothing about what they mean or how to interpret them consider studying some of the other answers in this thread for information on what is missing","comment"
"323340","","613443.0","nikolasrieble thanks ive considered about that but standerd linear svm may not satisfy misseparate the samples as little as possible since it tries to maximize the gap while we dont really care about the gap","comment"
"96144","","187473.0","are you asking about the applicability of these tests when residuals have strong correlations or are you just concerned about the very slight and inconsequential negative correlation arising from the least squares estimation procedure","comment"
"265198","","507396.0","for example x emails opened and n emails sent","comment"
"316826","","601657.0","thank you very much for your answer i have got your idea it is sufficient that the joint density be symmetric with respect to xaxis and to yaxis","comment"
"347178","","654807.0","thank you for glen_b when i ran the anova test on the unweighted data the results were very different and were statistically significant as my sample was not huge a couple of the gender and age groups had very high weightings one was over 16 which has meant that my values are much bigger than in the data that was collected do you think in this instance it would have been better to not weight the data","comment"
"415444","","775538.0","coolserdash youre right i completely overlooked that thank you i will try nlme","comment"
"174476","","330038.0","roland i tried the segmented rpackage i usenormallm lm testdataamounttestdatadate normalseg segmented normallm segz date psi list date na and get invalid type closure for variable date","comment"
"389814","","","understanding feed forward neural network output","title"
"81874","","160546.0","lets say that i am willing to make the strict assumptions necessary for the wilcox test that would also make it a test of means would that require changing the r code i have written above could this also be done for the kruskalwallis test","comment"
"28756","","54209.0","i dont understand the second comment it seems to suggest that the values 0 and 1 can occur i think what macro suggested transformations and i concurred we were both thinking of a continuous random variable on the interval 0 1 and not a random variable with discrete components at 0 and 1 the q q plots definitely show that the transformations did not make the residuals normal i dont understand the other curves at all","comment"
"419563","","787730.0","dave unless the sampling is totally biased ask white people in the rich neighborhoods how much money they have and ask black people in poor neighborhoods how much money they have well they their sampling was targeted to the ethnic groups and that they targeted census tracts where specific ethnic groups were known to reside and targeted ethnic group zip codes","comment"
"186957","","357154.0","this is computationally intensive but what if you took the empirical cdf started randomly generating brownian bridges each brownian bridge represents the delta between the ecdf and some hypothetical cdf compute the mean using the hypothetical cdf and weight it by the factor prescribed by ks test repeating this for a while youll have a weighted data set of means and can compute the confidence interval","comment"
"18907","18905.0","","do not the wikileaks texts suit you","post"
"376306","","707194.0","what you are doing is not a neural network it is called logistic regression i assume you are using cross entropy as your loss either way the biases are appropriate again assuming you have weight decay and your weights are not too big the large different in bias just means that your variables are well separated if this is indeed the case you should expect that your first set of weights are just the negative of the other and thus the bias is the negative of the other up to noise","comment"
"47379","","726249.0","confounded see the discussion in the comments here https statsstackexchangecom a 61298 25538 basically it works with reliability weights if they are not standardized normalized but if your reliability weights are since thats kind of the purpose usually then you lose the base frequency the total count and thus the bessel correction cannot be applied because the necessary total count information is lost","comment"
"372916","372815.0","","the question puts two functions into evidence which i will call theta and t they are maps from a space mathcal f of distributions defined on mathbb r 2 or more generally any set on which distributions may be defined into a parameter space thetasubsetmathbb r p or the real numbers mathbb r the parameterization associates the parameter values with the distributions and so may be considered an invertible mapthetamathcal f to thetasubset mathbb r pthe regression parameter b of the question is an example of a property of a distribution that there is exactly one b for each finmathcal f means b can be considered the value t f of some functiontmathcal f to mathbb r note that the components of theta are automatically properties according to this definition because the itext th component is the composition of theta and the projection pi_imathbb r pto mathbb r theta_imathcal f to mathbb r p to mathbb rtheta_i f pi_i theta f and is therefore a realvalued function defined on mathcal fthe question asks whether for any possible such theta and t it is necessarily the case that there exists a component i 1le i le p such thatt f theta_i f for all fin mathcal f that is clearly not true because distributions have infinitely many properties but there are only a finite number p of components of theta for instance for any number x the map t+xmathcal f tomathbb r given by t+x f t f + xis not the same property as twe could generalize the question to ask whether t must depend somehow on the parameter this is readily shown to be the case because the invertibility of theta implies that for any parameter pintheta there is a unique f theta 1 p inmathcal f associated with p andt f t theta 1 p tcirc theta 1 p defines a functiontcirctheta 1 mathbb r p to mathbb r to sum up these two conclusions in words we may saythe parameters are properties of a finitely parameterized family of distributions but they are not the only properties all properties are functions of the parameters howeveralthough i have been silent about technical issues of continuity or measurability or differentiability depending on the application the same analysis holds quite generally assuming we apply the same criteria to properties as we do to parameters","post"
"274915","","543670.0","distance covariance is a scalar it does not yield a matrix","comment"
"99718","","193153.0","i believe most readers would prefer an objectively correct answer even if it requires further explanation or is complex to set out over one that is basic but unnecessarily limited wrong or misleading","comment"
"359058","","675352.0","your answer makes this so clear many thanks lizzie","comment"
"183589","","","parametric competing risk survival analysis exponential distribution","title"
"300065","300048.0","","as youve described the study trial is nested within block but block isnt nested within subject that is trial 3 is a different question in blocks 1 and 2 but block 3 is the same set of 8 questions for each subject hence a natural way to structure the random effects would be to have one random intercept effect per subject plus 8n random intercepts nested into n batches of 8 where n is the number of blocks or if n is small you could treat block as a fixed effect and have a single batch of 8n pertrial random intercepts plus the aforementioned persubject intercepts you asked what the difference is between fancy randomeffects structures like these and cartesianproducting all the dummy variables in a study together to get one big batch of random effects new variable the difference is that each batch of random effects has its variance estimated separately and that orthogonal effects are obliged to behave consistently and of course the more random effects you have the harder it is to estimate each to use a simpler example imagine you have a model where each subject is a child and you have dummy variables for the childs father and mother assume the dataset has a lot of halfsiblings in it so that mother and father effects are distinguishable if you saylmer outcome 1 + fixed effects + 1 mother + 1 father then the model is allowed to believe eg that the effects of father vary more than the effects of mothers on the other hand if you make each motherfather pair its own value of a single dummy variable and saylmer outcome 1 + fixed effects + 1 new variable then new variable gets only one variance also whereas this model allows for arbitrarily complicated interactions between mother and father the first model postulates that the effects are purely additive and if m is the number of mothers and f the number of fathers the first model has m + f different random effects and the second has mffinally i dont think its wise to consider rt and correct in completely separate models shouldnt whether people answer a question correctly be related to how quickly they answer it","post"
"318376","","606051.0","the variables of interest are cm volume and time timestamp you can regress predicted_vol f cm time hm and the others are not of interest for now","comment"
"40873","40870.0","","the same situation happens in the simplest normal means model ymu + epsilon with epsilon n 0 sigma2 the mle of sigma2 is the sum of squares about the mean divided by n s2sum frac y_ibar y 2 n however this quantity is a biased estimator e s2 neq sigma2 dividing the sum of squares by n1 instead of n gives an unbiased estimator of the variancefurthermore sum frac y_ibar y 2 sigma2 n1 has a chi2 distribution with n1 degrees of freedom and ratios of independent mean squares will have the f distributionstatistical inference offers several criteria for bestness in estimators these include unbiasedness minimum variance minimizer of a loss function and increasingly predictive accuracy being the maximum likelihood estimator is also considered desirable because the likelihood supposedly contains all relevant information about the model this is debatable and has been debated the mle usually manages to be asymptotically unbiased and efficient what this means for a finite sample depends on the modellinear regression with normal errors works much like the simple normal means model that i gave here the unbiased estimator for the variance is not the mle the unbiased estimator is preferred because of its nice distributional properties geometrically you are partitioning rn into two linear subspaces one to contain the model and one to contain the residuals the dimension of the residual space is the degrees of freedom","post"
"276221","","530024.0","hi rsiddiqui 1 if you have no theory as to why the pdq should be the same and you are somehow trying to force your algorithm on the data then i think you have things backwards the data is not a slave to you 2 why interpolate it adds nothing to the process analyze the data at a quarterly level you could interpolate to a weekly level but why it makes no sense","comment"
"406710","","760113.0","i updated the results with the revised data the model behaves a little better for whatever reason within strain 1 the a enzymes are statistically similar you dont know if the different levels of a have an effect in strain 2 i would resist the urge to combine the treatments perhaps report the result that the different levels of a were similar in strain 1 and then refit the model without ah and al i think you will find some difference in opinions in how to proceed in cases like this but this gives you a fully crossed model","comment"
"164399","","","how is it possible to have a significant correlation between two variables but a low covariance what does this mean","post"
"176710","","334463.0","thanks very much righskewed this is exactly what i wanted have a great day","comment"
"143178","143121.0","","here is a hint let me know if you need more and ill expandshow that in particular make sure to check the assumptions i didnt sqrt n left bar x _n s_n2 mu nu right overset d to n 0 sigma for some constants mu nu and covariance matrix sigma consider then the function g x y x sqrt y what can you say about g bar x _n s_n2 and why","post"
"83679","","164222.0","ger the line in the first plot is the logistic curve that best fits the data the randomization is supposed to give you a sense of having equal density of data in each region if the fit is good after posting i was thinking it might be good to overlay the composition of proportions with the logistic curve as a better indication of how the densities compare of course the smooth densities are subject to a tuning parameter like the binning in your originals the examples are from jmp a commercial product that i work on","comment"
"17107","","149441.0","ctd indeed if id seen it before the bounty was put on it id have already voted to close with the bounty im inclined to see if we can avoid closing it but unless ive missed something its not suitable as is","comment"
"232223","","","ive been looking into statistical distributions lately having little background in statistics myself and the way it allows you to modelize complex experiments is absolutely fascinating to me lately though ive been getting into tabletop rpgs again and have found myself unable to put the world of darkness successbased roll mechanic into something that could make it work as a distributionthe mechanic is simple on paper for any given roll you define two constants based on the involved characters abilities and the situation at hand a difficulty between 2 and 10 more recent books have it always set to 8 and how many dices youre allowed to roll the game only uses 10sided dices and any dice with a value equal or greater to the difficulty value counts as a success the number of successes you end up with then determines the outcome of the event based on success thresholds and such but thats beyond the scope of my questionso far this sounds like a simple binomial distribution right n is however many dices you can roll and p1frac difficulty1 10 but the problem arises from 2 optional mechanics which are the tenagain rule and the botched roll ruletenagain is a subcase of the usual success where any 10s can also be rolled again for additionnal successes for as long as the player keep rolling 10s which sound somewhat like a geometric distribution but i dont know how to integrate it with the initial binomial experimentthe other one the botched roll is another optional mechanic whereby any 1s are substracted from the success tally unless no successes were rolled in which case this counts as some sort of critical failureagain since these two mechanics make this falls outside of the conventional definition of a bernouilli experiment i dont know how to put it together with the rest ive been wondering if im maybe looking in the wrong direction and if theres actually a better suited distribution for this sort of case or if i should use binomial coefficients instead or polynomial fractions this question comes extremely close to what im looking for but doesnt cover the possibility of a reroll what am i missing can this be adapted into a distribution at all","post"
"374153","","","i developed a procedure to measure the geometric accuracy of 3d building models based on the similarity to a 3d point cloud therefore i created mainly two quality criteria the result of my automatic method is an index for each building how well the model fits the point cloud i am interested in how to validate this accuracy result and if this is necessaryis it possible to alter a test dataset that fits the point cloud according to the quality criteria so that it contains errors i then would want to use this faulty test dataset to see whether the errors are correctly detected i am afraid that this method could be argued as too biased because i am introducing the errors error creation would be done either manually by hand or automatically eg by a shifting algorithm that changes the models positionedit for clarification3d point cloud means a set of points each with x y zcoordinate3dmodel is a mesh model represented by vertices which are also points with x y zcoordinatei am looking on the building from the top view and arrange the outline inside a regular grid for each grid cell i am calculating the mean median of zcoordinates inside for the points of the building model as well as for the points of the point cloud after i have this 2d representations in the xyplane i am calculating the differences between each cell these are my distance measures i now apply a threshold to the calculated distances to decide whether they are significant in terms of too high without going more into details each building gets assigned a percentage value how many cells contain differences considered as erroneous in fact i am using some more criteria but those are not relevant for the questionwhat i want to know if i have a set of 3d building models that are all correct in terms of my accuracy check all differences below threshold is it valid to change the correct building models so they contain errors in terms of my threshold and use them to test my accuracy check i want to validate my developed procedure if it is able to detect the erroneous buildings otherwise my percentage values are being calculated but it is not clear if the procedure is the right way to do it","post"
"160391","","305513.0","can you provide a definition for global outliers vs local outliers","comment"
"13655","","23853.0","while perhaps dispersed among several questions i think your post is already addressed on this site","comment"
"147254","","766320.0","dimitriy i believe your second eqn should have a carrot hat over the beta","comment"
"94182","","183991.0","wolfies yes i obtained that expression too it integrates the tail of k_0 because the exact distribution does depart from it in the extreme tails it did not seem worthwhile carrying the analysis of that integral any further the logical next step is a more discerning analysis of the tails which means going beyond the normal approximation","comment"
"350410","348952.0","","first in my understanding and in principle testing a set of different predefined hypothesis on a given data set is a valid procedure however it seems that your problematic is related to a set of nonepredefined hypothesis and in my understanding the very nature of your question is about what do you mean by draw conclusions as you mentioned in the comment your hypothesis were not planned or at least a part of them consequently your analysis will be at best purely explanatory and drawing definitive conclusions is out of your scope i suggest you this question and associated answers discussing about why this is the case a brief summary could be there is too much degree of freedom in a data set to draw conclusion from hypothesis generated after having see the datanevertheless documenting and discussing the effect sizes of sideobservations is relevant and useful just be aware and make your readers aware that these are observations needing to be tested properly but that still may served a reasoned discussion","post"
"221052","","418231.0","as you suggested upon incorporating the effect of known features the remaining series of tentative errors adjusted ys can be examined for whuber autoregressive structure if the data is longitudinal upon adjusting for any time space significant dependency the resultant can be examined for anomalies pulses be they either onetime or seasonal or reflective of a set of contiguous pulses suggestive of a group shift or trend","comment"
"160312","","305377.0","like i said the confidence limits for an arima model are based solely on the psi weights derivable from the model and the presumption that the estimated parameters of the arima model are identical to the population values this unwarranted assumption makes the confidence limits too narrow causing lase positives about outliers the fix is to decelop a probability distribution of possible forecasts which can then be used by you in the manner you want","comment"
"215944","","409618.0","fraction also works","comment"
"105807","","204888.0","angelorf have you good results for instance on mnist reconstruction","comment"
"209166","","","interpretation of main effect and interaction","title"
"53201","","104112.0","thanks for your answer how can i solve my problem through the use of regression analysis","comment"
"387905","387814.0","","if your overall residual plots look good there is no need to delve deeper into the individual components also keep in mind that for smaller sample sizes lowess is frequently going to be quite wavy its not necessarily a bad thing usually a result of having too few data points a visual inspection of the individual residuals reveals no real problems even with the wavy lowess curves","post"
"38506","38117.0","","youre not doing anything wrong the two functions are making different underlying assumptions about the distribution of the data your first implementation is assuming multivariate normal and the 2nd a multivariate tdistribution see covtrob in package mass the effect is easier to see if you pull out one group#pull out group 1pick group 1p3 qplot datadf pick xx yy tl with df pick ellipse cor x y scalec sd x sd y centrec mean x mean y p3 p3 + geom_path dataasdataframe tl aes xx yy p3 p3 + stat_ellipse level095 p3 # looks off centerp3 p3 + geom_point aes xmean x ymean y size2 colorred p3so although it is close to the same center and orientation they are not the same you can come close to the same size ellipse by using covtrob to get the correlation and scale for passing to ellipse and using the t argument to set the scaling equal to an fdistribution as stat_ellipse doestcv covtrob data pick 23 cortrue tl with df pick ellipse tcvcor 2 1 scalesqrt diag tcvcov tqf 095 2 length x 1 centretcvcenter p3 p3 + geom_path dataasdataframe tl aes xx yy colorred p3but the correspondence still isnt exact the difference must be arising between using the cholesky decomposition of the covariance matrix and creating the scaling from the correlation and the standard deviations im not enough of a mathematician to see exactly where the difference is which one is correct thats up to you to decide the stat_ellipse implementation will be less sensitive to outlying points while the first will be more conservative","post"
"418794","","781286.0","is this a question from a course or textbook if so please add the selfstudy tag read its wiki https statsstackexchangecom tags selfstudy info","comment"
"12860","","22352.0","cardinal would you care to explain it sounds very interesting","comment"
"32848","","69889.0","if it has to do with combining groups and taking averages than a t test test on the average over the combined samples should be okay","comment"
"155317","","296141.0","how can i calculate price elasticity of demand form loglog model on test period price index and demand data how should i use base period data on price and demand in this case","comment"
"134237","","255421.0","thats a good decision in the meantime i have upvoted this post becausedespite my misgivings about how might have hurt the feelings of some logarithmsit goes after the more fundamental issue of whether a transformation is the appropriate response to larger values of variables","comment"
"297489","","","how would you see if a difference is significant in a summary table with two independent categorical variables say i have green dogs red dogs green cats and red cats and i have data about how many in each group ate within the last 4 hours how do i know if being green or red is statistically significant being a cat vs being a dogfor examplegreen dogs 77 93red dogs 66 82green cats 64 84red cats 31 34","post"
"233216","","442546.0","gracias tengo ms variables pero todava no he comprobado si son o no estacionarias tengo que tener todas las variables con el mismo orden de diferencias si son ms de dos","comment"
"109256","108666.0","","not attempting to give a complete answer but i note that you dont mention the nlme package which provides for both nonlinear models and timeseries structures on the errors it is associated with a book by pinheiro and bates 2000 mixedeffects models in s and splus springer which includes a lot of examples","post"
"88047","","171947.0","thanks for your reply so would i be correct in interpreting this result as x4 being a better predictor of the dependent variable than x1 again i dont know if this information is relevant but the relationship between the dummy variables and the dependent variable is negative i have just noticed that the part correlation of each of these dummy variables is larger than their zeroorder correlation is this normal what does that mean","comment"
"71707","","","can i use bootstrapping to evaluate the effectiveness of a clinical intervention on a small sample","title"
"24402","","44413.0","pan that link is correct but hardly necessary see tagr","comment"
"106090","","205145.0","thank you for your comment my hypothesis is that the relationship between the two scales is unidirectional in that the higher the score on the 025 interval scale the higher the score on the 17 ordinal likert scale would that be a onetailed spearmans correlation test","comment"
"333139","","634130.0","one minor point on your model description do you have a good justification for the priors you set note that the estimate for betweenperson variability sd intercept of id is quite close to the boundary of your prior n 0 400 means 95 of mass is below 800 this indicates that the estimate might change noticeably if you put a different prior so unless you have good reasons to expect betweenperson variation below 800 you should try a fit with wider prior to check the robustness of your results","comment"
"192649","","381479.0","guillaumedehaene this was my first reaction as well when i read the question it is not possible to approximate a cauchy with a mixture of normals when the distance induces a strong topology like kullbackleibler","comment"
"286014","","","how to find accuracy of kmeans clustering","title"
"283337","","542378.0","richardhardy i meant palatable but reworded","comment"
"332447","","629518.0","sts theres a lot of noise in the data","comment"
"104264","","201817.0","the large sample size 67 000+ can contribute to large chisq values but even so your values are very high are the rates very different between groups how many degrees of freedom do you have for your chisq tests also why are you doing multiple chisq tests are they testing similar effects","comment"
"190392","","361995.0","i would chose none of them and use the raw data","comment"
"111690","","214212.0","you have just come across expected values by what path is it by mathematical statistics where they are defined as integrals sums that bring together the probability density mass function of a random variable and the support of the random variable it appears not since you look at the expected value symbol and yet you do not see an integral or a sum so how did you come across them","comment"
"52104","","102001.0","gmacfarlane ive tried to simulate data where mnl would be better than series of binary logistic regressions but every time on average the quality was the same i was comparing lift charts and after averaging results from few simulations they looke almost the same maybe you have an idea how to generate data so mnl beats binary logistic regressions although mnl had a great advantage its scores could be interpreted as probability","comment"
"235214","","446592.0","i understand that if the variance of a_i and b_i is correlated with c_j they covariance is nonzero but should the mean value of a_i and b_i is also correlated with the variance of c_j given the exchange symmetry i use delta x xlangle xrangle as a general statement to say that the observable is always centered around langle xrangle independent of labeling","comment"
"96098","","187401.0","the point is that if you have enough observations say 100 normality gaussianity just isnt an issue a second point is that there are many normality tests and the jb test is about the worst even worse then the kolmogorovsmirnov test with bad i mean that the pvalues dont mean what they should mean in a statistical test for a simulation showing this see here http wwwmaartenbuisnl software asl_normhtml one normality test that performs fairly well is the doornikhansen but there are others","comment"
"141675","","270584.0","is this for a homework assignment","comment"
"372421","","700212.0","ive made some substantial edits as i figured out how to do the first integral but not the second let me know if you see any obvious flaws","comment"
"46323","","90039.0","interactions are certainly tricky you are talking about an interaction between a continuous and a categorical variable the essence is the same but the interpretation is now slightly simpler the main effect of ability is now the effect of ability in whichever group was coded with a 0 to get the effect of ability in the group coded with a 1 you have to add the interaction parameter this assumes the groups are coded 01 which is usually a good thing","comment"
"287011","","","is there a nonparametric equivalent of the ttest for a small sample size","title"
"55171","","106621.0","i dont understand this question a hierarchical model often implies that a posterior predictive distribution on a parameter is a mixture distribution so maybe theres some confusion herefor a particular parameter the prior within the hierarchy can be any kind of distribution however its more common to use something simple like a conjugate prior and allow the complexity mixture modeling to come from the hierarchical structure itself","comment"
"233429","","442946.0","also see here http statsstackexchangecom questions 67422 volumeofthe95confidenceellipsoid 67429#67429","comment"
"68090","","259283.0","imjohns3 one thing i did just remember is reading about mixtures of pca models think there is a short discussion in bishops _prml_ might be of interest","comment"
"240311","","457182.0","unless you assume x and y are independent there is no unique answer assuming independence what do you know about how expectations of sums are related to the expectations of their component variables","comment"
"303617","","577431.0","whuber thank you for the comment x_i and y_i are indeed nonnegative the distribution of x_i can be well fitted by a gamma distribution the distribution of y_i is a bit trickier though it has too prominent peak value which is not really typical of a gamma distribution unfortunately i can not attach figures here","comment"
"358555","358554.0","","i would call it the level that is used to calculate the quantiles and percentiles i have seen this referred to as a confidence level but i dont like the insinuation that it would mostly be used for confidenceintervals and yes since you have 100 observations the rank of the 90 percentile observation is 90 if you had 200 observations then the 90 percentile observation would have rank 180","post"
"16480","","","how to quickly select important variables from a very large dataset","title"
"438507","","817124.0","thank you for that reference i see that this works for survival data but its unclear to me how i could use this for the present case","comment"
"263318","","","how did researchers calculate the hazard ratio","title"
"58777","","","which anova is most appropriate","title"
"62473","","119823.0","agree with user603 please check page 21 of http cranrprojectorg doc contrib farawayprapdf this is based on r but includes a good discussion of the theory behind linear regression","comment"
"138020","","264653.0","yes it is retail industry i understand but how does it affect the computation of the forecast should a use multiplier base on the data analogue","comment"
"232172","","440155.0","it depends on your knowledge i would suggest you start with the appendix a in mardia for the linear algebra background keep the meyer next to you and in case you have any doubts you can check in the book by meyer then go for chapter 8 and 13 in mardia","comment"
"115467","","225721.0","thanks emilie i did check with some colleagues indeed with some before this post and they all say it is an interesting idea and could be a good way how to gather new data but they query as to how this could be worked on statistically and i am just learning data analysis with gusto but still short of a good answer","comment"
"94280","","184130.0","if you dont know how to describe it in detail then its not about coding but about theory you are referencing a coding page but providing none of your own efforts at the moment it looks like a do my project for me question with a bit of make a good example too","comment"
"201296","","381813.0","+1 with the clarification that you never throw away any variables that are not significant refers to variables that have theoretical principled reasons for being in there in the first place hence my original comment to the op if it makes no sense for the variable to have been included in the first place hopefully were lucky enough that it ends up with a low pvalue","comment"
"8391","8389.0","","the distribution of the difference between two single independent samples from normal distributions has a mean which is the difference between the means of the original distributions and a variance which is the sum of the variances of the original distributions so in your case if the normal distribution of x has mean mu and variance sigma2 then x_1x_2 has a normal distribution with mean 0 and variance 2sigma2 if you prefer frac x_1x_2 sqrt 2 sigma has a standard normal distribution so long as you do a twotailed calculation that should be enough for you to find your answer to spell it out your answer should be 2 phi left frac t sqrt 2 sigma right 1where phi is the cumulative distribution function of a standard normal distribution","post"
"438678","438643.0","","a statistic is a function defined over one or more random variablesso yes a statistic is a random variable and follows a distributionanother answer gave the example of the mean of a bunch of iid normal random variablesx_1 x_nsim n mu sigma2 the mean is a statistic because it is a function defined over random variablesbar x g x_1 x_2 x_n frac 1 n sum_ i1 n x_i there is one condition however which is that a statistic cannot explicitly depend on unknown parameters take the following definition of g g x_1 frac x_1 mu sigma while g here is a function of a random variable and it follows a standard normal distribution its not a statistic unless mu and sigma are known for a more detailed explanation see pg 122 of this","post"
"28515","","53403.0","it happens when i omit any predictor so i think the first model was overfitted overparameterised","comment"
"17368","","31155.0","i agress with gjay that tricky question is who the magician actually is i think this is less a statistical than a philosophical question","comment"
"271736","","519733.0","let us continue this discussion in chat http chatstackexchangecom rooms 56527 discussionbetweenprateekbhatnagarandarunjose","comment"
"90628","","177412.0","as nickcox notes w durations i would be surprised if the sd wasnt larger than the mean if there was no censoring you might also consider the weibull distribution power analysis will probably have to be simulationbased on a different note i would guess an anova was invalid w data like that","comment"
"202567","","","why is the bootstrapped median have poor kurtosis but the bootstrapped mean have good kurtosis","title"
"257517","","","i did linear regression with and without regularization parameter ridge and found that regularization improves the regression accuracy for just some of my test data and error goes up for the rest so it seems that regularization is destructive in some cases so i am thinking to make ridge parameter as a function of something and this gets checked in the code if its found that regularization is not needed the parameter lambda is set to zero otherwise to another predefined value so i am wondering if there is any way that i can detect in a code if regularization needed or not how can i check this","post"
"394372","","740153.0","in your examples the probability decreases with the number of dice but your title says increase what is the question exactly","comment"
"118302","","229851.0","even for a uniform distribution about a ninth of the values will lie at least 154 sds from the mean if its continuous unimodal and symmetric there will be between 1109 and 1874 outside 154 sds from the mean","comment"
"329903","","625269.0","frankharrell this is a good point in simulations it makes more sense to use ground truth than crossvalidation i will think how to change my answer accordingly","comment"
"43","","11.0","there are many r guis available for windows so im not following your point","comment"
"29592","","56379.0","according to the authors it is also an improvement over elastic net","comment"
"213050","","405471.0","no i have one model that allows to calculate cdf p ymid x for given values of y and x i use this model to calculate quantiles numerically as q_tau y mid x inf left y p ymid x geq tau right","comment"
"176083","","336377.0","whuber then which one do you suggest i use the _simple_ correlation coefficient or the _partial correlation coefficient_","comment"
"322843","","612650.0","why use a family to test a mean difference just model the means with a linearmixed model instead","comment"
"234711","","445803.0","sorry i thought that an internal link to a cross validated page would be of a guaranteed persistency and hence acceptable","comment"
"265913","","508708.0","tommasoguerrini hi tommaso if i used fourier predictors instead for the year month weekday and hour would that solve the collinearity issue","comment"
"231425","","438787.0","ttnphns yes i guess i was more confused about the term independently generated observations with randomly generated in sampling we often hear simple random sampling which makes me feel like independent samples i guess if we really want to combine both characteristics in describing a sampling method it should be the selection of observations is not dependent on each other independently and the probability of selection an observation is known randomly","comment"
"314490","","","regression rmse when dependent variable is log transformed","title"
"420858","","785187.0","multiply ny apples column by 12 and and compare with la apples","comment"
"11757","","20353.0","sigma2 is the error variance for purposes of estimation it is replaced by s2 the mean squared error that when the t_ n2 distribution comes in","comment"
"176808","","","how to evaluate the quality of the probability distribution output of a classifier","title"
"37461","","627873.0","the connection is not mysterious it is because they are members of the exponential family of distributions the salient property of which is that they can be arrived at by substitution of variables and or parameters see longer answer below with examples","comment"
"37920","","","let there be observed data points x x_1 x_n x_n where each x_n in rd lets assume these are distributed as a gaussian x cong mathcal n mu sigma let us also assume that the mean has a normal prior and covariance has inverse wishart prior since these are conjugate priors given observed data x we can draw posterior samples for mean and covariance using standard posterior update equations what we have so far is just the usual bayesian stufflet the data points be formed from a union of two sets of data x x+ cup x the data is still distributed as gaussian however lets assume that while drawing posteriors for mean and covariance we have to give more weight to data points in x+ than x with the weight itself being some hyper parameternote that we are not talking about mixture of gaussians we dont say there are 2 mixtures and the data might have arised from one of these mixtures we say the data still arises from one gaussian with the mean and covariance primarily according to x+ but perhaps altered shifted by some proportion because of x influence i am not sure how i can write the distribution for such a thing i initially thought i can write x cong mathcal n amu+ + bmu asigma+ + bsigma we then draw mu+ sigma+ from x+ and similarly mu sigma from x by usual methods we treat a and b as some hyper parameters if i write like this what is my assumption of the data am i implying x x+ + x rather than a union or the implication is too nonsensical to even worth a descriptionwhy would i want to do this thats a lengthy explanation and would distract the specific question the specific question is i have data partitioned into two sets the data arises from a gaussian distribution i want to draw posterior samples for the gaussian parameters according to one of the set however the other set also has marginal influence on the parameters how can i reparametrize the distribution to account for this ps any approximate reparametrization is fine ive used the word sampling hopefully that wouldnt distract the specific question to ideas such as lets reject samples based on some function any references to the literature that does something similar is also welcomethanks in advance for suggestions","post"
"348836","","","what the n capital means in the following ngramapproximation to the conditional probability of the next word in a sequence","post"
"129781","","","why will higher order of polynomials using poly function solve in lme model and lower orders wont","title"
"22647","","41222.0","btw does anyone have a reference for this approach","comment"
"90101","","176345.0","from the plot it appears that you have values greater than 100 that could be a sideeffect of bins being eg 90 100 100 110 a model with logit link is indeed likely to work better than anova on this evidence","comment"
"91638","","","am researching on poverty and growth i built a model for poverty which includes 11 variables theoretically affecting it these are inflation agricultural growth gini gdp growth rate etc my data had few variables in growth rate and others in percentage and index poverty and gini actually are indexed values i somehow managed to transform all of them in growth rate authors suggested these variables that is why i narrowed down my variables to those but my r sq is too low less than 1 for instance when i run the pooled regression f test proves to be insignicant as well i am really confused am using stata 10 someone please comment on this","post"
"163614","","","it seems like a fairly straightforward question but when i really think about it stouffers method doesnt make sense to me this is why assume a twotailed hypothesis you first calculate z_i from pvalues so lets take a fairly simple example lets take two pvalues of 005 this means that z_1 and z_2 are both approx196 according to stouffers method z_1 and z_2 are combined such that z frac sumlimits_ i1 kz_i sqrt k frac 196 + 196 sqrt 2 277this zscore then gets converted to a pvalue once again resulting in a pvalue of 0005 whereas the pvalues from each z_i individually is about 005 in this sense it seems as though stouffers test artificially changes the resultant pvalue to a value dissimilar to the pvalues of each z_i which to me doesnt make senseam i misunderstanding this test or can someone help me understand how why it works","post"
"15565","","","contexti have been reading about item response theory and i find it fascinating i believe i understand the basics but i am left wondering how to apply statistical techniques related to the area below are two articles that are similar to the area i would like to apply itr inhttp wwwjstororg stable 4640738seq7http wwwncbinlmnihgov pubmed 21744971the second being the one i would actually like to extend at this point in timei have downloaded a free program called jmetrik and it seems to be working great i think it may be too basic as far as irt goes but i am unsure i know the best way would likely involve learning r however i dont know if i can spare the time to tackle that learning curve note that we have some funding to purchase software but from what i see there doesnt seem to be any great irt programs out therequestionswhat are your thoughts on the effectiveness of jmetrikhow would you suggest i go forward in applying irtwhat are the best programs for applying irtdo any of you use irt regularly if so how","post"
"398777","","","testing for nonzero values above and below the diagonal of a matrix","title"
"420465","","","map versus componentwise maximum marginal","title"
"145644","145641.0","","yes it seems to be correct the fractional filter is defined by the binomial expansion delta d left 1lright d 1dl+frac dleft d1right 2 l 2 frac dleft d1right left d2right 3 l 3 +cdotsnote that l is the lag operator and that this filter cannot be simplified when 0d1 now consider the processdelta d x_ t left 1lright d x_ t varepsilon_ t expanding we getdelta d x_ t left 1lright d x_ t x_ t dlx_ t +frac dleft d1right 2 l 2 x_ t frac dleft d1right left d2right 3 l 3 x_ t +cdotsvarepsilon_ t which can be written as x_ t dx_ t1 frac dleft d1right 2 x_ t2 +frac dleft d1right left d2right 3 x_ t3 cdots+varepsilon_ t see asset price dynamics volatility and prediction by stephen j taylor p 243 in the 2007 ed or time series theory and methods by brockwell and davis for further references","post"
"368076","","691570.0","kedarps thanks do you maybe have any formal proof theorem verifying that","comment"
"403494","403485.0","","corrections are made for familywise error rates so you effectively need to correct for each pvalue that you calculate that you believe to be in the same family of hypotheses it is hard to tell you exactly whether this needs to be three or six but my guess would be six as it sounds like all three tests are related some of the answers here might illuminate this further and help you decidealthough this is something of an aside you can avoid some of the multiple comparison issue by conducting a multivariate analysis of variance manova by effectively combining all three of your response variables into one and simultaneously testing if these differ between your two covariates meaning you only need to correct for two comparisons instead of three or six","post"
"114490","","","optimal bin width for two dimensional histogram","title"
"163774","","","interpretation of tsaarimax output model is presented in r","title"
"300778","","571696.0","sounds interesting but h must be always positive than and this is not the case for a bivariate normal","comment"
"439266","","818288.0","what you mean by labeled examples i have only lists since 60days wanted to set as output list next to the input lists","comment"
"136247","","259553.0","when you talk about restricting the range were you thinking of the truncated normal distribution http enwikipediaorg wiki truncated_normal_distribution","comment"
"185094","184706.0","","the poster asks how to calculate the expected value as others have suggested one must know the specific sequence of the events to understand the sum of the events the closest we can come to calculating the result would be the binomial distribution as others have already demonstrated i was interested in the question and wanted to provide some sense of the number of turns as originally asked i used a monte carlo method similar to antonis solution in r codeeq5 0for j in 1100000 sum 0 trial 0 turn 0 for i in 1100 trial i sample c 1 1 1 replacetrue sum cumsum trial turn i if sum i 5 break eq5 j turn hist eq5 xlimc 0 100 breaksc 0100 xaxtn axis side1 atc 0100 table eq5 note i ran only 100 turns causing a truncation of the very long rightsided tail the most common number of turns where a value of 5 occurs is shared between the 7th and 9th turn","post"
"433529","","808575.0","heres one reason lets imagine you were metaanalyzing two studies the first found that some treatment doubled the rate of the outcome the second study however found that the same intervention cut the rate of the outcome in half in this case i would expect the metaanalyzed rr to be consistent with the null effect an rr of 1 but averaging an rr of 2 and 05 results in a combined rr of 125 taking the natural log first ensures that the average rr is 1","comment"
"8746","","15789.0","+1 thank you for your answer it was helpful to my search but i cant mark two answers as accepted","comment"
"301479","","","i have been looking for a method similar to quantiles but using means instead medians but i couldnt find anythingi mean a procedure where we obtain the mean of the whole sample bar x then take the mean of only the data above bar x and next the mean of only the data below bar x we get here the mean and two submeans around the mean we dont need to stop here and can further continue subdividing the sample if there is enough datafor instance for the sample 0167 0177 0181 0181 0182 0183 0184 0186 0187 0189 the mean is 01817 the submeans are 01765 and 01852 the subsubmeans are 01670 01797 01830 and 01873 and so on until finally we obtain the original values of the sample in the fifth iterationi dont know if this method exists and if it exists i dont know the name and cannot find information about it","post"
"346823","","654207.0","x_1 and x_2 are independent but not given sum x_i","comment"
"237406","","451152.0","user1093107 then it is off topic here","comment"
"237761","","451938.0","hi seanv507 i edited the original post to explain as much as i can unfortunately this is an assignment work for a job interview and i dont have more info bu i need quick results","comment"
"90139","90134.0","","what about optimizationlets see if i understand you correctly you have a model p y x theta conditioned on some observation x and a set of parameters theta and a prior p theta leading to a joint likelihood of mathcal l p y x theta p theta the parameters are distributed according to a known multivariate normal ie theta sim mathcal n mu sigma you want to find the map solution to this problem ietext argmax _ theta mathcal l a special case of this problem is well studied in the neural networks community known as weight decay in that case mumathbf 0 and sigma mathbf i sigma2as you already noted the trick is that text argmax _ theta mathcal l text argmax _ theta log mathcal l when you take the log of the gaussian density many ugly terms the exponential vanish and you will end up with sth like log p theta 1 over 2 theta mu tsigma 1 theta mu + text const if you differentiate that sam roweis matrix identities will come in handy and let you arrive at 1 over 2 partial theta mu tsigma 1 theta mu over partial theta sigma 1 theta mu please verify this was done quickly and in my head together with the derivatives of your model you can use offtheshelf optimizers to arrive at a map solutionupdate incorporated comment by david j harris formulas should be correct now","post"
"154409","","","assume i have a function g x that i want to integrate int_ infty infty g x dxof course assuming g x goes to zero at the endpoints no blowups nice function one way that ive been fiddling with is to use the metropolishastings algorithm to generate a list of samples x_1 x_2 dots x_n from the distribution proportional to g x which is missing the normalization constant n int_ infty infty g x dx which i will call p x and then computing some statistic f x on these xs frac 1 n sum_ i0 n f x_i approx int_ infty infty f x p x dx since p x g x n i can substitute in f x u x g x to cancel g from the integral resulting in an expression of the form frac 1 n int_ infty infty frac u x g x g x dx frac 1 n int_ infty infty u x dxso provided that u x integrates to 1 along that region i should get the result 1 n which i could just take the reciprocal to get the answer i want therefore i could take the range of my sample to most effectively use the points r x_max x_min and let u x 1 r for each sample ive drawn that way u x evaluates to zero outside of the region where my samples arent but integrates to 1 in that region so if i now take the expected value i should geteleft frac u x g x right frac 1 n approx frac 1 n sum_ i0 n frac u x g x i tried testing this in r for the sample function g x e x2 in this case i do not use metropolishastings to generate the samples but use the actual probabilities with rnorm to generate samples just to test i do not quite get the results i am looking for basically the full expression of what id be calculating isfrac 1 n x_ max x_min sum_ i0 n frac 1 e x_i2 this should in my theory evaluate to 1 sqrt pi it gets close but it certainly does not converge in the expected way am i doing something wrongys rnorm 1000000 0 1 sqrt 2 r max ys min ys sum sapply ys function x 1 r exp x2 length ys ## evaluates to 06019741 1 sqrt pi 05641896edit for cliffabthe reason i use the range is just to easily define a function that is nonzero over the region where my points are but that integrates to 1 on the range infty infty the full specification of the function is u x begin cases frac 1 x_max x_min x_max x x_min 0 text otherwise end cases i did not have to use u x as this uniform density i could have used some other density that integrated to 1 for example the probability density p x frac 1 sqrt pi e x2 however this would have made summing the individual samples trivial ie frac 1 n sum_ i0 n frac p x g x frac 1 n sum_ i0 n frac e x_i2 sqrt pi e x_i2 frac 1 n sum_ i0 n frac 1 sqrt pi frac 1 sqrt pi i could try this technique for other distributions that integrate to 1 however i would still like to know why it doesnt work for a uniform distribution","post"
"6382","6353.0","","if this is an spss syntax question the answer is just put the categorical variable coded appropriately into the variable list for independent variables along with the continuous one on the statistics is your categorical variable binary if so you need to use a dummy or other valid contrast code if it is not binary is your categorical variable ordinal or nominal if nominal then again you must use some contrasting code strategyin effect modeling the impact of each level of the variable on the outcome or dependent variable if the categorical variable is ordinal then most likely the sensible thing to do is to enter it asis into the model just as you would with a continuous predictor ie independent variable you would be assuming in that case that the increments between levels of the categorical predictor indepdent variable only rarely will this be a mistake but when it is you should again use a contrast code model the impact of each level this question comes up in this forum quite often here is a good analaysishow to handle missing data is in my view a completely separate matter my understanding is that pairwise deletion is not viewed as a valid approach for multivariate regression listwise is pretty common but can can also bias results certainly is a shame multiple imputation is a thing of beauty","post"
"76111","","148585.0","you could add a constant to the coefficients but that destroys the meaning of it as a correlation why do you need a negative exponential function would some other function be ok","comment"
"729","","108389.0","its axiomatically true","comment"
"295460","","562244.0","horacet more importantly i think the financial markets are inherently unpredictable in the following sense once someone finds a pattern according to which the market currently develops one invests in the market and by that very action changes the supplydemand balance and thus changes the pattern it is like some elementary particles in physics once you measure them they are changed forever you cannot measure the particle without changing it i think financial markets are similar it is similar with macroeconomics too where macroeconomic policy affects the economy","comment"
"51982","","101593.0","linear regression does not require that the predictors be normal nor that the dependent variable be normal only that the residuals be normal","comment"
"33578","","66665.0","added a few classical test theory i believe reliability measures attempted an itemresponse model but im not quite sure how to approach the data all the ordinal data examples i can find map hundreds of individuals to a few questions with ordinal responses this data is thousands of comments mapped to 20 individuals who gave ordinal responses as such i can run the analysis without data manipulation but im not quite sure what im testing","comment"
"258253","","591183.0","this is exactly what i am dealing with right now how do you define the threshold minimum analyte in training set + rmsep","comment"
"408863","","","ipw for the effect of treatment on treated with a continuous treatment","title"
"436580","","813880.0","the moderndive package was not written for use with bamls objects so there is no reason to expect that you would get identical information from commands that were written for bamls objects like summary you just need to play with the commands i suggested and see what you get","comment"
"436327","","813545.0","windstorm1981 can you then elaborate here what you are training and testing what is the parameter you are estimating with the training data as far as i can tell this is simply telling us to partition our data into several smaller sets calculate a performance measure after doing knn on each then calculate the averaage across sets there is no distinction between training and testing sets in this case","comment"
"126577","","241661.0","aksakal thank you for advise to ensure the correction of statistic analysis i have given up the interpolation idea to generate new points and decided to generate true observations by spatial analysis but i have learned a lot from your answers thanks","comment"
"309788","","588709.0","so this package http wwwstatsmodelsorg dev generated statsmodelsstatsproportionproportions_ztesthtml","comment"
"185852","","","probability function of two coin flips is pfrac 1 4 valid","title"
"200700","","380680.0","tim tks for the feedbak which example are you refering","comment"
"185294","","419519.0","are you still interested in this question","comment"
"71914","","139527.0","patrickcoulombe hehe you are right but sometimes some friendly force helps to get better questions","comment"
"337927","","638646.0","there are lots of generators that are much faster than the mersenne twister that are also highquality surely high quality enough for your purposes so dont feel those are your only two choices","comment"
"10560","10557.0","","most obvious solution would be to change your code in for loop with the following names mydat c name paste newname i sep but you need to clarify what your variable name is at the moment this loop will do 4 renames of the single columnin general if the names which you want to change are in vector this is a standard subsetting procedure names mydat names mydat in names_to_be_changed name_changes","post"
"321661","","610617.0","i suggest to start with christopher bishop pattern recognition and machine learning book if you dont have it you can check his paper video on modelbased machine learning he explains the main concepts very well after that you should be able to find the references yourself and remember this is called probabilistic programming i took the edx mit course on probabilistic inference and reasoning the course was pretty tough for me but i learned a ton","comment"
"148694","","284806.0","whuber it seemed like thats what he was using but it sounds like he came up with it independently","comment"
"390321","","732919.0","ben bolker i am trying to study statistics and r this is my first try you know knowing how to use mle is very important for parameter estimation","comment"
"184482","","","ive got spatiotemporal disease data at the county annual level for 20002014 im analyzing it to try to pull out temporal variations in disease incidence and was told that i should generate a spatiotemporal variogram the variogram appears to show a relationship but im having a hard time finding out how to interpret the graph which is attached is this graph showing that disease rates 1015 years apart are more strongly correlated at 100 km 200 km and 500km and that the temporal correlation drops off sharply at around 8 years apartif i remember correctly the lower the gamma the greater the autocorrelation ive looked for a guide on interpretation for a while now but all i can find are guides on how to generate models from this data","post"
"248476","","498420.0","it depends how do you define pdfs pmf may be thought as a special case of pdf you can define pdfs of discrete distributions in terms of dirac deltas etc so it is not a problem that distribution is of discrete or mixed type","comment"
"137152","","261424.0","there is a somewhat famous paper in economics arguing that something like what you are doing can be thought of as approximating a bayesian approach to model selection im too ignorant of baeyesian statistics to have an opinion on whether they are right or not heres the link http wwwnberorg papers w7750","comment"
"6605","","10908.0","eduardo the problem with most forms of statistical inference is that they are hard to generalise becuase they are often based on intuition rather than solid foundations the pvalue is such an example how to adjust for multiple comparisons how to deal with nuisance parameters arbitrary choice of the statistic on which the pvalue is based without a set of more broad axioms or desiderata to guide the generalisation pvalues are basically restricted to the realm of 1 parameter problems or where pivotal quantities exist","comment"
"348502","","656993.0","i do not understand the question regarding the null hypothesis i am not planning on performing significant tests but to work only with effect sizes in particular this is a research on education and it seems to me that cohen d is the more useful effect size since it can be compared across experiments for example https onlinelibrarywileycom doi abs 101111 j17508606200800061x but i would be interested and thankful on any reference on criticism of effect sizes","comment"
"139072","","","distribution of the maximum of two correlated normal variables","title"
"92384","","180865.0","the duplicate contains a thorough review of these issues our site contains literally thousands of examples you can find them by searching on keywords related to violations of these assumptions such as heteroscedastic nonlinear and autocorrelation as well as by looking at questions addressing residuals","comment"
"380377","","716671.0","whuber do i need to first convert my correlation matrix into a varcovariance matrix using r_x_iy_i sd_x_isd_y_i and then go from there","comment"
"59275","","","mapping gini index","title"
"40819","","79617.0","general social survey they collect data on all sorts of things","comment"
"324698","","615608.0","i have read that book several times its on my shelf and it was the first reference that came to mind when i saw your question i think i have some other explanations around too from the publications by the princetonharvard groups in the 1980s on eda and robust methods but i would have to search harder to find the relevant material","comment"
"91395","","","im working on a panel survey data where each individuals income was multiplied by a individualspecific random number each random number is evenly distributed from 05 to 15 to avoid any participant being recognized from the data basically i cannot directly compare two individuals income using the data for this type of multiplicative error i was wondering do i need to use any particular method to correct it any advice will be appreciatedproblem solved please see nestors answer","post"
"263909","173636.0","","ttnphns has provided a good answer doing clustering well is often about thinking very hard about your data so lets do some of that to my mind the most fundamental aspect of your data is that they are compositional on the other hand your primary concern seems to be that you have a lot of 0s for green products and specifically wonder if you can transform only the green values to make it more similar to the rest but because these are compositional data you cannot think about one set of counts independently from the rest moreover it appears that what you are really interested in are customers probabilities of purchasing different colored products but because many have not purchased any green ones you worry that you cannot estimate those probabilities one way to address this is to use a somewhat bayesian approach in which we nudge customers estimated proportions towards a mean proportion with the amount of the shift influenced by how far they are from the mean and how much data you have to estimate their true probabilities below i use your example dataset to illustrate in r one way to approach your situation i read in the data and convert them into rowwise proportions and then compute mean proportions by column i add the means back to each count to get adjusted counts and new rowwise proportions this nudges each customers estimated proportion towards the mean proportion for each product if you wanted a stronger nudge you could use a multiple of the means such as 15meanprops instead d readtable textid red blue greenc3 4 8 1 headertrue tab astable asmatrix d 1 rownames tab paste0 c 03 tab# red blue green# c0 12 5 0# c1 3 4 0# c2 2 21 0# c3 4 8 1props proptable tab 1 props# red blue green# c0 070588235 029411765 000000000# c1 042857143 057142857 000000000# c2 008695652 091304348 000000000# c3 030769231 061538462 007692308meanprops apply props 2 funfunction x weightedmean x rowsums tab meanprops# red blue green # 035000000 063333333 001666667 adjcounts sweep tab 2 meanprops fun+ adjcounts# red blue green# c0 1235000000 563333333 001666667# c1 335000000 463333333 001666667# c2 235000000 2163333333 001666667# c3 435000000 863333333 101666667adjprops proptable adjcounts 1 adjprops# red blue green# c0 06861111111 03129629630 00009259259# c1 04187500000 05791666667 00020833333# c2 00979166667 09013888889 00006944444# c3 03107142857 06166666667 00726190476there are several results of this one of which is that you now have nonzero estimates of the underlying probabilities of purchasing green products even when a customer doesnt actually have any record of having purchased any green products yet another consequence is that you now have somewhat continuous values whereas the original proportions were more discrete that is the set of possible estimates is less constricted so a distance measure like the squared euclidean distance might make more sense now we can visualize the data to see what happened because these are compositional data we only actually have two pieces of information and we can plot these in a single scatterplot with most of the information in the red and blue categories it makes sense to use those as the axes you can see that the adjusted proportions the red numbers are shifted a little from their original positions windows plot props 1 props 2 pchascharacter 03 xlabproportion red ylabproportion blue xlimc 0 1 ylimc 0 1 points adjprops 1 adjprops 2 pchascharacter 03 colred at this point you have data and a lot of people would begin by standardizing them again because these are compositional data i would run cluster analyses without doing any standardizationthese values are already commensurate and standardization would destroy some of the relational information in fact from looking at the plot i think you really have only one dimension of information here at least in the sample dataset your real dataset may well be different unless from a business point of view you think its important to recognize people who have any substantial probability of purchasing green products as a distinct cluster of customers i would extract scores on the first principal component which accounts for 995 of the variance in this dataset and just cluster that pcaprops prcomp adjprops 12 centert scalet cumsum pcapropssdev2 sum pcapropssdev2 # 1 09946557 1000000pcapropsx# pc1 pc2# c0 17398975 003897251# c1 01853614 004803648# c2 16882400 006707115# c3 02370189 015408015library mclust mc mclust pcapropsx 1 summary mc # # gaussian finite mixture model fitted by em algorithm # # # mclust e univariate equal variance model with 3 components# # loglikelihood n df bic icl# 2228357 4 6 1277448 1277448# # clustering table# 1 2 3 # 1 2 1","post"
"291491","","556036.0","deepnorth thx i added a little bit more to the ci part","comment"
"658","643.0","","what i loved most with clt is the cases when it is not applicable this gives me a hope that the life is a bit more interesting that gauss curve suggests so show him the cauchy distribution","post"
"45401","","89274.0","just out of curiosity why are you using gamma its a less frequently used model can you say a little about your situation data and goals also is there something specific that you are worried about do you think there may be a certain problem w your model or are you just wanting to do your due diligence","comment"
"405249","","757567.0","whuber i did impossible explained lebesque integral without measure theory","comment"
"276386","","741160.0","i use the same data for training and testing for coil20 dataset the training accuracy is lower than the testing accuracy 09743 training 09958 testing accuracy i dont use any regularization","comment"
"200523","","380378.0","you really need to explain why you want to get the error and transform it into a percentage","comment"
"82346","","161457.0","i first posted that on stackoverflow here are the answers i got there see practical vs statistical significance http statsstackexchangecom q 81778 3601 over on the statistics stackoverflow site aaron yesterdayalso this answer and others to why is statistically significant http statsstackexchangecom a 79304 3601 not enough aaron yesterday","comment"
"111478","","","unbalanced dataset roc curve to compare classifiers","title"
"80895","","158488.0","rocinante given the information stated in the original question i dont see what else you can do but pick the largest dollars per day given more information sales on each day reason for why some ads ran longer than others you can do more","comment"
"204577","","388161.0","orangetrout in my example you cant compare beta_x beta_z ones for absolute and the other for relative change in independent variable you can easily compare beta_z beta_s the scaling factor is ln 10 of course you have to make sure that the units of measure are the same","comment"
"13377","","693531.0","the standard proof does not assume that x has a density","comment"
"16051","","623592.0","what exactly are you trying to achieve how the dataset looks like","comment"
"272260","","529966.0","richardhardy its no problem ive submitted the paper already so its done now this was a totally new area for me and a lot of stuff i had to self teach myself as id not done any modules on this but i really wanted to come back and thank you for all your help truly i dont think i could have done it without the assistance of yourself and others on here so thanks a lot have a great day","comment"
"410298","","","why does crosscorrelation function ccf in r have nonzero values","title"
"215126","","408370.0","i am referring to the techniques for calculating derivatives known as automatic differentiation or the differentiation of algorithms using these ideas one sets up a general scheme for calculating derivatives which many advantages one of which is that the user does not have to calculate the derivatives by hand for some reason these techniques are not nearly as well known in computational statistics as they should be","comment"
"104899","","202895.0","yes the data is a counting of 6 cell cultures where each have been treated with a different drug 6 drugs in total for each counting the cells were divied into two classes phenotype + and phenotype p+ and p so one counting can for example have 60 p+ cells and 40 p phenotype my question is if i can use a statistical test to produce some pvalue for the increase or decrease of the p+ or p phenotype after drug treatment compared to control the problem is i only have one replicate so i can not use the tests i normally use anova for instance","comment"
"394036","","739668.0","thanks guys for helping out its running a lot faster after i implemented the 1 cdf idea","comment"
"309305","309302.0","","genetic approaches are generally fairly junky at least since we have the gradient available typically using the gradient will learn fasterits true that karpathy openai have proposed to use es evolutionary strategies and showed that it learns faster in terms of elapsed time but they are parallelizing onto zillions of machines at a time on a single machine es runs something like ten times slower than eg pg policy gradients a gradient approach or perhaps trpo trust region policy optimization just with eg 100 computers in parallel the ability to parallelize onto 100 machines means that overall es will train 10 times faster than trpo albeit at huge expense in terms of hardware costneat only works for simpletastic tasks look at the paper they detect a big circle in a picture with a big circle and a small circleif you must use genetic methods es is probably as good approach as any since you have the resources documentation of openai behind it eg https blogopenaicom evolutionstrategies edit in response to questions1 why does neat only work for simple tasksnot using a gradient makes it very hard to know in which direction to move in parameter hyperspace neat doesnt handle functions that are not very smooth eg critical factors in the performance of hyperneat van der berg and whiteson 2013 does some analysis on this point using the term fracture to describe this concept admittedly this is for hyperneat rather than basic neat http wwwcsoxacuk people shimonwhiteson pubs vandenberggecco13pdf2 how to backprop in rlyou run an episode and get a reward the reward is then the supervised training signal that you backprop through the network given the sequence of actions and states that you walked through during the episode as the inputshighly recommend david silvers lectures on the topic https wwwyoutubecom watchv2pwv7govuf0listpl7jpktc4r78wczcqn5iqyuwhbz8foxtalternatively karpathys blog post on policy gradients to play pong is pretty short and easy to read understand http karpathygithubio 2016 05 31 rl","post"
"43226","","84256.0","michaelmcgowan good point i agree","comment"
"7318","","","which nonparametric test can i use to identify significant interactions of independent variables","title"
"136836","","262683.0","i think a more constructive way of reading the question would be that it seeks a piecewise linear model for value in terms of temperature that uses two different coefficients one for low temperatures and another for high temperatures a clearer way of stipulating that would be valueb0+b1 temperature when temperature is high +b2 temperature when temperature is low the op used an unfortunate shorthand for this but the intention seems pretty clear from the rest of the question","comment"
"379670","","713869.0","please register or merge your accounts you can find information on how to do this in the my account section of our help then you will be able to edit comment on your own question","comment"
"394439","","743811.0","martijnweterings i dont think i saw that comment and cant seem to find it am i missing something i am very interested in this topic and like you suspect there are some in climatology research that are deliberately obfuscating the analysis of their research","comment"
"125523","","","what im trying to do is to construct a linear model in a form likey beta_0x_0beta_1x_1+beta_2x_2 + beta_3where beta_0 beta_1 and beta_2 are coefficient of predictors x_0 x_1 and beta_2 respectively and they all are positive which means my assumption of the model is that x_1 has negative effect towards the response y perhaps i misunderstand the concept of regression but if anyone has an idea how to achieve this in r please enlighten me or any other approach apart from regression model thanks in advance","post"
"24537","","","i intend to use a hierarchical binomial model which would look like the following in bugs language model for i in 1i y i dbin theta k i n k i for j in 1j theta j dprior alpha beta alpha dhyperprioralphabeta dhyperpriorbeta i wonder about the choice of the prior on theta and the hyperpriors do you know how to do this choice in order that1 theres no need to use mcmc classical simulations suffice and or2 this is a good noninformative choice in the sense of the frequentistmatching property the frequentist coverage of the posterior credible intervals is close to the credibility level","post"
"105793","","204912.0","the relevance of the clt for regression is that the coefficient estimates tend towards having a normal distribution as sample size increases even when the errors have a different distribution","comment"
"48757","","94863.0","i agree the assignment part cannot be directly put into the mathematical form only by this isolated step can we move the centroids around to minimize the function heres how i look at gradient descent if by bad initialization we are near the local minima the gradient descent will drag you down to local minima if you are near the global minima by good initialization it will drag you down the global minima but how this movement is mapping to cluster assignments is a blur","comment"
"250391","","","variance of a function of a random variable","title"
"268548","","513500.0","i think tdist converges to the normal distribution so its not a special case of normal","comment"
"313141","","594667.0","do you have any explanation for why this is so hard to clarify you can make assumptions about the densities","comment"
"140579","","703410.0","could you help answering a related question i asked on statsstackexchange https statsstackexchangecom questions 374248 whattoconcludeforthedatasetwhenthevarianceforprincipalcomponentsis","comment"
"178355","","","r gbm lower shrinkage gives worse results","title"
"169908","","321930.0","ok i see the confusion some people define sample variance as ssum x_ibar x n1 some people define it as ssum x_ibar x n","comment"
"424947","","793127.0","sure take the derivative of the pdf its calculus you know how to do the chain rule i think you want to take the derivative of the cdf and thats really how a density is defined","comment"
"272597","","522965.0","please tell us what you mean by cumulative variance your formula by ignoring variation in the means of those elements does not appear to provide their actual cumulative variance and could you also stipulate what you mean by best way would that mean simplest fastest clearest most reliable or something else","comment"
"93529","","","dummies instead of the chow test","title"
"247185","","470387.0","we have been using regression in my undergrad econometrics course i was first introduced to the term in basic statistics with the concept of regression to the mean but i have heard the term regression used to describe linear regression and nonlinear regression terms as to whose definitions i am still confused i have heard that regression has many useful applications but it seems so broad and nebulous so i was trying to pin down the definition especially because its name i have been told is misleading and related to a study by its creator was it francis galton","comment"
"76643","","","combining pvalues from different statistical tests applied on the same data","title"
"50860","","99541.0","thank you for the answer yes mean subtraction is necessary but what is appropriate scaling should i scale everything to 1 1 or only part of variables it is not so clear choosing different scaling for different variables gives different importance for different variables it should depend on task what is the appropriate importance but i do not see the right one in my case","comment"
"15093","15058.0","","use the npcmstest package in library np if you are using the r platform warning the function may take several minutes to evaluate your modelyou can also consider an informationtheoretic comparison of the response distribution and the predictive distribution ie kl divergence crossentropy etc","post"
"35766","","","the wikipedia entry on the clt states at one point for fixed large n one can also say that the distribution of s_n is close to the normal distribution with mean mu and variance frac1nsigma2s_n sum_ i1 nx_n n and x_i are iids with mean mu and variance sigma2i dont quite see how this follows from the other more formal definitions is this statement true and what is the source proof","post"
"117889","","","i found on the book casella berger statistical inference the following theorem 234 if x is a random variable with finite variance then for any constants a and b operatorname var ax+b a2operatorname var xthis makes me wonder the following problemgive an example of a random variable x with infinite variance and real numbers a b such that operatorname var ax+b ne a2operatorname var x","post"
"290516","","554250.0","this is hard to reproduce and can only prove the sampling erroris that the only error in hypothesis test","comment"
"174248","","329910.0","would it be possible to do the anova by hand as if the coefficient and intercept from the first model were the best fits to the second model and then use an f test as before eg if firstdata gave 5+6x fit then compare 5+6newx to the intercept only model","comment"
"294509","294498.0","","anova and linear regression with only one independent variable that is categorical are equivalent in this case f test from anova is exactly the same as f test for whole regressionso you can run anova and apply some posthoc test ill recommend dunnetts test because it compares one level of your independent variable to every other levelstoy example with r first i create random y my dependent variable and x independent with 5 levels a b c d and e setseed 123 yrnorm 100 xgl 5 20 labels letters 15 then i run anova anova lm yx analysis of variance tableresponse y df sum sq mean sq f value pr f x 4 2965 074117 08854 04758residuals 95 79525 083711 and dunnett test to compare a with b c d and e library multcomp summary glht lm yx linfct mcp x dunnett simultaneous tests for general linear hypothesesmultiple comparisons of means dunnett contrastsfit lm formula y x linear hypotheses estimate std error t value pr t b a 0 019288 028933 0667 0907c a 0 003514 028933 0121 1000d a 0 026154 028933 0904 0776e a 0 023347 028933 0807 0836 adjusted p values reported singlestep method and for comparison linear regression with dummiessummary lm yx calllm formula y x residuals min 1q median 3q max 218925 059938 003713 060848 217000 coefficients estimate std error t value pr t intercept 014162 020459 0692 0490xb 019288 028933 0667 0507xc 003514 028933 0121 0904xd 026154 028933 0904 0368xe 023347 028933 0807 0422residual standard error 09149 on 95 degrees of freedommultiple rsquared 003594 adjusted rsquared 0004652 fstatistic 08854 on 4 and 95 df pvalue 04758see thatf test from anova and regression are the sametest statistics for dummies are the same as dunnetts statistics butpvalues are different since dunnetts test corrects for multiple comparisons","post"
"67700","","668498.0","another option is to parameterize by concentration c sqrt alpha2 + beta2 which scales independent of the mean velten et al 2015 http msbembopressorg content 11 6 812 use this with uniform prior on the mean and lognormal prior on c details are in their supplement pdf http msbembopressorg content msb 11 6 812 dc1 embed inlinesupplementarymaterial1pdfdownloadtrue","comment"
"328845","","623000.0","there are a few issues here the main one is why lme4 is not reliably producing a warning message the second one is the intuition behind lmer producing a nonzero variance under reml the third which i dont claim to understand is what the correct specification in sas is or at least what specifications are equivalent in r vs sas","comment"
"49139","49136.0","","gender w is the reference category it has b 0 in all cases","post"
"427803","","798149.0","question text updated the xgboost predictions are fractional predictions probabilities","comment"
"218381","","413120.0","so we dont have any limiting distribution of it whuber","comment"
"325510","","617218.0","peterellis any recommendation on how to be more precise in this estimation then increase trials without the fix in place to be more certain of the failure probability","comment"
"339734","","641827.0","anthonybell perhaps i misunderstood the question are you saying you have a known hierarchy and you wish to classify observations according to it","comment"
"366613","","688714.0","in that case kmeans or gmms gaussian mixture models are popular clustering algorithms that work well out of the box","comment"
"289044","","","quantitative method for monte carlo convergence check","title"
"5847","","8900.0","i probably misunderstood the question but where is the 100 value coming from","comment"
"261157","","","explanation of the decomposition in the non negative matrix factorization","title"
"232064","","439943.0","juho kokkala thanks for pointing out under the model i have described it follows that varepsilon x_i sim n x_i mu sigma_b2","comment"
"176807","","","i would like to learn basic statistical methods for quantitative data analysis in the social sciences i volunteer for a nonprofit and i want to take on a quantitative research project analyzing things like demographic and lawenforcement data the admittedly rather vague goal is to learn things from existing raw data that would be useful in informing policy decisions i have a masters in pure math focus on algebra i know how to learn independently out of dense textbooks but im quite weak on applied math and statistics i think what i want is to give myself the crash course version of an intro to quantitative research methods class taken by sociology phd students but leaning more on mathematical sophistication im not claiming to be any more sophisticated than sociology students im guessing that embracing the math side of things helps you learn enough to do basic research and i want to take advantage of my background so i want a book that does not presuppose any knowledge of statistics but also does not shy away from expounding on the theoretical framework when its helpful if it matters ill be using r yes i am prepared to invest plenty of time in the learning curvedoes anyone have suggestions as to resources thanks for the helpps i thought this question might have been asked before but i couldnt find it if ive started a duplicate thread then my apologies and id be grateful to be directed to the existing question","post"
"126459","","250227.0","amoeba in my answer i was considering a kind of minimally np approach choosing a test statistic to have maximum power against a specified alternative i will edit it thanks for your help","comment"
"331284","","","how do i interpret the jtest result in this result from gmm command from gmm package","title"
"266174","","","i had a job interview for a data science position during the interview i was asked what do i do to make sure the model is not overfitting my first answer was to use crossvalidation to assess the performance of the model however the interviewer said that even crossvalidation cannot identify completely overfitting then i mentioned regularization but the interviewer said that this could help to reduce overfitting which i agree but not to detect it are there other techniques that can be used to make sure a model is not overfitting","post"
"424644","","792580.0","validity depends on how you obtained the sample not on sample size","comment"
"430489","","803756.0","apologies should have been rather than","comment"
"97465","","189817.0","another derivative will tell you if the difference is linear or at least polynomial","comment"
"384212","","721870.0","also i did upvote but unfortunately as i dont have enough reputation it is not displayed publicly","comment"
"405263","","757608.0","speaking about se mean all i can think of is the coefficient of variation which you already meantioned sd mean or the confidence interval of the mean mean + 196 se how does the se mean value provide information for power analysis for example if se mean 013 what does this rule of thump mean for the power maybe you could mention an example","comment"
"417797","","779636.0","you could impute and then summarize the imputed values eg as mean and sd on a suitable scale if your subsequent models can deal with that as an input","comment"
"305089","","579986.0","could you explain how you would convert a length like 14 meters into a proportion what proportion would that be","comment"
"434586","","","i am trying to solve for the variance of x n a time series process x n +a_1x n1 w n where w n is white noise with zero mean and variance sigma2_v also a_1 1i am aware that the equation can be solved using the yule walker equations but i was trying to solve it brute force using expectationhere is what i didtake the ztransform of the process x z + a_1x z z 1 w z solving for the system function h z frac x z w z frac 1 1+a_1z 1 inverse ztransform h n a_1 nu n using convolution x n sum_ k0 n1 a_1 kw nk solving for the variance e x n x n e sum_ k0 n1 a_1 kw nk sum_ l0 n1 a_1 lw nl e x n x n sum_ k0 n1 a_1 ksum_ l0 n1 a_1 le w nk w nl since w n is a white noise process e w nk w nl 0 kneq l e w nk w nl sigma2_v klso the expectation becomes e x n x n sum_ m0 n1 a_12 m sigma2_vtherefore var x n sigma2_v frac 1 a_12 n 1a_12 but this is not correct this answer is nonstationary and we should expect a stationary solution for the zero mean white noisethe correct answer is frac sigma2_v 1a2_1 it would be very helpful if someone could point out what i did wrong i would like to solve this problem using this method that way i can more appreciate using yulewalker equations thank you","post"
"241243","","","how to support my claims in the following scenario","title"
"33811","","91524.0","the remark about maintaining old packages goes just as well for a user written macro or an old proc that sas hasnt updated","comment"
"130920","","596714.0","yes there is no one only way of modeling you always have different options","comment"
"33295","","65953.0","and this is not the only weakness of this algorithm what would you recommend then","comment"
"258132","","","i have a 2d data set and i do not know its distributioni have used r to calculate mean and covariancehow can i fit a bivariate normal distribution to my data set","post"
"247806","","763389.0","possible duplicate of enormous coefficients in logistic regression what does it mean and what to do https statsstackexchangecom questions 48739 enormouscoefficientsinlogisticregressionwhatdoesitmeanandwhattodo","comment"
"240796","","461008.0","could the next token be or na can you provide some example data to illustrate the situation you have in mind","comment"
"259365","","","in a single stream of observations i have some prior knowledge about the level of noiseness of the data that get feed into my kalman filter some points are more noisy than the othersto make use of this knowledge i have two thoughts 1 partition the data into two streams low confidence and high confidence 2 scale my confidence in data to a factor b w 0 and 1 and multiply the kalman gain with this factorcan someone shed some light on this problem with some new thoughts or comment on my thoughts any comments would be appreciated","post"
"21478","","128217.0","i took the liberty of formatting your post using some of cvs markdown options to make it a little more readable mattkrause if you dont like it roll them back w my apologies","comment"
"65707","","127058.0","user34790 you may take a look of the book bayesian data analysis chapter 3 by a gelman et al","comment"
"158158","","","does it make sense to compute adjusted r2 with test set","title"
"288217","","","i have a list of numbers and i am trying to detect a drastic point difference from the previous numbers and see if the pattern has changed for instance x 5000 5500 6250 4800 3950 7200 5500 800 1200 900 500 400 300 200 above there is high spending until 800 and then it seems that there is high spending before the 800 and low spending after the 800 all in all after 800 the spending has decreased a good bit i want to try and divide the list based on this drastic point and then check if there is a different pattern ie there is high spending before and low spending after or if there is low spending before and low spending after or low spending before high spending after or high spending before and high spending after essentially i am looking for this sort of inflection point and trying to detect if there are two different classes of numbers i know i could set a threshold and check each number or assume a normal distribution and check standard deviations is there a better way to approach this problem from a more statistical point of view note that the numbers could be of larger scale purchases could be tens of thousands of dollars and then drop to thousands of dollars and this should indicate a change im wondering if there is some statistical based method that is better than using standard deviations and means for a set of numbers like x above the following does not work welldef reject_outliers data m 2 d npabs data npmedian data mdev npmedian d s d mdev if mdev else 0 return data sm","post"
"362982","","","in the evaluation of classification models ive found one model to have a higher accuracy and cstatistic auc as compared to a second model however the second model has higher sensitivity specificity positive predictive value and negative predictive value is this mathematically possible","post"
"31007","","60002.0","thanks this makes sense i am glad to see the asymptotic variance you posted it is the same i found in other stats books and the one i am using however i am a little worried that the kappa variance traditionally referred in the remote sensing literature is very different would you by the way also know the reason for that the remote sensing version is the one reported by congalton page 106 http wwwscribdcom doc 53393988 60 kappa","comment"
"193158","","367035.0","reemmalhaj logistic regression is inherently about modeling the response of a binary variable to one or more other variables while that doesnt necessarily suggest that one causes the other its hard to back that exposure out the other direction id suggest depending on your data something as simple as an anova if you want to know the level of biomarker as it varies between groups","comment"
"10325","","","what does inversechisquare in fisher method classifying exactly do","title"
"140930","140922.0","","not entirely sure about the do i have to generate a normal sampling to accomplish this but here are the parallel examplesin stata the predict command will not work unless you have done some analysis before that for example linear regression using reg commandsysuse autoreg price mpgpredict uhat residualthis will give you the residual called uhatin r same idea youll need to have an object first after the lm command a set of residual will be saved in the model output youll need to use residual to get it again here is an example in linear regressionx rnorm 1000 y x 15 + rnorm 1000 0 250 m01 lm y x uhat m01residualhist uhat","post"
"153445","","293096.0","something seems to be missing from your problem description why not continue your logic with z e","comment"
"325007","","616373.0","hxd1011 i commented on it","comment"
"390181","","","please is it ever possible for the prior distribution to contain more information about parameter s than the posterior distribution if yes when can that occur is it the same concept as the posterior being diffuse with respect to the prior i am reading the paper on a comparative review of dimension reduction methods in approximate bayesian computation by blum et al 2013 and i came across the concept of a diffuse posterior on page 6 i would appreciate an explanationthanks","post"
"423954","423941.0","","what if the data does contain only one large cluster with 95 of the data in it maybe most of your customers behave similarly in the available datayour visualization only shows one big cluster so kmeans does what it is supposed to doyour best bet is to preprocess the data differently and also consider other algorithms since your data is not continuous and quite sparse i would rather use association rule mining to identify buying patterns rather than clustering","post"
"328965","","623129.0","why would this answer my question i would like to know the details","comment"
"104601","","202169.0","the reason for making these bins requires a lengthy explanation in short the objects kp for instance is just too less ie the frequency when compared to other objects treating them as individual objects is trivial so for my analysis i found that it is better to combine such objects together may be hk can be combined together into one bin too so what binning technique to use if any is the question","comment"
"274051","","","how can i determine the classification of my logit model without a cutoff point","title"
"276631","","","machine learning to find an optimal set of parameters for a segmentation algorithm","title"
"103483","","200143.0","momo woops sorry i thought you used that synonymously they are confidence intervals","comment"
"321344","","610305.0","identifying peaks and troughs is surely programmable by selecting values higher lower than their neighbours remember to wrap around and work on visits 24 visits 1 visits 24 visits 1 the bigger deal is how and how much to smooth before you do that","comment"
"58245","","111792.0","six locations isnt too few if youre interested in those six locations","comment"
"368182","","","i have recently been given a set of practice problems for my probabilities course and i have no idea where to even start on this question the distribution of x the number of toppings ordered by a randomly selected customer is given in the table below it turns out that x is independent of the size of the pizza and the type of cheese and that each topping is equally popular as are the two cheese types p xx 0 031 032 013 01 4 02what is the mgf of xi am pretty sure this is a binomial distribution so would i just put 1 + et n as my answer overall i am really confused on this topic and would like some help that isnt too discrete thank you","post"
"273557","","524561.0","adrian thanks what i was thinking was that we may be able to calculate the probability of the label p z_ new k for the new observation based on the given data p z_ new k x to get a new set of lambda_k and then use them as the mixing probabilities but maybe i am mistaken","comment"
"214198","","406740.0","although i agree with ben bolker that the comparison is of limited use id recommend here a direct quantilequantile plot often called a normal probability plot and various other names histograms always raise small or even large questions of the number and width of bins and their origin which a qq plot avoids in r i believe that is usually got with qqnorm","comment"
"210697","","402160.0","wouldnt total ice cream melt in a county be more closely related to population of the county than to its area","comment"
"212900","","404264.0","+1 nice way of showing the answer comes out to 7 13 as opposed to my methods just producing a decimal number which equals 7 13 i think my ways are more straightforward and allow for a mechanical solution method i have an initial state distribution and transition matrix let them via matlab or whatever do the work ha ha my calculations also show the probability of having first gotten to 1 2 or having first gotten to 6 6 as of any desired number of die rolls ie the transient behavior so i think some of your logic would have to be aborted or modified to get all that","comment"
"287264","","","a helpful hint would be appreciated because i cannot seem to figure out how to calculate the expected valuea lot contains 17 items each of which is subject to inspection by two quality assurance engineers each engineer randomly and independently selects 4 items from the lot determine the expected number of items selected bya both engineersb neither engineerc exactly one engineer","post"
"76543","","","why do pca and factor analysis return different results in this example","title"
"247294","","","i have a data set for which the explanatory variable is being compared against two response variables fairly standard however my task is to make the fit of the model be constrained between the upper and lower measurements of my explanatory variableany advice citations code anecdotes to help me search the net will all be greatly appreciatedthanks","post"
"175441","","332367.0","it must be some logic behind to report dont you think i mean that such situation is not always the case i have number of occurrences with not so strong significance in the omnibus but nevertheless post hoc is positive for one or two pairs","comment"
"33941","","67391.0","what is your answer to the question what kind of factors should be examined in this kind of performance profiling for sports and specifically for the ye case","comment"
"369711","","694893.0","i got 083 as well by p tea and coffee p tea p coffee 015 02 09 083 they might have got p tea and coffee to be 016 somehow and that would give 089 anyway your answer solidified mine thank you","comment"
"131147","","249688.0","the given definition of continuous random variable is random variables with values that are measured are called continuous random variables the example given is the maximum temperature reached during a chemical reaction for discrete random variables the definition is quantities that are counted and the example given is number of heads in 10 throws of a fair coin no definition was given for mixed random variables","comment"
"402508","","753196.0","welcome to the site id highly recommend adding a reproducible example to your post","comment"
"330565","330559.0","","its not that it is necessarily better than text sigmoid in other words its not the center of an activation fuction that makes it better and the idea behind both functions is the same and they also share a similar trend needless to say that the tanh function is called a shifted version of the text sigmoid functionthe real reason that text tanh is preferred compared to text sigmoid especially when it comes to big data when you are usually struggling to find quickly the local or global minimum is that the derivatives of the text tanh are larger than the derivatives of the text sigmoid in other words you minimize your cost function faster if you use text tanh as an activation fuction but why does the hyperbolic tangent have larger derivatives just to give you a very simple intuition you may observe the following graphthe fact that the range is between 1 and 1 compared to 0 and 1 makes the function to be more convenient for neural networks apart from that if i use some math i can prove that tanh x 2 2x 1 and in general we may prove that in most cases big frac partialtanh x partial x big big frac partialtext x partial x big","post"
"394965","","741017.0","i do have multiple discrete numeric explanatory variables and a single response variable i wanted to know if i could get statistically significant trends even with discrete explanatory variables","comment"
"73597","","143146.0","thank you very much actually i am applying fuzzy rdd on some simulations where for simplicity errors are normally distributed and homoschedasticity holds still my dgp is characterized by heterogeneity i am wondering if it is possible to address it through the the biprobit approach","comment"
"122753","","233914.0","i had a look at the video could you please explain a bit more your last statement thanks alot","comment"
"129522","","246707.0","jitter means to edit your plot so that overlying points are placed beside eachother to not obscure the view of one datapoint over the other its often used in r plotting functions","comment"
"375229","","706055.0","need to be step by step given p 010 for 4 wayinteraction fit a model just exclude 4 way interaction then check 3 3wayinteractions if largest p 010 exclude it fit another new model sometimes the notsigterms will become sig after you exclude others","comment"
"9782","","42332.0","franck did you note my question http statsstackexchangecom questions 23238 crossvalidationandordinallogisticregression on ordinal regression on the outcome of various function from your rms package how do i interpret a given value of dxy your answer here helped me but can you tell more and a crossvalidate dxy if you dont have time to write a long answer a bibliographic pointer would be welcome","comment"
"232184","","","let x_ 1 ldots x_ n be the order statistic of an iid sample of size n from exp lambda suppose the data is censored so we see only the top 1p times 100 percent of the data that is x_ lfloor p n rfloor x_ lfloor p nrfloor + 1 ldots x_ n put m lfloor p n rfloor what is the asymptotic distribution of left x_ m frac sum_ i m+1 n x_ i nm right this is somewhat related to this question and this and also marginally to this question any help would be appreciated i tried different approaches but was not able to progress much","post"
"427418","","797263.0","isnt everything in nature somehow autocorrelated phenomena that are not time series would not be autocorrelated because autocorrelation is a property of a time series though there are notions of spatial correlation and other to reflect relationships along dimensions other than time but since everything is taking place in time autocorrelation might indeed be pretty ubiquitous","comment"
"19555","","285743.0","are you planning on doing any statistical inference confidence bands hypothesis tests etc that would alter the approach","comment"
"99502","","193938.0","is there anything i can do to better specify the question i expected at least a clue on which statistical law to follow i could find the remainder next if i dug deep in my statistics archives","comment"
"410859","","767622.0","perhaps observation would have been a better term but if we have patients who visit us at all different ages we track many of their characteristics from biometric ones like their height or weight to functional ones like their visual acuity","comment"
"166311","","315453.0","ctd if you seek mathematical proof for most biological phenomena youll be wasting your time but if you expect to find anything other than nonnegative correlation for such repeated measures in the above circumstance in a realistic noncontrived situation youll be hard pressed to find one","comment"
"220316","219828.0","","summarythe question misinterprets the coefficients the software output shows that the log odds of the response dont depend appreciably on x because its coefficient is small and not significant p0138 therefore the proportion of positive results in the data equal to 100 1995 approx 80 ought to have a log odds close to the intercept of 164 indeed logleft frac 80 20 right log 4 approx 14is only about one standard error 022 away from the intercept everything looks consistentdetailed analysisthis generalized linear model supposes that the log odds of the response h being 1 when the independent variable x has a particular value x is some linear function of x text log odds h1 xx beta_0 + beta_1 xtag 1 the glm command in r estimated these unknown coefficients with values hatbeta_0 1641666pm 02290133 and hatbeta_1 00014039pm 00009466the dataset contains a large number n of observations with various values of x written x_i for i1 2 ldots n which range from 823 to 3916 and average bar x 2238 formula 1 enables us to compute the estimated probabilities of each outcome pr h1 xx_i if the model is any good the average of those probabilities ought to be close to the average of the outcomessince the odds are by definition the ratio of a probability to its complement we can use simple algebra to find the estimated probabilities in terms of the log oddswidehatpr h1 xx 1 frac 1 1 + expleft hatbeta_0 + hatbeta_1 xright as a nonlinear function of x thats difficult to average however provided beta_1 x is small much less than 1 in size and 1+exp hatbeta_0 is not small it exceeds 6 in this case we can safely use a linear approximationfrac 1 1 + expleft hatbeta_0 + hatbeta_1 xright frac 1 1 + exp hatbeta_0 left 1 hatbeta_1 x frac exp hatbeta_0 1 + exp hatbeta_0 right + oleft hatbeta_1 xright 2since the x_i never exceed 3916 hatbeta_1 x_i never exceeds 3916times 00014039 approx 055 so were ok consequently the average of the outcomes may be approximated aseqalign frac 1 n sum_ i1 n widehatpr h1 xx approx frac 1 n sum_ i1 n left 1 frac 1 1 + exp hatbeta_0 left 1 hatbeta_1 x_i frac exp hatbeta_0 1 + exp hatbeta_0 right right 0162238 + 0000190814 bar x 204943 although thats not exactly equal to the 1995 observed in the data it is more than close enough because hatbeta_1 has a relatively large standard error for example if beta_1 were increased by only 03 of its standard error to 00011271 then the previous calculation would produce 1995 exactly","post"
"232420","","440770.0","eg what you have is a slightly simpler version of this http statsstackexchangecom questions 139522 completingthesquareforgaussianmultivariateestimation and this http statsstackexchangecom questions 33418 howdoicompletethesquarewithnormallikelihoodandnormalprior","comment"
"88253","","172474.0","if thats the assignment i think youve done a good job","comment"
"241353","82045.0","","sumio watanabe algebraic geometry and statistical learning theory cambridge university press cambridge uk 2009sure to be influential this book lays the foundations for the use of algebraic geometry in statistical learning theory many widely used statistical models and learning machines applied to information science have a parameter space that is singular mixture models neural networks hmms bayesian networks and stochastic contextfree grammars are major examples algebraic geometry and singularity theory provide the necessary tools for studying such nonsmooth models four main formulas are established the log likelihood function can be given a common standard form using resolution of singularities even applied to more complex models the asymptotic behaviour of the marginal likelihood or the evidence is derived based on zeta function theory new methods are derived to estimate the generalization errors in bayes and gibbs estimations from training errors the generalization errors of maximum likelihood and a posteriori methods are clarified by empirical process theory on algebraic varieties","post"
"135621","","","what is the proper way to do vector based linear regression in r","title"
"13867","","","correct use of partial derivatives example polynomial regression","title"
"364223","","684271.0","i think i need to rewrite the questionits about can we build a neural network which can say whether a given input is present in our dataset already or not we are not talking about y its just whether a newly taken input which isnt in our dataset and to know whether that input already exists in our dataset or not","comment"
"137037","","261100.0","and how does it finds that suitable placewhat is the logic","comment"
"173144","","327130.0","why do you need distances at all what do you want to do with them if you have mixed types of variables people typically use gowers distance eg see here http statsstackexchangecom a 164694 7290","comment"
"143505","","","when adjusting for x1 have we adjusted for x2 to the extent that x2 is related to x1","title"
"27197","","50400.0","yeah that http enwikipediaorg wiki bias_of_an_estimator#effect_of_transformations is always a problem with unbiased estimators they are not invariant and you have to find ad hoc estimators","comment"
"4176","","6162.0","csgillespie thank you for taking the time to write this answer i have added an update section to my original answer one more question i have for you is if there is an underlying reason why we believe failures rates to be exponentially distributed or are we just back fitting the empirical data we see from known failures to a distribution that fits","comment"
"198380","","","so the backshift operator is defined as bx_tx_ t1 and the forward shift operator is defined asfx_tx_ t+1 can someone please help show me how fb 1 and bf 1 are truemy attemptfx_tx_ t+1 sof 1 x_tx_ t1 bx_tandbx_tx_ t1 sob 1 x_tx_ t+1 fx_t","post"
"435127","435123.0","","here is my take using vectorization can make this a little easier comments indicate my logicimport numpy as npimport numba numbajitdef sim # draw the number of claims for the entire year in advance num_claims nprandompoisson lam 10 size 365 #compute the cost of the claims cost_of_claims npzeros_like num_claims for i day in enumerate num_claims cost_of_claims i nprandomexponential scale 1000 size day sum # capital is where we start plus how much we gain lose on the ith day capital 250000 + npcumsum 110000cost_of_claims # any negative return npmin capital 0sims sim for _ in range 1000 1000 iterations takes less than a second and winds up with 76 of the runs having an instance where the insurance company has less than 0 capitalhere is a plot of the simulationsand here is one with a confidence interval","post"
"191412","","","i am new to predictive modelling i am unable to choose the correct model for predicting if a student will pass or fail a particular exammy data set input variables total_tests_taken historic_pass_percentage attendance_percentage etc response variable passfailflag yes no i have around 150000 sample student records with both input and response variables populatedbusiness case predict if a student will pass or fail an exam based on his historical track recordplease suggest me a model glm or randomforest or regression or cart etc to choose","post"
"6455","","10118.0","mpiktas the ljungbox test is based on a statistic whose distribution is asymptotically as h becomes large chisquared as h gets large relative to n though the power of the test decreases to 0 hence the desire to choose h large enough that the distribution is close to chisquared but small enough to have useful power i do not know what the risk of a false negative is when h is small","comment"
"184845","","351237.0","here is my question when you say it wont generalize to new cases why is that this auc and everything i am seeing is on new cases only test data and not training data so there is no overfitting happening here if you think that ways","comment"
"189136","","359492.0","whuber ive included a brief description on the applied part kindly notify of any necessary correction","comment"
"380797","","716020.0","thanks i was just curious i didnt need it for anything in particular this is interesting though","comment"
"163022","","309962.0","hacking would be a polite term for this practice","comment"
"262669","262658.0","","your question applies only to continuous random variables in the case of discrete random variables you do use probabilities and not densities for a continuous random variable the probability of each point one value of the variable is 0 and only intervals have positive probabilities obtained by integrating the density function over the interval since the sample consists of points you cannot multiply probabilities the result will always be 0 and you must multiply densities which are in some sense a representative of the probability but cannot be called probability to be even more specific probability density and density are one and the same two names for the same function to understand what the density function means you should have a knowledge of calculus the density function f x can be explained as the slope of the probability at point x f x dx can be explained as the probability of the point x which on one hand is equal to 0 because dx is equal to 0 but on the other hand becomes greater than 0 when integrated over an interval so f x only represents how dense the probability is at point x but is not the probability still can be used as a proxy to probability","post"
"13325","","23209.0","jefflovejapan yes you can say that it comes from probability theory ac is the complement of a in omega","comment"
"40404","","79107.0","bgreene thanks for your followup remarks i see my comment on subtracting the mean may not have been as clearly stated as i intended i simply meant that as an alternative in conjunction with a lpf to a bandpass filter in effect only removing the dc component instead of rolling off at low frequencies which might distort some of the desired signal of interest given its low frequency nice answer","comment"
"266586","","509919.0","when i started my answer i considered that because it had two parts whats normality and parametric vs nonparametric that there would be no single post to close as a duplicate of so i answered and then pointed to questions discussing each of those parts this is why we dont post separate questions all in one post but then about 10 minutes later i remembered that there had been a recent change to the way markingasduplicate worked mods and people with relevant gold tag badges can now add several posts to the list of duplicates slapping my forehead at my forgetfulness i did so","comment"
"25384","","46492.0","as you can see they are able to use the estimates with positive and negative estimates in a very clear manner which is what i was excited about when i got the original estimates with similar patterns of positive and negative but they have all the estimates and an intercept so i dont know how they did this","comment"
"436299","","813267.0","what does statically inconsistent mean this phrase is extremely rare and doesnt appear in any statistics book or journal afaik","comment"
"144684","","277097.0","i just had a quick play with this if you are interested it is possible to obtain an exact closed form solution for the product of n random variables that are iid n 0 sigma2 the nonzero mu case makes things much more complicated","comment"
"120005","","228868.0","whuber thanks thats exactly what i needed did you want to make it into an answer so i can accept it","comment"
"312046","","","batch normalization how to update gamma and beta during backpropagation training step","title"
"7235","7223.0","","you might check out david freedmans paper a note on screening regression equations ungated using completely uncorrelated data in a simulation he shows that if there are many predictors relative to the number of observations then a standard screening procedure will produce a final regression that contains many more than by chance significant predictors and a highly significant f statistic the final model suggests that it is effective at predicting the outcome but this success is spurious he also illustrates these results using asymptotic calculations suggested solutions include screening on a sample and assessing the model on the full data set and using at least an order of magnitude more observations than predictors","post"
"41857","","84479.0","max tks really is new for me how i can edit equation","comment"
"273531","","","plot two ys against each other with the same confounders","title"
"27170","27169.0","","if selection of validation data is difficult because of dependencies then it might be better to consider using bayesian regularisation instead of early stopping to avoid overfitting as the validation set is not then required most neural network packages have an implementation of this procedure for matlab i would recommend netlabnote however that the dependencies in the data may cause a problem of model misspecification as the standard bayesian approach assumes the data are iid so it is worth a try but not guaranteed to work","post"
"287117","","548859.0","i saw there was an attempt to edit the old one with a decision pending was it someone else than you who did that","comment"
"164885","","313628.0","the response variable presence absence of mutated form for aim 3 will come from national surveillance data independent variables will be derived from other national datasets","comment"
"50068","","97954.0","douglas it looks like the l2 norm on pp 54 and 69 where it is specifically called that thats equivalent to the frobenius norm in those cases where it appears to be applied to vectors at the bottom of p 26 its clearly the frobenius norm of a matrix","comment"
"195433","","432286.0","please dont crosspost","comment"
"229458","","434182.0","glen_b can this mc pass as the permutation test you suggest in your comment","comment"
"124042","","236366.0","thank you very much is this what is called a gls with log link function also i know the expression of hat beta in the case of a weightedleast squares but once i add the log transformation i dont know how to adjust the beta expression","comment"
"111298","","213542.0","if im understanding this right you are looking at mixtures of distributions on 0 1 d which are restricted to be independent within cluster i wouldnt call this a mixture of bernoulli distributions since a mixture of bernoulli distributions is trivially just another bernoulli distribution mixture of multivariate bernoulli distributions is a more appropriate name","comment"
"315592","","599352.0","note that the acf of lag 11 in the autobox model is 07 quite nonsignificant a possible reason is that your models with and without seasonal differencing are deficient is that the only seasonality that exists is deterministic for two months of the year free software often comes with a price","comment"
"210413","","400534.0","on my computer rhuber seems to work what do you mean by rubbish does it return letters","comment"
"424578","","792422.0","what if this simply is a quite bad paper does not sound like a high class venue to me","comment"
"112997","","223451.0","thanks glen_b i tried your suggestion with gamlss bezi distribution but i got the same problem in the end i ended up transforming the data with this formula http statsstackexchangecom questions 31300 dealingwith01valuesinabetaregression and using standard beta regression this approach works fine however i still wonder what was wrong with the oneinflated model","comment"
"435510","","","i know this is an openended question but id like to leverage wisdom of the crowd to make an optimal decisionthe case is i never learned statistics until 2 years ago since then im quite attracted to it and want to solidify my knowledge on the field by looking for some courses to enroll in i came across these three which sound interesting the problem is i can only afford to do one of them so if you have to choose which one would you go forthis course uses statistics the art and science of learning from data 4th edition 2017alan agresti christine franklin bernhard klingenberg as texbook upon successful completion of this subject students should be able to identify and justify statistical concepts and tools to analyse arealworld research problem use appropriate statistical techniques to conduct exploratory dataanalysis and present numerical and graphical summaries apply inferences from sample data to populationsexplain and relate the assumptions underlying the use ofparticular statistical techniques and check whether they areappropriate for a given data sampleconduct statistical analysis and interpret associated computer outputthis course uses kborovkov elements of stochastic modelling and e platen d heath a benchmark approach to quantitative finance as textbooks upon successful completion of this subject students should be able todefine and illustrate the terms used in probability and stochastic processes discuss and demonstrate the techniques of proof used in probability and some of the mathematical derivations that are important in the theory of stochastic processes state and apply the basic limit theorems of probability demonstrate an ability to use mathematical techniques to analyse the behaviour of various stochastic processes especially the longrun or steady state behaviour formulate and solve applied and theoretical problems involving probability and stochastic processesthis course is called advanced bayesian methods the subject outline doesnt spell out specific outcomes and no textbook nominatedideally i want to learn how to learn something practical and useful for my role as safety analyst i want to be able to use machine learning to model something such as how many accidents will happen given the historical data your advice would be much appreciatedthanks in advance","post"
"55820","54544.0","","very good question in this link it is explained that sas uses a numerical approximation which basically consists of a modification of the newtonraphson algorithm setting this option to both produces two sets of cl based on the wald test and on the profilelikelihood approach venzon d j and moolgavkar s h 1988 a method for computing profilelikelihood based confidence intervals applied statistics 37 8794 the link to the paper in jstor is here and the abstract is shown belowthe method of constructing confidence regions based on the generalised likelihood ratio statistic is well known for parameter vectors a similar construction of a confidence interval for a single entry of a vector can be implemented by repeatedly maximising over the other parameters we present an algorithm for finding these confidence interval endpoints that requires less computation it employs a modified newtonraphson iteration to solve a system of equations that defines the endpointsaccording to this abstract it seems like this is the secret of the speedy calculation","post"
"272760","","522926.0","thank you alecos the thing is the russian phrase is literally random value https ruwikipediaorg wiki d0a1d0bbd183d187d0b0d0b9d0bdd0b0d18f_d0b2d0b5d0bbd0b8d187d0b8d0bdd0b0 and this may have confused the reviewer the word variable peremennaya has a slightly different range of usage in russian","comment"
"385173","","723376.0","check here https statsstackexchangecom questions 7004 calculatingrequiredsamplesizeprecisionofvarianceestimate of course in mixed models you can gain information from the repeated measurements but still you need sufficient number of levels","comment"
"348621","","672556.0","the point was to give an example to show why it is much better to update them the mixing of the chain assuming it actually corresponds to some stationary distribution will potentially be made much worse by freezing i was just giving an extreme example i cannot see any upside in freezing aside from saving a pretty negligible amount of compute time","comment"
"64313","","124251.0","another question is at the 4 significance level test the claim that the population variances are equal against the alternative claim that the variances are unequal i know how to do this i have been working hypothesis tests of equality of variances for other problems but i encounter difficulty as 004 is not in my table and to get f critical i need to use the df which i know and the alpha which i know but the alpha of 004 is not in my table this is what i have gathered on the method for calculating the f critical value from watching youtube videos","comment"
"70384","70379.0","","just specify age as an additional between variable that will give you all of the interactions the term ancova for what is described in the paper is pretty much meaningless its just a fully factorial anova with a continuous predictor includedfollowing what the references article said if your dependent variable is accuracy you should really look at dixon 2008 and maybe jaeger 2008 dixon p 2008 models of accuracy in repeatedmeasures design journal of memory and language 59 447456jaeger f t 2008 categorical data analysis away from anovas transformation or not and towards logit mixed models journal of memory and language 59 434446","post"
"379889","","714267.0","xian i think i see what you meant i updated again hopefully someone can help me to understand what i am missing or need to do thank you very much for your time","comment"
"266074","","573163.0","i deleted some very misleading comments note that you have enough information to find all relevant means sds ses and any moments you choose thus you could compare the two samples very simply by quoting their means and standard errors or you could conduct a formal hypothesis test to compare their means have you reviewed our posts on tests of proportions https statsstackexchangecom searchqbinomial+test+of+proportion","comment"
"167521","","317836.0","sorry cant chat now have to sleep","comment"
"272404","","521183.0","at the beginning i just wanted to put forward a pure math problem and didnt indicate the engineering backgroud behind it which results in something important missing i am really sorry if viewers have other confusions please tell me","comment"
"245196","245188.0","","to the best of my knowledge there is no opposition between bayesian models bm and hierarchical bayesian models hbm see eg relation between bayesian analysis and bayesian hierarchical analysis and the fact is that analytically hbms are bms hierarchical models simply allow you to design a convoled prior structures that is more likely to represents eg interactions between variables of your model and thus to provide more suited inference then you should use hierarchical model at the instant hyperparameters appear naturally in the modeling of your problem a simple example is when you need to account for individual level and group level variation for exampley_ ij sim n mu_j sigma2_j mbox individual level variation mu_j sim gamma k_ mu theta_ mu mbox group level variation with k and theta and sigma2_j if unknown assigned to well chosen priors","post"
"198124","","375557.0","thanks roland i have added select but im even more unsure of how to interpret this it result in a model with fit statistics almost the same slightly worse in terms of r2 aic etc but the pvalue for the s x term is now much lower so if the parameter is not getting shrunk what is changing","comment"
"29242","29239.0","","i actually often run into that problem my two favorite ways to generate a time series with autocorrelation in r depend on if i want a stationary process or notfor a non stationary time series i use a brownian motion for example for a length 1000 i dox diffinv rnorm 999 for a stationary time series i filter a gaussian noise for example this looks likex filter rnorm 1000 filterrep 1 3 circulartrue in that case the autocorrelation at lag tau is 0 if tau 2 in other cases we have to compute the correlation between sums of variables for example for tau 1 the covariance is cov x_1 x_2 cov y_1+y_2+y_3 y_2+y_3+y_4 var y_2 + var y_3 2 so you see that the autocovariance drops down linearly up until n where n is the length of the filteryou can also want to do long memory time series like fractional brownian motion but this is more involved i have an r implementention of the daviesharte method that i can send you if you wish","post"
"94594","","184703.0","appealing to etymology can give one useful handles for remembering what technical terms mean it works well for me but using etymology to justify them is to be avoided quite a few terms in statistics and elsewhere are properly understood only through careful study of their mathematical definitions understanding this answer requires a clear conception of the intended uses of words and phrases like determined by set externally changes to external forces and partly a function none of which are immediately apparent or unambiguous","comment"
"252560","","480913.0","is this machine learning to allow imputation or is it descriptive analysis","comment"
"257888","","492871.0","its not clear to me what you can do with f so you may want to give more background on f you state i can evaluate the unnormalized target distribution for a given theta this is what is necessary to perform most mcmc so yes mcmc seems like the way to go although it is never the only solution","comment"
"14315","","25521.0","that would be good to include in your original question itll get harder to answer things in the comments","comment"
"288586","","555867.0","those both are superb coding solutions ive been working on applying them as you said to expect were struggling with convergence etc we might be asking too much of our data it sure pays to ask where the top experts reply thank youas for prior reference to crashing wrong verb on my part all that crashed was my forehead into my palm when r repeatedly told me i was still mixed up","comment"
"171153","","323695.0","im using the stepwise method","comment"
"213171","","404783.0","yes thats correct","comment"
"290523","","","im trying to determine the probability that the prediction is true that warriors wont lose consecutive games during an 82 game season assuming that warriors have an 80 chance of winning every game i was trying to get an approximation using binomial probability ncrpx 1p nx but i dont know how to model consecutive lose trials and im not sure if this is the best approach any help would be appreciated thanks","post"
"7516","","12120.0","+1 nelsen is quite readable i bought a copy a few years ago even after going through a lot of the online materials","comment"
"279775","","536131.0","yes you may add the iv and its squared value if it is supported theoretically and diagnostic shows that it is needed","comment"
"313526","","","when learning about confidence intervals we are told that we must not talk about the probability that the true value lies within the interval because frequentist probability is the limit of frequencies in a mostlyfictional population of experiments and has nothing to say about whether a fixed value lies in a fixed interval except perhaps to unhelpfully say that it is either 0 or 1 nitpicky note i am aware that if we consider the confidence interval to be a random variable then a useful probability statement can be made about it but whenever i calculate one from realworld data i seem to get a fixed interval and not a random variable dbut1 in every 100 million people have the brainexploding flu a man is tested for bef with a test that is 99 accurate in both directions and the test results come up positive the man asks his doctor if he really has brainexploding flui feel like the doctor should be able to say almost certainly not just eyeballing it the probability is about 1 in a million here or is it doesnt the same objection apply this is also a single event and whether the man has bef is an unknown but fixed value","post"
"124452","","237270.0","perhaps start here http enwikipediaorg wiki order_statistic#probability_distributions_of_order_statistics a number of questions here deal with uniform order statistics","comment"
"272870","","656386.0","wrt to the seminal dropout paper yes dropout is an awesome tool for regularizing nnets but its not an allornothing proposition the amount of dropout regularization both rate and layer placement matters and again theres no theoretical basis to say that a given dropout rate for a given architecture will be sufficient to prevent overfitting typically it has to be tuned experimentally to find the sweet spot between under and overfitting ability to understand and moreover diagnose experimentally the tradeoff is fundamental to successful use of nnets","comment"
"95624","","186397.0","andyw i linked to the paper","comment"
"312063","","592714.0","it is logistic regression my friend i run logistic regression","comment"
"187742","187726.0","","the median over all organizations would be the first thingconsider a thought experiment with only two regions one region has a single organization and the other all the organizations except that one lets say theres ten thousand of themnow here the median of region a is 60 and the median of region b is 100 if you think the correct thing to calculate is 80 which lies at the 235 percentile of the complete set of organizations ie that the thing you want should really give as much weight to one organization as it does to the other ten thousand if the organizations in region b are counted once then the one for the smaller region is effectively counted ten thousand times then what you seek is something other than the median over all organizations one question you might like to consider is what quantity is the second thing you mention an estimate of note that if you wanted an overall mean whats the mean over all organizations then youd certainly want to give more weight to larger regions precisely because they have more organizations why would the same consideration not apply to the median","post"
"408416","","","how to impute the missing values by woe","title"
"80969","","158752.0","could you say more about what the actual dependent variable is here","comment"
"319389","319375.0","","i wouldnt overemphasize normal distributions as an ideal here especially given your outliers i used bootstrapping in stata to get confidence intervals for your mean the intervals all include zero whichever way you do it output is slightly edited set seed 2803 bootstrap r mean reps 10000 nodots su profit_loss meanonlybootstrap results number of obs 274 replications 10 000 command summarize profit_loss meanonly _bs_1 r mean observed bootstrap normalbased coef std err z p z 95 conf interval + _bs_1 1852161 1240647 149 0135 5794623 4283783 estat bootstrap allbootstrap results number of obs 274 replications 10000 command summarize profit_loss meanonly _bs_1 r mean observed bootstrap coef bias std err 95 conf interval + _bs_1 18521605 7320308 12406467 5794623 4283783 n 8130128 3995401 p 1038894 3853036 bc n normal confidence interval p percentile confidence interval bc biascorrected confidence interval","post"
"20815","","37638.0","thank you chl montgomerys was on my shopping list but i chosed not to buy it since it was more geared towards engineering than ecology i have noticed a new edition is due to be published on april 2012 will you update your r companion to it","comment"
"286950","","548795.0","whuber i have so far been unsuccessful to turn your help into a solution in that vein does mathematica see wolfies answer below overlook some way to produce a closedform result","comment"
"433220","","808117.0","there is no elbow in this plot clearly the clustering failed to produce good clusters for any k","comment"
"31512","","61281.0","the usa has loads of projections you can get good advice about which one s to use and how to carry out those projections at http gisstackexchangecom","comment"
"233588","","443748.0","it looks like your questions are extensively answered at http statsstackexchangecom questions 26450 could you identify and highlight any that are not","comment"
"116061","","222417.0","this doesnt answer you question but as far as i know the division by n1 rather than by n in the calculation of sample variance is to make the estimator unbiased it doesnt have to do with degrees of freedom","comment"
"161354","","","i want to include two predictors total brain volume and corrected gray matter volume into one regression model in order to predict the level of cognition dependent variable however this corrected gray matter volume is calculated asgray matter volume total brain volume corrected gray matter volumemaking the regression model look like thistotal brain volume + gray matter volume total brain volume cognitionis this statistically sounds or are there issues with total brain volume essentially featuring twice in one model","post"
"397207","","766774.0","related question looking for good references on neural networks math for learning algorithms https statsstackexchangecom questions 243961","comment"
"337393","","","the demand curve p_i b_1 +b_2 q_i +b_3p_ sub i +b_4p_ com i +b_5w_i+u_ i1 the supply curvep_i a_1 +a_2 q_i +a_3p_ sub i +a_4p_ com i +a_5w_i+u_ i1 which instruments would you use to instrument q_i in each of the two equations and which restriction should i apply in order to estimate first one as demand and second one as supply curvep_ sub i is the price of the substitute of the good ip_ com i is the price of the complement of the good iw_i the wagep the price of the good iq is the quantity of good i my idea is that maybe it is wrong i am not sure in the demand equation wage is the instrument to q in the supply curve p_ com is the instrument to q what do you think","post"
"307449","","584473.0","just to state the obvious because i am uncertain if you appreciate it the 95 conf interval you report is a statement about the percentage of conf intervals that contain the true parameter value say 10 it is not that 95 of the future estimates of this experiment will fall in this 8 12 range a conf interval refers to future conf intervals not point estimates","comment"
"333326","","631211.0","okay yeah got it ill add an edit to the answer to reflect this","comment"
"102731","","198770.0","you have definitely answered my question thank you","comment"
"96602","","188268.0","i am not doing phd but i will figure out how to find the book through or not through springerlink","comment"
"19731","","","r process data frame subgroups and merge results together","title"
"154316","","","using dirac delta functions for estimating a probability distribution","title"
"379633","379629.0","","there is no need to be concerned about either the different sample sizes or any possible difference in variance what i would suggest though is that since your measures are concentrations which have a meaning in themselves you should report the difference between the means with its confidence interval and not convert this to one of the measures you suggest like d or g these are only needed when the measures do not have any obvious meaning or scaleif the package you are using does not work on the original scale then you need the means bar x_a and bar x_b and their standard deviations s_a and s_b and sample sizes from which you can work out the degrees of freedom by subtracting 1 then you need the pooled standard deviations sqrt frac df_a s_a2 + df_b s_b2 df_a + df_b hence the standard error of the differencese s sqrt frac 1 n_a + frac 1 n_b and finally the confidence interval formed by multiplying the standard error by the appropriate value of students t for df_a + df_b degrees of freedom and taking that on either side of the mean difference difference between the means in your case since you have large samples you could just use the value of the normal deviate z or 196 for a 95 interval","post"
"198258","198246.0","","a model relates your observations and unknown parameters we are interested in in the case of image processing the observation is most of the time the acquired image and the unknown parameters can be eg segmentation labels image for a segmentation task true intensity image for a denoising task your question is about how to express p f p f_i i indexing pixel the prior probability on the unknown eg segmentation mask the unknown parameter of interest in fact in image processing prior information p f is crucial and consists mainly in introducing dependencies between the values of the unknown parameter of neighbour pixels as an example in segmentation task neighbour pixels can be more likely to have the same label segmentation mask with large regions of a given label are more likely than patchy mask with many isolated different labels this is typically done by introducing a markov random field structure to the parameter of interest practically it consists in introducing the pixelwise prior probabilities of adjacent pixelsp f_i f_j _ j in v i where v i are the neighboor pixels of pixel i that will be used to define completelty p f p f_i i suggest you to look for a dedicated lecture eg http wwwinfuszegedhu ssip 2008 presentations2 kato_ssip2008pdf for more detailsanother example of prior could consists in having some shape prior eg a region with label 1 looks like a circle a region with label 2 looks like a dog eg http wwweceucsbedu manj manjbio2008 07_vu_cvpr2008pdf","post"
"155388","","296262.0","the method isnt very robust but something similar can be robustified the basic idea is local regression and common somewhat robust version of it for this is variously called loess or lowess see wikipedia on local regression http enwikipediaorg wiki local_regression there are a number of related ideas but thats where id start","comment"
"110122","","211517.0","just an observation on your particular plot but the y values appear to fixed at integer intervals if the y values are known to be accurate you only need regression on the x values or to look at it another way as in glen_bs bins example you already have bins im no expert on this stuff though","comment"
"415668","","775697.0","hi both a few parameters are estimated yes id share the simulation code but its pretty intensive its a large scale simulation of stellar cluster formation so goes through the entire birth death process of stars etc in order to produce mass distributions i was hoping for some intuition on what this pvalue skew might mean in general it seems incorrect to say it fit better than expected","comment"
"16174","","29069.0","alix axel considering that atm they even answer stuff like this http mathstackexchangecom questions 67801 whatdoestheexclamationmarkdo id at least try it","comment"
"280242","","537145.0","whuber my bad my original question isnt very clear im intending to model a 2d game that reflects a broadcast game since the broadcast cameras arent fixed still for different frames the reference lines arent same they are moving so far im only considering model a very short clip so that we eliminate the possibility of camera video on audience etc if all the reference points are moving does this make the all project impossible to do","comment"
"112864","","","i have a kind of data and want to find the equation poly coeff of given data for example equation for given sample data is simple a2b+10 ab 5 10 15________________________3 55 100 1454 90 170 2505 135 260 3856 190 370 550what is the correct algorithms","post"
"61524","","118039.0","yes i have several locations and i performing the two models for each of those location and the number of values whic correpond to the number of measurement of rainfall and productivity differ from a location to another onewhat kind of plant growth model do you have in mind when you talk about model with qualitatively correct limiting behaviourin my previous comment who was edited by the moderator i said that we expected a sshape curve","comment"
"91114","","","distribution of the convolution of squared normal and chisquared variables","title"
"6657","","10612.0","see eg http mathfurmanedu tlewis math110 maki chap8 sec4handoutpdf for a quick discussion of calculating absorption probabilities","comment"
"231741","","","id like to compute pvalues for simple pearson correlations when using bootstrappingis there any opportunity to do this im using spss and the only thing you can do within this software is to compute the cis can i just take the pvalues that a simple linear regression will give me spss can do this or is this incorrectthanks for your answerid like to have the pvalue as a bootstrapped estimate just like in the linear regression the normal pvalue gets not bootstrapped as i know even if the bootstrapped icon is selectedi need bootstrapping because the assumptions arent that clealry fulfilled in my dataso it is not an option to use the bootstrapped pvalue that spss gives me for a simple linear regressionthanks","post"
"95227","","185777.0","things that arise in nature are normally studied things studied are often on wikipedia anyway since i dont have a way to decide what the distribution is all i can do is try to fit data to many distributions and wiki seems to have a comprehensive list as this is for an engineering project it does not matter if i dont have the exact distribution only if the distribution causes results which are expected two distributions could both be suitable","comment"
"253880","","","using blocking in propensity score analysis","title"
"11035","","19052.0","the package arm has the function invlogit which is your function logit2prop","comment"
"48025","","","citation for continuous space hill climbing algorithm pseudocode on wikipedia","title"
"199942","","381766.0","make a 2d nonparametric density on top of your right plot there are clean multiple modes","comment"
"20209","","36499.0","it could be a model for what the distribution of bullets that actually fall on the disk look like","comment"
"327766","","621205.0","how certain as compared to what assuming constant + noise means assuming that it is a random variable with some mean","comment"
"225157","","","i have n small floating point vectors of length k typically n is in the millions and k9 i need to compute a lot millions and millions of squared euclidean distances between those vectors it would be great if i could reduce this 9vector elements to say length 3 or 4 i was thinking about pca i could precompute and store the reduced vectors and then proceed with the distances computation could this work for instances each of those vectors is a vectorized 3x3 patch around given pixel of a natural image a patch is created for every pixelin your opinion could this work","post"
"384918","","722724.0","could you explain what you understand standardization which is the correct term here of a variable to be accomplishing in most applications its purpose is to create a variable with zero mean and unit variancebut that will happen no matter what the underlying distribution is assuming it has finite variance so obviously this is unrelated to normality what then are the additional properties you hope to achieve","comment"
"52434","","103172.0","im not sure you offer a useful explanation how to run the model with glmfit","comment"
"231412","","439142.0","im not a sem expert but as i understand them it takes a lot of thinking and justification to use them in a principled way i cant immediately think of another technique that is less compatible with automated methods of adjustment selection than sems","comment"
"333241","332745.0","","this is a complicated issue that introduces many related issues of 1 clearly specifying a hypothesis 2 understanding what causal mechanisms may underlie a hypothesized effect and 3 choice style of presentation youre right that if we apply sound statistical practice to claim that groups are similar one would have to perform a test of equivalence however tests of equivalence suffer the same issues as their nhst counterpart the power is merely a reflection of the sample size and the number of comparisons we expect differences but their extent and effect on a main analysis is far more important when confronted by these situations baseline comparisons are almost always redherrings better methods of science and statistics can be applied i have a few stock concepts responses that i consider when answering questions like this a total column is more important than splitbytreatment columns a discussion is warranted of those valuesin clinical trials the safety sample is usually analyzed this is the subset of those who were first approached then consented then randomized and finally exposed to at least one iteration of control or treatment in that process we face varying degrees of participation bias probably the most important and omitted aspect of these studies is presenting table 1 results in aggregate this achieves the most important purpose of a table 1 demonstrating to other investigators how generalizable the study sample is to the broader population in which the results apply i find it surprising how fixated investigators readers and reviewers are on the tangential trends within patient characteristics when there is a complete disregard to the inclusion exclusion criteria and the generalizability of the sample im ashamed to say i was an analyst on a trial that overlooked this as an issue we recruited patients and then due to logistical issues we waited nearly a year before implementing the intervention not only did the consort diagram show a huge drop between those periods but the sample shifted the result was largely un underemployed older and healthier than the people we intended to reach i had deep concerns about the generalizability of the study but it was difficult to lobby for those concerns to be made knownthe power and typei error of tests to detect imbalance in baseline characteristics depends on the actual number of characteristicsthe point of presenting such a detailed listing of baseline variables as mentioned previously is to give a thorough snapshot of the sample their patient history labs medications and demographics these are all aspects that clinicians use to recommend treatment to patients they are all believed to predict the outcome but the number of such factors is staggering as many as 30 different variables can be compared the crude risk of type i error is 1 1005 30 079 bonferroni or permutation corrections are advisable if testing must be performedstatistical testing in its purest form is meant to be impartial and it is supposed to be prespecified however the choice and presentation of baseline characteristics is often relative i feel the latter approach is appropriate if we find like in my trial there are interesting traits that describe the sample effectively we should have the liberty to choose to present those values ad hoc testing can be performed if it is of any value but the usual caveats apply they are not hypotheses of interest there is a high risk of confusion as to what significant and nonsignificant results imply and the results are more a reflection of sample size and presentation considerations than of any truthrerandomization can be done but only before patients are exposed to treatmentas i mentioned the analyzed sample is typically the safety sample however rerandomization is a heavily advocated and theoretically consistent approach to patients who have not been exposed to study treatment this only applies to settings in which batch enrollment is performed here 100 participants are recruited and randomized if for instance probability assigns a high proportion of older people to one group then the sample can be rerandomized to balance age this cant be done with sequential or staggered enrollment which is the setting in which most trials are conducted this is because timing of enrollment tends to predict patient status by prevalent case bias confusing incident and prevalent eligibility criteria balanced design is not a requirement for valid inferencethe randomization assumption says that theoretically all participants will have on average equal distributions of covariates however as mentioned earlier when comparing 30 or more levels the cumulative probability of imbalance is nonnegligible in fact imbalance of covariates may be irrelevant when considering the whole if the randomization is fair we may see age is elevated in the treatment group but smoking is elevated in the control group both of which contribute individually to the risk of the outcome what is needed for efficient and valid inference is that the propensity score is balanced between groups this is a much weaker condition unfortunately propensity cannot be inspected for balance without a risk model however its easy to see that such propensity depends on a combination of covariates and the likelihood of an imbalance in propensities in a randomized sample is far less probable despite being impossible to show exactlyif a risk model is known or strong predictors of the outcome are present more efficient and valid rcts are done by simply adjusting for those factors regardless of whether theyre balanced between treatment groupsone of my favorite papers 7 myths of randomized controlled trials discusses this adjustment improves efficiency when the adjustment variable is strongly predictive of the outcome it turns out that even with perfect 50 50 balance using say blocked randomization or even as a coincidence of how randomization was performed the adjustment will shrink cis requiring fewer participants to have an equally powered study this reduces costs and risks it is shocking that this isnt done more often observational studies require control for confounding regardless of what table 1 showsthe randomization assumption eliminates confounding with nonrandomized treatment there is confounding a confounder is a variable which is causal of the outcome and predicts receipt of the quasiexperimental treatment there is no test to determine which variable s is are confounders the risk of peeking into the data to answer these questions is that confounders are virtually indistinguishable from mediators or colliders without utterly perfect measurement of longitudinal values and even then adjusting for mediators attenuates any effect collideradjustment can cause any type of bias further one need not adjust for a total set of confounders but rather they must remove the backdoor criterion for instance in a study of lung function and smoking in adolescents older kids are more likely to smoke but since they are taller their lung function is greater it turns out the adjusting for height alone suffices to remove confounding since it satisfies the backdoor criterion further adjustment for age simply loses efficiency however merely inspecting the balance of a table 1 in smokers and nonsmokers would suggest that both age and height are imbalanced and thus should be controlled for that is incorrect","post"
"124915","124538.0","","hmm after i done an example in my matmatelanguage i see that there is already a pythonanswer which might be preferable because python is widely used but because you had still questions i show you my approach using the matmatematrixlanguage perhaps it is more selfcommentingmethod 1 using matmate v12 12 variablesf3 subsetcorrelation based on 3 common factorsvg v f variables per subsets generate hidden factormatrix randomu rows cols lowbound ubound gives uniform random matrix without explicite bounds the default is randomu rows cols 0 100 l randomu vg f randomu vg f 100 randomu vg f 100 _ randomu vg f 100 randomu vg f randomu vg f 100 _ randomu vg f 100 randomu vg f 100 randomu vg f make sure there is itemspecific variance by appending a diagonalmatrix with random positive entriesl l mkdiag randomu v 1 10 20 make covariance and correlation matrixcov l l multiplied with its transposecor covtocorr cov set ccdezweite3 ccfeldweite8 list corcor 1000 0321 0919 0489 0025 0019 0019 0030 0025 0017 0014 0014 0321 1000 0540 0923 0016 0015 0012 0030 0033 0016 0012 0015 0919 0540 1000 0679 0018 0014 0012 0029 0028 0014 0012 0012 0489 0923 0679 1000 0025 0022 0020 0040 0031 0014 0011 0014 0025 0016 0018 0025 1000 0815 0909 0758 0038 0012 0018 0014 0019 0015 0014 0022 0815 1000 0943 0884 0035 0012 0014 0012 0019 0012 0012 0020 0909 0943 1000 0831 0036 0013 0015 0010 0030 0030 0029 0040 0758 0884 0831 1000 0041 0017 0022 0020 0025 0033 0028 0031 0038 0035 0036 0041 1000 0831 0868 0780 0017 0016 0014 0014 0012 0012 0013 0017 0831 1000 0876 0848 0014 0012 0012 0011 0018 0014 0015 0022 0868 0876 1000 0904 0014 0015 0012 0014 0014 0012 0010 0020 0780 0848 0904 1000the problem here might be that we define blocks of submatrices which have high correlations within with little correlation between and this is not programmatically but by the constant concatenationexpressions maybe this approach could be modeled more elegantly in pythonmethod 2 a after that there is a completely different approach where we fill the possible remaining covariance by random amounts of 100 percent into a factorloadingsmatrix this is done in pari gp l matrix 8 8 generate an empty factorloadingsmatrixfor r1 8 rv10 remaining variance for variable is 10 for c1 8 pvif c8 random 100 1000 10 define randomly part of remaining variance cv pv rv compute current partial variance rv rv cv compute the now remaining variance sg 1 random 100 2 also introduce randomly + signs l r c sgsqrt cv compute factor loading as signed sqrt of cv cor l land the produced correlationmatrix is 1000 07111 008648 07806 08394 07674 06812 02765 07111 1000 006073 07485 07550 08052 08273 005863 008648 006073 1000 05146 01614 01459 04760 001800 07806 07485 05146 1000 08274 07644 09373 006388 08394 07550 01614 08274 1000 05823 08065 01929 07674 08052 01459 07644 05823 1000 07261 04822 06812 08273 04760 09373 08065 07261 1000 01526 02765 005863 001800 006388 01929 04822 01526 1000possibly this generates a correlationmatrix with dominant principal components because of the cumulative generatingrule for the factorloadingsmatrix also it might be better to assure positive definiteness by making the last portion of variance a unique factor i left it in the program to keep the focus on the general principle a 100x100 correlationmatrix had the following frequencies of correlations rounded to 1 dec place e f e entry rounded f frequency 1000 108000 0900 460000 0800 582000 0700 604000 0600 548000 0500 540000 0400 506000 0300 482000 0200 488000 0100 464000 0000 434000 0100 486000 0200 454000 0300 468000 0400 462000 0500 618000 0600 556000 0700 586000 0800 536000 0900 420000 1000 198000 update hmm the 100x100 matrix is badly conditioned pari gp cannot determine the eigenvalues correctly with the polroots charpoly function even with 200 digits precision ive done a jacobirotation to pcaform on the loadingsmatrix l and find mostly extremely small eigenvalues printed them in logarithms to base 10 which give roughly the position of the decimal point read from left to right and then row by rowlog_10 eigenvalues 1684 1444 1029 0818 0455 0241 0117 0423 0664 1040 1647 1799 1959 2298 2729 3059 3497 3833 4014 4467 4992 5396 5511 6366 6615 6834 7535 8138 8263 8766 9082 9482 9940 10167 10566 11110 11434 11788 12079 12722 13122 13322 13444 13933 14390 14614 15070 15334 15904 16278 16396 16708 17022 17746 18090 18358 18617 18903 19186 19476 19661 19764 20342 20648 20805 20922 21394 21740 21991 22291 22792 23184 23680 24100 24222 24631 24979 25161 25282 26211 27181 27626 27861 28054 28266 28369 29074 29329 29539 29689 30216 30784 31269 31760 32218 32446 32785 33003 33448 34318 update 2 method 2 b an improvement might be to increase the itemspecific variance to some nonmarginal level and reduce to a reasonably smaller number of common factors for instance integersquareroot of itemnumber dimr 100 dimc sqrtint dimr 10 common factors l matrix dimr dimr+dimc loadings matrix with dimr itemspecific and dimc common factors for r1 dim vr10 complete variance per item vu005+random 100 10000 random variance +005 for itemspecific variance l r r sqrt vu itemspecific factor loading vrvrvu for c1 dimc cvif cdimc random 100 100 10 vr vrvrcv l r dimr+c 1 random 100 2 sqrt cv covll cpcharpoly cov does not work even with 200 digits precision prpolroots cp spurious negative and complex eigenvaluesthe structure of the result in term of the distribution of correlations remains similar also the nasty non decomposability by parigp but the eigenvalues when found by jacobirotation of the loadingsmatrix have now a better structure for a newly computed example i got the eigenvalues as log_10 eigenvalues 1677 1326 1063 0754 0415 0116 0262 0516 0587 0783 0835 0844 0851 0854 0858 0862 0862 0868 0872 0873 0878 0882 0884 0890 0895 0896 0896 0898 0902 0904 0904 0909 0911 0914 0920 0923 0925 0927 0931 0935 0939 0939 0943 0948 0951 0955 0956 0960 0967 0969 0973 0981 0986 0989 0997 1003 1005 1011 1014 1019 1022 1024 1031 1038 1040 1048 1051 1061 1064 1068 1070 1074 1092 1092 1108 1113 1120 1134 1139 1147 1150 1155 1158 1166 1171 1175 1184 1184 1192 1196 1200 1220 1237 1245 1252 1262 1269 1282 1287 1290","post"
"125238","","238678.0","thanks a lot that works gamma was there from my another tests i will update the code","comment"
"245155","","","we know that the random variable z sum_i x_i formed by summing iid x_i sim n mu sigma2 with itself n times is z sim n nmu nsigma2 given this knowledge is it possible to take m samples of x by taking m samples of z then dividing the samples by n that is suppose we have m samples z_1 z_2 dots z_m of z is frac z_1 n dots frac z_m n a set of m samples from x sim n mu sigma2","post"
"286357","","","there are two different type of games pc and console and each game is played on two different screen sizes 9 inches and 27 inches 50 subjects are recruited to play the pc game on both screen sizes one will start on the small screen size for 15 min and then play on the bigger screen size for 15 min and second subject will start on the big screen and then shift onto the small screen and so onanother 50 subjects are recruited to play the console game first on the big screen and then on the small screen and so onthe dependent variable is funthe 100 participants who played the pc and the console games are screened into two groups of expertise novice and expert the researcher wants to examine if both novice and expert players will derive the same amount of fun dependent var below is my rationale kindly advise if i am in the right directionit is 3way mixed anova design with 3 independent var expertise inbetween subject tests with two levels novice and expert and type of games inbetween subject tests with two levels pc and console and screen size withinsubject tests with two levels 9 and 17 inches i plan to enter the data into spss as shown below i i can see the design as a manova with two independent var groups and game types and two dependent var fun score for 9 and 27 screen sizes please advise if this logic is correct ii alternatively i am not sure if the design can be considered a 3way mixed anova with 2 inbetween subject test groups 2 levels and game types 2 levels and withinsubject test screen sizes 2 levels however the dependent var data fun score for each screen size are displayed in the two columns 9 and 27 screen sizes iii a third thought it looks like a multivariate regression with two dependent var fun score for 9 and 27 and two independent var the issue is i have data for only novice and expert following the use of a screening questionnairekindly advise which of the above options are most appropriatefigure 1","post"
"387141","387069.0","","there are several possible reasons which could be responsible for the scenario you describe1 collinearity among some or all of your predictor variablesdid you check if some of your predictor variables are engaged in collinearity that might be one possible explanation you can use the vif function in the car package to check for collinearity if you find any predictor variables which have high vif variance inflation factor values say larger than 5 you may need to exclude some of those from your model and see if that resolves your collinearity 2 too many predictor variables in your model relative to the number of eventshow many observations do you have in your model and how many events an event corresponds to y 1 where y is your response variable there are rules of thumb for how many events you should have per predictor variable included in your model eg 10 events per variable which you can use to determine if your model includes too many predictor variables relative to the available number of events if it does you will need to include fewer predictor variables in your model 3 too few observations in your modelperhaps the predictor variables included in your model have significant effects on the log odds of success ie achieving a value of 1 for y but you just dont have enough observations in your model to detect these effects to see if this might be the case look at the confidence intervals for each predictor in relation to zero and see i how wide the confidence intervals are and ii how far the centers of the confidence intervals are from 0 use the confint function to extract the 95 confidence intervals from your model and the coef function to extract the centers of these intervals if the centers of the intervals are not too close to zero and the intervals are wide that would suggest you need more data in your study to be able to detect the effects of interest recall that r works on the log odds scale by default there may be other reasons as well perhaps others on this forum can highlight them","post"
"191270","","363334.0","it looks like you have around 1000 variables per plant timepoint condition so what you want to do is to reduce the number of variables and i am not sure why you call it observations also each measured spectrum is one observation so for many plants and treatments you have many observations does this describe your situation correctly","comment"
"431783","","805558.0","you have been misled by the language in the question correlation is used in a colloquial sense to mean relationship or association please do not take that to mean you must compute or analyze any kind of correlation coefficient indeed thats going to be a dead end for these data you need to learn about and apply nonlinear regression models","comment"
"2186","","129607.0","can you somehow comment on the exercises of these books i want to do some exercises from a graduate level mathematicsbiased text book to enhance thank you","comment"
"115052","","525017.0","alexis i was suggesting that using a nonparametric predictive model such as those based on trees should in principle not have the shortcomings you described from binary coding what do you think","comment"
"298283","","566931.0","thats precisely what the binomial theorem implies its easy to verify since 1m_0 1 o 1 1+m_0 1 o 1 1+o 1 2cong 1 operatorname mod o 1 the expression i gave for the inverse really works","comment"
"787","","687.0","hillaire belloc nice work on digging that up","comment"
"45807","","88799.0","marianosurezalvarez can this be moved to statsstackexchange i did statistics undergraduate i know some of the actual issues here but i would prefer this asked of someone who works on it full time or not answered in a more authoritative manner","comment"
"91279","","178839.0","many thanks for your comment even if incomplete this is clearly useful i was also unsure how to handle the rotations maybe youve seen it already but these are demonstrated in a 2x2 matrix in the thread i linked too in the original post https statsstackexchangecom questions 9898 howtoplotanellipsefromeigenvaluesandeigenvectorsinr although in a different example that i quoted i will take a look at implementing what you suggest today and seeing if i can resolve this remaining question thanks again","comment"
"354475","","","i am currently learning r and i am relatively inexperienced in the field hope i can get some advice from you guys i am working on a project where i have to estimate the average processing time of different work items tasks i have the following panel datamy sample size is n2000 individual workers and t10 each time interval is a four week period independent variables 51 different work items i have count data for each work item # of times they are performed by each worker over a four week period dependent variable total working hour of the worker over a 4 week period the goal of my analysis is to find the regression coefficents which are estimtes of the average completion time of each work item i may also include other regressors other than #of work items such as experience age into my modely bo + b1x1 ++bkxk + e y total working hoursx # of each work items type issuesright now i finished cleaning and processing the data and i performed some exploratory data analysissome work items have a lot of zeros the work item is only performed once or twice by several workers in the time period from vif i can see that there are imperfect multicollinearity in the independent variables some independent variables have vif of 5 to 6questionsany advice on how i should specify my model i look at boxplots and eliminate outliers of each regressor i see that some regressors are highly skewed due to lots of zros i also plot each regressors against the total compltion time to see if there is any linear relation so do other looks more like a quadratic relationany way to deal with the multicollinearity aside from eliminating the regressors that have high vif this is because i need to estimate the coefficent of each of the work itemshould i set the intercept to 0 i know for sure that when all the regressors are 0 # of work items are all 0 i should have zero total working hours i would also welcome any other advice for this problem thanks","post"
"234119","","444879.0","what question would you hope to answer with all 5 timepoints at the same time","comment"
"330844","","626788.0","aksakal but in the blog with visualization clearly length and distance are two different quantities","comment"
"72568","","","one proves mathematically that if assumptions of a model are satisfied then the coverage rate of a 100p confidence interval is 100p but then statistics gets applied to the world where model assumptions may not be satisfied are there any studies comparing the coverage rates of confidence intervals applied to the real world with theoretical coverage rates","post"
"100316","","194279.0","not at all the op observes that the inverse does not exist when x is 1times k why n and this is right he misses that __in the linked post__ the row vector is x not x let x be a row vector containing the values of the predictors for the forecasts etc please notice that x xx 1 x _is not defined_ if the row vector is of dimension n because xx 1 is of dimension ktimes k the op is about a row vector of dimension k not n the op is about the _variance of the forecast_ can you see what youve missed","comment"
"146420","","280711.0","thanks for your answer i made a mistake with my definition of the chi squared distribution which is my answer makes practically no sense im aware of these properties and i was using them in my answer just in dragged out steps and a wrong understanding of x_n the trick for expectation of chi_12 is useful to know as well and im sure i can work out how to derive variance with a little bit of googling thanks glen_b","comment"
"17354","","31108.0","im assuming that the dependent variable in each condition is numeric and is measured the same way in each condition","comment"
"1762","","1921.0","sorry if this is a dumb question but what do the lighting conditions have to do with chronograph accuracy","comment"
"229449","","434157.0","matt it comes down to whether lifetime means for women who have reached menopause or for all women even teenagers the figures in this question are for the latter but the simulation needs the former +1 for a stimulating question btw","comment"
"208939","","398174.0","there is nothing wrong with the above model mean and precision are a priori independent this is one possible choice for a prior","comment"
"15841","","28661.0","okay to phrase the question slightly differently are you interested only in testing consistency of means or are you also interested in consistency of other distributional properties such as variance or skew","comment"
"51634","","140692.0","this code looks very interesting would it be possible for you to post your imput dataset somewhere so i could play with it i cant quite visualize what you are doing with all of your which melt statements","comment"
"59165","","113468.0","this blog post may be of some interest http blogphilbirnbaumcom 2006 08 findingtruetalentlevelforoutcomehtml","comment"
"156725","","298627.0","that said thanks for the link after reading it i think i have an idea of what might be going on hmm","comment"
"361975","","680074.0","but this would depend on properties of your data","comment"
"121651","","232084.0","the plot shows the model you fit to your data all the results are a consequence your two histograms correspond to the ranges of levels of the curve in the two regions to the left class 2 and right class 1 of 5 the curve must pass very close to the point 0 1 2 because you split a large set of data averaging 0 into the two classes the curve has to flow continuously in a decreasing manner from left to right these qualitative characteristics are easily predictable before ever fitting the modelyou dont need any software to draw the curve","comment"
"69045","","133376.0","glen_b thought it was easier to compare between the two datasets and recognize geometric shapes all of which are readily visible in the scatterplots","comment"
"120953","","","lets say you have two columns of data column a represents a value you had last month and column b represents a value you have this month if you want the percent change between the two you math it new old oldor in many languages and tools preventing a division by zero errorif old 0 then 0 else new old oldhowever if you had zero last month and something this month thats good news depending on what the story is so outputting zero may not be the best thing to show is there a best practice for visualizing a lift from zero in the context of a percent change in this situationzero seems wrong 100 seems wrong inf seems wrong n a seems wrong div0 seems wrong","post"
"384318","","721603.0","it is indeed the case that p x0","comment"
"219609","219579.0","","ponder the following story if you willi also remember sitting in a statistics course and the professor told us extrapolation was a bad idea then during the next class he told us it was a bad idea again in fact he said it twicei was sick for the rest of the semester but i was certain i couldnt have missed a lot of material because by the last week the guy must surely have been doing nothing but telling people again and again how extrapolation was a bad ideastrangely enough i didnt score very high on the exam","post"
"74040","","144128.0","so you are saying that the formula used is the correct one but the formula doesnt correct for the autocorrelation in x and y and i assume this autocorrelation will have an effect on the correlation coefficient","comment"
"182151","","349977.0","according to your latest edit youve got four setups per treatment and four measurements on each setup thats good news of course you can ignore some of my previous comments","comment"
"272597","","","how to calculate collective noise from individual standard errors","title"
"384535","","722109.0","no only transform if https statsstackexchangecom questions 18844 whenandwhyshouldyoutakethelogofadistributionofnumbers build and identify an arima model on the original series and incorporate deterministic structure as discussed here http docplayernet 12080848outlierslevelshiftsandvariancechangesintimeserieshtml","comment"
"211526","","401810.0","either case is possible you could get the same final model or they can be rather different","comment"
"108311","","208290.0","what is the purpose of the normalization","comment"
"322920","","","how to estimate the margins distribution using ecdf","title"
"372405","","700617.0","i just edited my answer to show some more detail","comment"
"14113","","24848.0","ok then its important to disclose the criteria you have used to identify this sequence for instance did it stand out among other sequences as having more variation in its frequency within the string it will also help to say something about how the string is generated because serial correlation among the characters can account for such variation","comment"
"64026","","","in this comment nick cox wrotebinning into classes is an ancient method while histograms can be useful modern statistical software makes it easy as well as advisable to fit distributions to the raw data binning just throws away detail that is crucial in determining which distributions are plausiblethe context of this comment suggests using qqplots as an alternative means to evaluate the fit the statement sounds very plausible but id like to know about a reliable reference supporting this statement is there some paper which does a more thorough investigation of this fact beyond a simple well this sounds obvious any actual systematic comparisons of results or the likesid also like to see how far this benefit of qqplots over histograms can be stretched to applications other than model fitting answers on this question agree that a qqplot just tells you that something is wrong i am thinking about using them as a tool to identify structure in observed data as compared to a null model and wonder whether there exist any established procedures to use qqplots or their underlying data to not only detect but also describe nonrandom structure in the observed data references which include this direction would therefore be particularly useful","post"
"66124","","127636.0","what i meant was that if you use proportional classified correctly as an accuracy measure that measure uses only 1 bit of information from the predictions so it is the lowest information content measure you can have other than no information at all this is reflected in low precision wide confidence interval for the accuracy score proper scoring rules are much preferred eg logarithmic probability score deviance or quadratic score brier","comment"
"384294","","721554.0","can we have more information please what is your model formula what is summary of your data set in particular is trial numeric or categorical factor","comment"
"222178","","","i am using the m5 model implemented in the rweka package for predicting a continues variable based on several independent ecological variablesmodel m5p t_apr datatrain i would like to use this to further build an ensemble model in r but im having difficulties finding a way how to do this therefore my question how to build an ensemble using m5 models in r","post"
"218727","218688.0","","you can use the lasso or elastic net regularisation both are available in glmnet if you are an r user with a poisson dependent variable using the familypoisson option hopefully you have sufficient observations to be able to split the dataset and do cross validationstepwise selection methods are generally best avoided particularly with 2000 variableslog y isnt a good idea if the data are poisson since you will be taking logs of zero of course you could use log y+1 but since glmnet supports the poisson distribution this doesnt seem necessary unless there are computational limitations","post"
"168084","","318831.0","what do the stars and crosses denote if simply multiplication id recommend leaving them out as is standard mathematical notation","comment"
"376270","","708097.0","it seems that the structural break is messing with my time series","comment"
"269433","","515767.0","yes i think the regression with panelid and year fixed effects needs to bextnbreg y itreat##ipost iyear fe robust","comment"
"5479","","","i am teaching myself dlms using rs dlm package and have two strange results i am modeling a time series using three combined elements a trend dlmmodpoly seasonality dlmmodtrig and moving seasonality dlmmodreg the first strange result is with the f onestepahead foreacast result most of this forecast appears to be one month behind the actual data which i believe ive seen in many examples of onestepahead forecasting online and in books the strange thing is that the moving seasonality is not similarly lagged but hits exactly where it should is this normalif i use the results m to manually assemble the componenet everything lines up perfectly so its weird though it makes sense in a way the moving seasonality has exogenous data to help it while the rest of the forecast does not still itd be nice to simply lag the resulting f and see a nice match more troubling is the difference i see if i change the degree of dlmmodpolys polynomial from 1 to 2 in an attempt to get a smoother level this introduces a huge spike in all three components at month 9 the spikes all basically cancel out in the composite but obviously make each piece say the level or the seasonality look rather ridiculous thereis this just one of those things that happens and i should be prepared to throw away the results first year of data as breakin or is it an indication that something is wrong even in the degree 1 polynomial case the first years moving seasonalitys level is a bit unsettled but no huge spike as when i use a degree 2 polynomial here is my r codelvl0 log mydata 1 slp0 mean diff log mydata buildptr2 function x pm dlmmodpoly order1 dvexp x 1 dwexp x 2 m0lvl0 tm dlmmodtrig s12 dvexp x 1 q2 dwexp x 34 rm dlmmodreg movingseason dvexp x 1 ptrm pm + tm + rm return ptrm mlptr2 dlmmle log mydata rep 1 6 buildptr2 dptr2 buildptr2 mlptr2par dptrf2 dlmfilter log mydata dptr2 tsdiag dptrf2 buildptr3 function x pm dlmmodpoly order2 dvexp x 1 dwc 0 exp x 2 m0c lvl0 slp0 tm dlmmodtrig s12 dvexp x 1 q2 dwexp x 34 rm dlmmodreg movingseason dvexp x 1 ptrm pm + tm + rm return ptrm mlptr3 dlmmle log mydata rep 1 8 buildptr3 dptr3 buildptr3 mlptr3par dptrf3 dlmfilter log mydata dptr3 per the followon question the data itself is monthly data for 10 years with each month being the weekly average attendance at a theatrical production the data definitely has seasonal and moving seasonal effects i want to model the trend and the seasonal effects to give the management some insight and to prepare for forecasting which is not directly possible with dlm when you include a dlmmodreg component though thats the next step i am trying to use an order2 polynomial component that i believe creates an irw trend which is supposed to be nicely smooth if it matters my moving seasonality is a yearly big bash gala event that can fall in two different months and i indicate it with 0 for most months and 1 for months in which the big bash falls","post"
"64760","","125230.0","often it is better to plot power curves rather than just computing a single value of power have the different effect sizes on the xaxis and the resulting power on the y you can include a few different curves representing different sample sizes these can show that while the initial conditions dont give the power desired a few small changes would or they may show that the experiment is not worth pursuing with the available resources","comment"
"306508","","582861.0","whuber nice exercise i think i have a correct answer but since this looks like selfstudy i dont think i should write it here can i create a private chatroom and show you my solution so that you can tell me if its correct","comment"
"368479","","692496.0","adamo i am curious as to what you mean by efficiency of statistical tests there are several definitions which one are you using to state that the ttest is efficient even for a general class of finitevariance distributions","comment"
"310387","","589744.0","the package marss fits linear gaussian statespace models with a kalman filter which easily accommodates missing values have you tried just sticking na values in your series for the middle part there is a priori no requirement for these two sessions to be of the same length","comment"
"199117","","384139.0","hey vinay i am curious did it work out in the end","comment"
"197286","","374180.0","ctd eg the same approach is used in the opening paragraph of this answer http statsstackexchangecom questions 125648 transformationchisquaredtonormaldistribution 125653#125653 to convert a chisquared 1 variate to a normal and in the opening paragraph of this answer http statsstackexchangecom questions 86135 isitpossibletoconvertarayleighdistributionintoagaussiandistribution 86143#86143 to convert a rayleighdistributed random variate with known parameter to normal","comment"
"389089","389087.0","","what is your hypothesis ie can you put what you want to test into individual questions the ideal tests then become more obviouseg your hypothesis might be that higher consumption of ultraprocessed food is negatively correlated with dietary diversity this is a statement hypothesis that you can then test to see whether your data provide statistically significant support for the hypothesis or notto me it appears that you have ordinal data the original dietary diversity score from 0 to 10 the consumption of ultraprocessed food is counts data ordinal data categorical data depending on how you look at it perhaps consider exploring ordinal regression firsta chisquared test seems reasonable if you decide to keep your data grouped as you have it at the momentyou might consider goodmankruskals gamma given that you have ordinal data but in general to choose between the tests look at the assumptions of each test and try to work out which one of them your data conforms to better","post"
"380544","","","so i tried to use ancova in r and when i was doing the diagnostics test turns out the data is not normalized now i am stuck and i do not know where to go from here please help is there a way that i can normalize my data or is there another test i can do","post"
"362069","","680523.0","give an example","comment"
"128667","","","fitting logistic function in r response unconstrained to 0 y 1","title"
"6332","","10163.0","as i keep saying use outofsample performance on a large set of series you cant easily compare insample performance of the two model classes","comment"
"31057","","60241.0","cardinal fantastic should have thought of that myself","comment"
"63321","","121786.0","with n500 and such a closetonormal looking distribution the clt kicks in very quickly and the uncertainty in the standard error of the mean starts to become very small tests of coefficients and confidence intervals should work perfectly well as is","comment"
"218296","","412938.0","are you asking for logistic regression what kinds of regressions are implied regression trees and neural networks as well","comment"
"425403","","794305.0","im talking about random lowdimensional feature maps like the rahimirecht one which uses the fourier transform of a kernel https peopleeecsberkeleyedu brecht papers 07rahrecnipspdf","comment"
"146228","","280589.0","mu_2 and sigma_22 are the first two moments of the distribution of a bean size after sampling with replacement from the original distribution which is normally distributed with mean mu and standard deviation sigma did it answer your question thank you","comment"
"299695","","","i am conducting an analysis of pharmacokinetics data in which i have plasma concentration of drug for different dosage groups in this way the area under the concentrationdose curve is computed asaucsum bar c _i + bar c _ i+1 times d_ i+1 d_i 2 in which bar c _i and d_i are the mean plasma concentration and dose of ith dosage group separately in this case i assume that bar c _i are independent random variables that follow the normal distributions n mu_i sigma_i2 and d_i are constants i want to estimate the 95 confidence interval of auc for this purpose i need the standard error for sum of random variables i am not sure whether the following formula is correcte auc sum mu_i+mu_ i+1 times d_ i+1 d_i 2 var auc sum sigma_i2+sigma_ i+1 2 times frac d_ i+1 d_i 2 2 any information on the formula for se for sum of random variables or any links which gives an idea is highly appreciated","post"
"332899","","","i have data collected over two years whereyear 1 34 successes in the representative sample of 49 taken from the finite population of 122 andyear 2 45 successes in the representative sample of 68 taken from the finite population of 175 what test do i use to see if there is any significant difference between the two","post"
"200335","","380145.0","lacerbi thats essentially correct except i would say more precisely that im comparing whether an interval around the cv estimate has a high probability of overlapping an interval around the bo estimate i appreciate the bias vs variance concern influential observations are a concern too but i wonder if they would already be priced in to the bo estimate at that point ie already influencing estimates of the mean and variance","comment"
"424503","424499.0","","i will mention several approaches that come to mind with illustrations in r maybe some of them will match with contents of your course and some can be easily done in pythonfirst here is a histogram of data that i assume from your comment is somewhat like yours the binning in most software depends on the sample size and range of the data and you cannot generally rely that tallest histogram bars will correspond to centers of the three distributions being mixedhist x probt br20 colskyblue2 rug x lines density x adj2 lwd2 colred the red curve is a kernel density estimate kde of the overall population i have used rs default density estimator in r the output of density gives x and yvectors you mightscan the values of the yvector to find its relative maxima find the corresponding xlocations and put classification barriers at troughs between the lower and central and between central and higher humpsnotice that this method does not apportion your 1024 observations intothree groups of about 340 observations if there is an unstated assumptionthat the three constituent distributions are sampled with equal probability you will have to take that information into account inmaking your classificationthe rug shows locations of points with overplotting for ties you can get an idea of their locations by sorting the data and you might takedifferences of the sorted data looking for two relatively large gapsyour idea of listing the frequencies in the various bins seemsreasonable here is a frequency histogram with the frequency ofeach bin shown atop the appropriate bar for these data in which the separation of the three categories is quite distinct the list of frequencies seems a good guide to to locating peaks and troughshowever in general i think kdes would be more reliablehere is another histogram of exactly the same data but rs default choice of bins the boundaries of the three categories would not be much different however the kde does not change as histogram binning changesthe exact maximum of the kde is at 10589 by taking differencesof the yden values one can also find exact locationsof troughsxden density x x yden density x yxden yden max yden 1 1058938in certain circumstances kdes or lists of frequenciesmay substitute for human eyeballs in classification by componentthere are also some clustering and discrimination methods i have not mentioned here i dont know anything about your course butthis problem may be an invitation to look ahead in the textbook for methods that may be coming soonfinally as an additional potential complication you should know that not all mixture distributions will show noticeable dips in the histogram or kde nor noticeable gaps in between tick marks or sortedobservations for example if three normal distributions all have the same standard deviation then you will see dips or gaps only whenthe means differ by more than two standard deviations reference if standard deviations and probabilities of components differ then the rules for visible gaps are more intricatesetseed 4321 x1 rnorm 100 50 5 x2 rnorm 100 75 5 x3 rnorm 100 85 5 x c x1 x2 x3 hist x probt colskyblue2 rug x lines density x lwd2 colred","post"
"351674","","","lag between forecast and actual value","title"
"338308","","639485.0","ah i see thank you for that the maths there has really made that much clearer effectively what you are saying is that 3 measures the effect that the growth of military expenditure as a share of gdp would have on the growth of gdp per capita as the level of corruption changes","comment"
"351650","351649.0","","this supposed statistician is mistaken many many naturally occurring phenomena tend towards gaussian no sample will perfectly conform but when compared against other competitor distributions the gaussian will tend to win for things like heights and weights and iq scores and things more so when the sample size increases but if you change the situation this isnt true the arrival times of radioactive alpha particles from uranium is exponential the number of heads coin tosses out of n throws is binomial this all said the central limit theorem states that the distribution of means is nearly always gaussian regardless of the form of the parent distribution heres an example consider the distribution of heights or weights for all people inside day care centers while human height is normally distributed in general at any one day care youll get a lot of short and light people and a couple taller heavier people very skewed now take the mean height or weight for each day care center in the united states that distribution of means will be normal","post"
"3091","","","ive seen a little bit here about the difference between statistical inference for random samples and what happens when we actually have population data most arguments seem to suggest you never actually have the population and the population data you think you have represents some unobservable super population which is the datagenerating process but say we have a state covered by counties and we have data on some environmental aspect of each county for example the total area of forest we want to see if some binary countylevel outcome is related to forest lets say presence or absence of a disease thinking lyme disease here as far as i can tell this qualifies as population level data would running a regular logistic regression and the normal tests on parameters yield correct estimates one thing that comes into this is whether the samples are independent in this case forest in county a is probably related to forest in the county b next door meaning a reduction in the degrees of freedom and imprecision in standard error estimates as i understand it but even if we had totally independent data would standard samplebased statistical inference be appropriate here","post"
"435711","","812418.0","i appreciate your valuable advice but it does not seem to solve my problem in the group lasso you mentioned a group of variables must be included together or excluded together in my problem however there is priority for example main effects have the highest priority and interaction terms and quadratic terms are the second if x22 should be included then x2 must be included but including x2 and excluding x22 is okay this is the difference from the group lasso you mentioned","comment"
"186064","","353558.0","+1 upvoting on the ground that questions leading to interesting answers must have some merit","comment"
"345229","","","i am looking for a way to estimate the number of observations needed for a regression analysis my hypothesised model is y_post y_pre + x1x2 my interest lies in whether the interaction term x1x2 is statistically significant i assumed a smalltomedium effect size for this x1x2 term my question is assuming a smalltomedium effect size what is the sample size needed to detect the effect with a power of 80 and alpha 005after looking into similar questions i couldnt find a good and systematic reference books for power analysis could anyone suggest a good reference book book chapter on how to conduct a sample size estimation using simulation in r i want to learn more about simulation because when i encounter different experimental designs in the future i could simulate the the sample size again by myselfmy background is psychology it would be wonderful if the references is also from this field although it is not necessary many similar questions remained unanswered or not satisfactorily eg power analysis by simulation how to simulate a custom power analysis of an lm model using r simulating responses from a factorial experiment for power analysis your help would be very much appreciated thank you","post"
"354890","","668452.0","possible duplicate of line of best fit does not look like a good fit why https statsstackexchangecom questions 332819 lineofbestfitdoesnotlooklikeagoodfitwhy","comment"
"194299","","369308.0","i appreciate the clearly thoughtful and likely timeconsuming response but i feel like this answer is a bit of a departure from the answers conveyed as accessibly as possible mandate of the question","comment"
"78667","","597822.0","what do you mean by the value found for rho you havent given any method or equations to determine it if you believe its the correlation of x then run your code with a value of 099 and compute the actual correlations that arise in samples","comment"
"159711","","304162.0","whuber yes but i cant understand what that means about the hypotesis that model 1 fits the training data better than model 2 or vice versa im being told to look at the pvalue but in both cases it gives the same probability","comment"
"248911","","474242.0","generally low activity in december people are to preoccupied with exams and christmas preparations","comment"
"277125","","531357.0","well i had to put the regions in my regression as control variables and after i got the output my teacher looked at it and said and are these results the same as in your figure with the average shares per region over that time period so i made the assumptions that the results of my regression had to be somewhat similar or logical as in the figure","comment"
"367293","","690573.0","jimmyjames i basically agree if you look at the skewness of the sample and its always precisely 0 its odd","comment"
"7730","","12661.0","chl is that because excel is slow or just incapable ie would give inaccurate results","comment"
"114883","","220394.0","what happened to to consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination he can perhaps say what the experiment died of formulating your substantive nonstatistical research question is desirable but if the question cannot be answer by the available methods and or data is useless","comment"
"348258","","","i have data from an experiment in which each subject was assigned to one of 4 experimental conditions condition a betweensubject factor and then faced 4 similar but distinct tasks task a withinsubject factor my dv is a continuous variable dv and the hypothesis is with respect to an interaction between condition and task for the analysis i am using the lme4 package in r and the model looks like thism lmer dv condition task + 1+order subj data where order is simply the order by which a subject faced the task 1 2 3 or 4 and subj is a unique subject idthis was an exploratory study and i now would like to use it to perform a power analysis for a larger study my question regards the way to perform a power analysis considering my goal is to find a significant interaction between a betweensubject factor and a withinsubject factor i tried looking at package simr which seems the most flexible one for performing power analysis of mixed models but cannot figure out how to do thisthanks in advance","post"
"192521","","365841.0","yeah it is more about defining rules of simulation the data is timeseries so ive spent some time on arima exp smooting modelling with poor results ols regression however did some improvement to my mae score","comment"
"157992","","","i have a group of subjects which did tests under 3 different conditionsif i now want to compare variables with a anova is it right that i get the same pvalues as if i would run 3 ttests comparing each of the conditions with each other","post"
"244783","","465695.0","edwardyu conflicting features will never be a problem the training will select useful features and combination and complicated transformation of features automatically","comment"
"355854","","670427.0","i suppose i mean cycles not seasons ultimately what i really want is for my data to fit any assumptions necessary to run the mannkendall trend test which from the following states constant variance and no serial correlation page 327 https pubsusgsgov twri twri4a3 pdf chapter12pdf ill also post the data the site is in northern california","comment"
"236017","","","the manual page for rs cor sayssome people have noted that the code for kendalls tau is slow for very large datasets many more than 1000 cases it rarely makes sense to do such a computation but see function corfk in package pcappwhy wouldnt it make sense to compute a kendalls tau for a large sample is there some reason tau is less useful or meaningful with larger samples or is it just that tau is hard to compute and you might as well approximate it by randomly sampling pairs of points and checking how often they agree","post"
"9448","","16029.0","linear model every meteorologist will tell you that everything they do is just a small adjustment to tomorrow will be the same","comment"
"66037","","127552.0","in addition to the mathematical proof of the equivalence you can think about it substantively if x_2 c x_1 then x_2 adds no new information it would be good if you could tell us what these variables are and why both xs should be in the model because as youve written it it makes no sense","comment"
"50969","","99769.0","here is a site on how to do it in four common software https wikisuittuftsedu confluence display sssi wilcoxon+signedrank+test","comment"
"10459","","17923.0","nothing that will inspire anybody to take up mathematics stick to books","comment"
"196854","","767901.0","this rule seemed to work quite well with a number of different data sets and three hidden layers one thing is for certain using this rule the number of neurons in the hidden layer s will be less than the number of input features size n","comment"
"249123","","476484.0","the f are the historical average fraction of the xi for each month compared to the eventual known total so the f and x are not correlated but the f and xi are","comment"
"316300","","600695.0","all the models you mention are linear in the parameters and in the vectors of predictors nonlinear regression https enwikipediaorg wiki nonlinear_regression is usually reserved for the case where the model is not linear in the parameters rather than merely curved in some original x so for example polynomial regression is generally referred to as multiple linear regression rather than nonlinear regression","comment"
"308669","","","robust pca as developed by candes et al 2009 or better yet netrepalli et al 2014 is a popular method for multivariate outlier detection but mahalanobis distance can also be used for outlier detection given a robust regularized estimate of the covariance matrix im curious about the dis advantages of using one method over the othermy intuition tells me that the greatest distinction between the two is the following when the data set is small in a statistical sense robust pca will give a lowerrank covariance while robust covariance matrix estimation will instead give a fullrank covariance due to the ledoitwolf regularization how does this in turn affect outlier detection","post"
"22833","","41485.0","benbolker a c all true","comment"
"416676","416668.0","","there is a vast literature about this topic the model schematic you have drawn shows a hidden markov model this class of model assumes that p y_t is independent of all other variables given z_t iep y_2 y_1 y_3 sum_ z2 p y_2 z_2 p z_2 y_1 y_3 therefore what you are really asking about is how to estimate p z_2 y_ 1t what you are then asking about is how filtering and smoothing work filtering generally is finding p z_t y_ 1t1 and smoothing is finding p z_t y_ 1t there are many resources about this online but here is a freely available and excellent on the topic bookyou may also find section 2 of this tutorial helpful and a more concise treatment of this topic","post"
"279182","279172.0","","one test not the only one you could use is the jonckheere terpstra test this compares ordinal data to either nominal or ordinal data","post"
"14928","","26809.0","andy w why should it be clear a priori that the hypothesis tests for the individual parameters do not say anything about significance of the overall model it is not assumed in ops question or my comments by the way that the model significance can be quantified only by ftests and even in that case there are examples where f is equivalent to a ttest so why not contemplate the possibility of a more complicated f being computable or estimable from a suite of ttests","comment"
"64752","","125135.0","i have all the data i can use much more individuals at no extra cost apart from some effort on my behalf surely using more data will yield more reliable results eg less risk for bias and narrower confidence intervals i want to get the best possible result with my resources","comment"
"390980","","","does this mean my model is useless","title"
"244660","","465431.0","have you looked into gaussian cox processes they may work nicely here and can be fitted with the r package inla https wwwgooglede urlsatsourcewebrctjurlhttps wwwmathntnuno inla rinlaorg papers s172010pdfved0ahukewin3awx5zbqahxj8ywkhrdod2wqfgg5maiusgafqjcnf_dhozzcvfsvck4xfspb6ur4kg0a pdf","comment"
"97714","","","as far as i know i have two options for tests in linear regression the ftest for the model if it explains more variance than it has error variance and the ttest to see if the slope is not zero with more than one predictor i can see why both tests are there but in my case i only have one predictor in my opinion the ftest and the ttest do the same in this case because if the slope is zero it is exactly the model it is compared to in the ftest where is the flaw in my logic","post"
"435765","","","i know the normal distribution is conjugate to itself are there others is there some sort of intuition behind why a given distribution would be conjugate to itself","post"
"179067","","339442.0","i agree with sheep that your list is nice but probably too much although i think mixed model the basis of it are essentials in today experimental design","comment"
"353339","","","the data consists of several discretetime markov chains indexed by a global time i assume all the chains are governed by the same transition matrix but that this can change in time i want to perform bayesian inference on the timevarying transition matrix with a prior that it varies smoothly in time this is in the context of a gibbs sampler and im particularly interested in drawing samples from the distribution of transition matrices at each time point in the homogeneous case i had been using a dirichletmultinomial distribution taking into account all transitions to tackle the nonhomogeneous case a first idea is to use a dirichletmultinomial distribution at each time point using all the transitions observed at that time point the chains mostly overlap in time but the number of transitions observed at a given time point can vary from 1 to 100 im concerned that for time points with few observed transitions the inferred timevarying probabilities could be nonsmooth one possibility for the lack of data at some time points would be to group together data of consecutive points and assume they are governed by the same transition matrix decreasing the time resolution do you have any suggestions i wonder about the possibility of incorporating a gaussian process prior over some parametrization of the transition matrixedit improve the question","post"
"135464","32105.0","","reading page 7 herehttp wwwcsprincetonedu picasso mats pcatutorialintuition_jppdfthey note that pca assumes that the distribution of whatever we are explaining can be described by a mean of zero and variance alone which they say can only be the normal distribution basically in addition to cams answer but i dont have enough reputation to comment","post"
"19032","","34428.0","hmm what does it mean for one point estimate to be contained within another","comment"
"144540","","276855.0","+1 for references i have never had any success in tbats in real world business forecasting would love to hear your perspective on its use in business forecasting","comment"
"363687","","683261.0","could you elaborate on what would constitute a method from one point of view writing down the regression model is a method which leaves us no actual question to answer what are you hoping to see in the answers","comment"
"310039","","589141.0","i will post an answer later but i have a question in the meantime just because i am curious about how these questions are born in what type of context did you get this question is this for instance a question arising in business or is it a hypothetical question as part of study and homework and maybe you can tell a bit more about it","comment"
"311311","311242.0","","essentially any model where the kernel of the posterior distribution in log units could be expressed in the stan language by incrementing the target keyword and you can add any prior assumptions you want on the parameters getting the stan mcmc algorithm to sample sufficiently efficiently is another matter but you have a good chance of that in this situationyour stan program might look something likedata intlower1 c number of colors intlower1 s number of shapes intlower1 t number of time periods vector t r_m market returns vector c s t r stacked returns parameters vector c beta_c vector s beta_s reallower0 sigma c s model vector c s t mu conditional mean vector c s t sigma_flat error sd int pos 1 for c in 1c for s in 1s mu pos pos + t 1 beta_c c beta_s s r_m sigma_flat pos pos + t 1 rep_vector sigma c s t pos pos + t target + normal_lpdf r mu sigma_flat loglikelihood illustrative priors target + normal_lpdf beta_c 0 1 target + normal_lpdf beta_s 0 1 target + exponential_lpdf to_vector sigma 1 but you will have to modify it if your panels are unbalanced if you have better priors or if you want to relax the assumption that beta_c and beta_s are independent a priori","post"
"308657","","","suppose i have a team team a and i am able to make an clone of them team b that is identical except for one difference for example they are baseball teams and team b uses a different type of bat than team a if team a and team b play a number of games against each other can the statistics from the headtohead games for example number of hits by each team strikeouts walks etc be compared using a ttest otherwise the games and teams will be identical same players same stadiums teams alternate which team bats first same pitchers etc ive been getting conflicting information about whether this is valid since their stats are not against a common opponent instead headtohead comparison so im not sure if a ttest is suitablesample data can i use a ttest to see if team as hits are statistically significantly different than team bs hits game # team a hits team b hits 1 5 9 2 4 5 3 7 12","post"
"158862","","302684.0","updated the answer","comment"
"403738","","755095.0","at first i thought it could be related to the conditional probabilities that they influence the classification accuracy of each class however i couldnt come up with a pattern in the probabilities that could explain this scenario","comment"
"344383","","649926.0","i added the selfstudy tag and edited the math notations to use latex formatting see https statsstackexchangecom editinghelp#latex","comment"
"62204","","119324.0","and by guestimate i mean fit the model without the correlation term fit the spherical model to the residuals get those parameters and then refit with gls but state the parameter values rather than use the formula using argument value without the nugget youd only need to specify the range as a parameter but with the nugget you will need both the range and 1 cor i j when i and j are two observations close together in space see corspher for details you dont even need to fit the model without the correlation structure if you can have reasonable guesses at those parameters","comment"
"21257","","38440.0","find the factor with the smallest coefficient make it the new base level and rerun the regression or just modify all related coefficients if all you want is their estimates","comment"
"293","192.0","","i think you need to rework this question it all depends on the problem data which has generated the crosstab","post"
"261879","261643.0","","id just like to add to cbeleites answer which is good with some quotes from a recent blog post of frank harrells on splitsample validation data splitting is an unstable method for validating models or classifiers especially when the number of subjects is less than about 20 000 fewer if signalnoise ratio is high this is because were you to split the data again develop a new model on the training sample and test it on the holdout sample the results are likely to vary significantlyanddata are too precious to not be used in model development parameter estimation resampling methods allow the data to be used for both development and validation and they do a good job in estimating the likely future performance of a model data splitting only has an advantage when the test sample is held by another researcher to ensure that the validation is unbiasedin other words in most cases you should prefer a resamplingbased validation approach and especially so when you have few observations in a n p situation the modelling process can have super high variance even if you use highbias methods such penalized regression if you dont do resampling you might spend a lot of time trying to interpret noise speaking from experience edit so whats resamplingbased validation anywaythere are a few ways to do validation by resampling the common idea is to make new samples from your original sample a popular and by now quite wellunderstood method is to use the bootstrap which would go like thismake a bootstrap sample by drawing n observations from your original nsized sample with replacementsince you drew the bootstrap sample with replacement some observations will appear more than once others wont be in there at all roughly about 60 of the observations will be in there the other 40 wontfit your model to the bootstrap sampleestimate your validation statistic by predicting on the observations that arent in the bootstrap samplerepeat for as long as you have the patience tothis gives you an empirical distribution over the validation statistic which you can use to say something about how well your method will generalize this could be for eg to estimate uncertainty or to calculate a point estimate preferably both other possible methods repeated holdout set repeated crossvalidation","post"
"285716","","546981.0","thanks for the answer any chance you could pull the relevant equation s from the book or point me to the primary source that wellek refers to both book versions are very rare to obtain the book in the local library i would need to get it shipped from another library deposit","comment"
"318253","","604609.0","worth to mentionslope 3 should be a tan of your desired angle in this case slope tan pi 18","comment"
"248176","","472561.0","thank you for the suggestion for the dharma package i have been working thought it today since my two covariates are categorical do you have a suggestion on how to interpret the plotresiduals plots i got an error message when i originally tried plotting them vs the simulated residuals that x must be numeric i converted the variables to numeric values but the graphs arent what i was expecting","comment"
"11899","","20563.0","jeromy scaling of ordinal data makes sense but how would optimal scaling of purely nominal categorical data work could you point me to a place where i could learn about this","comment"
"18770","","34017.0","interesting so essentially obs1 would become 5 separate obs of subject1 obs2 would become 4 or six or whatever obs of subject2on the outset i like it 2 questions 1 is this really legal and 2 it may work here but it doesnt seen scaleable say if the x and y axis were larger scales 100 or 1000 just for my theoretical understanding","comment"
"257057","","491013.0","but having no other information wouldnt you best guess at the next round of 60 flips be 42 heads why not use the mle of the success probability there","comment"
"354017","","666944.0","could you please decide whether you are asking about the sum or the difference","comment"
"299976","","570128.0","i modified the question because i realized there was only 1 question the rest was just an irrelevant train of thought of mine","comment"
"266094","","509501.0","it would help to know what baseline and effect models are doing however in general i would model this as a nonlinear pharmacokinetic or pharmacodynamic times series at 0 1 2 3 and 4 hours with a dummy variable for meal type the regression type i would use would depend on the exact characteristics of the modelling equation and type of noise the data transformation type might include none semi log linear log square root the regressions might include anova weighted least squares and tikhonov regularization adaptive for the pharmacokinetic parameter of interest","comment"
"406000","","758913.0","consider the x y dataset 0 1 1 1 by inspectionno calculation is neededyou can immediately determine the hat u_i which of a d are true for this dataset","comment"
